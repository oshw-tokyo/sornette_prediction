#!/usr/bin/env python3
"""
å¼·åŒ–ã•ã‚ŒãŸåˆ†æå®Ÿè¡Œå™¨
- ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å¤±æ•—è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
- ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã¨ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å¤±æ•—ã®è©³ç´°è¨˜éŒ²
- é‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
"""

import sys
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
from datetime import datetime, timedelta
import json
import traceback

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from infrastructure.database.fitting_failure_tracker import FittingFailureTracker
from infrastructure.database.failure_metadata_generator import FailureMetadataGenerator

class EnhancedAnalysisRunner:
    """å¼·åŒ–ã•ã‚ŒãŸåˆ†æå®Ÿè¡Œå™¨"""
    
    def __init__(self, db_path: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        """
        self.failure_tracker = FittingFailureTracker(db_path)
        self.metadata_generator = FailureMetadataGenerator()
    
    def run_symbol_analysis(self, symbol: str, analysis_basis_date: str,
                          schedule_name: str = None, backfill_batch_id: str = None,
                          force_retry: bool = False) -> Dict[str, Any]:
        """
        éŠ˜æŸ„åˆ†æã®å®Ÿè¡Œï¼ˆå¤±æ•—è¿½è·¡æ©Ÿèƒ½ä»˜ãï¼‰
        
        Args:
            symbol: éŠ˜æŸ„ã‚·ãƒ³ãƒœãƒ«
            analysis_basis_date: åˆ†æåŸºæº–æ—¥
            schedule_name: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å
            backfill_batch_id: ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ«ãƒãƒƒãƒID
            force_retry: å¼·åˆ¶ãƒªãƒˆãƒ©ã‚¤
            
        Returns:
            dict: å®Ÿè¡Œçµæœ
        """
        
        # 1. é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if not force_retry:
            check_result = self.failure_tracker.check_analysis_needed(
                symbol, analysis_basis_date, schedule_name
            )
            
            if not check_result['needed']:
                return {
                    'status': 'skipped',
                    'reason': check_result['reason'],
                    'symbol': symbol,
                    'analysis_basis_date': analysis_basis_date
                }
        
        # 2. è§£æé–‹å§‹è¨˜éŒ²
        status_id = self.failure_tracker.start_analysis(
            symbol, analysis_basis_date, schedule_name
        )
        
        print(f"ğŸ”„ {symbol} ({analysis_basis_date}) è§£æé–‹å§‹")
        
        try:
            # 3. ãƒ‡ãƒ¼ã‚¿å–å¾—æ®µéš
            data_result = self._retrieve_market_data(symbol, analysis_basis_date)
            
            if not data_result['success']:
                # ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—
                self._record_data_retrieval_failure(
                    status_id, symbol, analysis_basis_date, data_result, 
                    schedule_name, backfill_batch_id
                )
                return {
                    'status': 'failed',
                    'stage': 'data_retrieval',
                    'reason': data_result['error'],
                    'symbol': symbol,
                    'analysis_basis_date': analysis_basis_date
                }
            
            # 4. ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸè¨˜éŒ²
            data_info = data_result['data_info']
            self.failure_tracker.record_data_retrieval(
                status_id, data_info['source'], data_info['points'],
                data_info['start_date'], data_info['end_date']
            )
            
            # 5. ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ®µéš
            processing_result = self._process_market_data(data_result['data'])
            
            if not processing_result['success']:
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†å¤±æ•—
                self._record_data_processing_failure(
                    status_id, symbol, analysis_basis_date, processing_result, 
                    data_info, schedule_name, backfill_batch_id
                )
                return {
                    'status': 'failed',
                    'stage': 'data_processing',
                    'reason': processing_result['error'],
                    'symbol': symbol,
                    'analysis_basis_date': analysis_basis_date
                }
            
            # 6. LPPL ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°æ®µéš
            fitting_result = self._execute_lppl_fitting(
                processing_result['processed_data'], symbol, analysis_basis_date
            )
            
            if not fitting_result['success']:
                # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å¤±æ•—
                self._record_fitting_failure(
                    status_id, symbol, analysis_basis_date, fitting_result,
                    data_info, schedule_name, backfill_batch_id
                )
                return {
                    'status': 'failed',
                    'stage': 'fitting_execution',
                    'reason': fitting_result['error'],
                    'symbol': symbol,
                    'analysis_basis_date': analysis_basis_date
                }
            
            # 7. å“è³ªãƒã‚§ãƒƒã‚¯æ®µéš
            quality_result = self._check_fitting_quality(fitting_result['fit_result'])
            
            if not quality_result['success']:
                # å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—
                self._record_quality_check_failure(
                    status_id, symbol, analysis_basis_date, quality_result,
                    data_info, schedule_name, backfill_batch_id
                )
                return {
                    'status': 'failed',
                    'stage': 'quality_check',
                    'reason': quality_result['error'],
                    'symbol': symbol,
                    'analysis_basis_date': analysis_basis_date
                }
            
            # 8. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            result_id = self._save_analysis_result(
                fitting_result['fit_result'], data_info, schedule_name, backfill_batch_id
            )
            
            # 9. æˆåŠŸè¨˜éŒ²
            self.failure_tracker.record_success(status_id, result_id)
            
            print(f"âœ… {symbol} ({analysis_basis_date}) è§£ææˆåŠŸ")
            
            return {
                'status': 'success',
                'result_id': result_id,
                'symbol': symbol,
                'analysis_basis_date': analysis_basis_date,
                'quality_score': quality_result['quality_score']
            }
            
        except Exception as e:
            # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼
            error_details = {
                'exception_type': type(e).__name__,
                'exception_message': str(e),
                'traceback': traceback.format_exc()
            }
            
            self.failure_tracker.record_failure(
                status_id, symbol, analysis_basis_date, 'unknown',
                'unexpected_error', 'system_error', str(e),
                error_details, None, {}, schedule_name, backfill_batch_id
            )
            
            print(f"âŒ {symbol} ({analysis_basis_date}) äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            return {
                'status': 'failed',
                'stage': 'unexpected_error',
                'reason': str(e),
                'symbol': symbol,
                'analysis_basis_date': analysis_basis_date
            }
    
    def _retrieve_market_data(self, symbol: str, analysis_basis_date: str) -> Dict[str, Any]:
        """ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
        
        print(f"   ğŸ“Š ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {symbol}")
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ unified_data_client ã‚’ä½¿ç”¨
        try:
            # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—: å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«å®Ÿè£…
            # from infrastructure.data_sources.unified_data_client import UnifiedDataClient
            # client = UnifiedDataClient()
            # data, source = client.get_data_with_fallback(symbol, start_date, end_date)
            
            # ãƒ†ã‚¹ãƒˆç”¨ã®æˆåŠŸã‚±ãƒ¼ã‚¹
            if symbol in ['TEST_SUCCESS', 'BTC', 'ETH']:
                return {
                    'success': True,
                    'data': {'mock': 'data'},  # å®Ÿéš›ã®DataFrameãŒå…¥ã‚‹
                    'data_info': {
                        'source': 'coingecko',
                        'points': 365,
                        'start_date': '2024-08-10',
                        'end_date': '2025-08-10'
                    }
                }
            
            # ãƒ†ã‚¹ãƒˆç”¨ã®å¤±æ•—ã‚±ãƒ¼ã‚¹  
            elif symbol == 'TEST_RATE_LIMIT':
                return {
                    'success': False,
                    'error': 'Rate limit exceeded',
                    'http_status': 429,
                    'api_provider': 'coingecko',
                    'request_details': {'symbol': symbol, 'period': '365 days'}
                }
            
            elif symbol == 'TEST_NOT_FOUND':
                return {
                    'success': False,
                    'error': 'Symbol not found',
                    'http_status': 404,
                    'api_provider': 'coingecko'
                }
            
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆåŠŸ
                return {
                    'success': True,
                    'data': {'mock': 'data'},
                    'data_info': {
                        'source': 'mock',
                        'points': 200,
                        'start_date': '2024-08-10',
                        'end_date': '2025-08-10'
                    }
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'Network error: {str(e)}',
                'network_error': str(e)
            }
    
    def _process_market_data(self, data: Any) -> Dict[str, Any]:
        """ãƒãƒ¼ã‚±ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
        
        print(f"   ğŸ”§ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­")
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ data validation, transformationç­‰
        try:
            # ãƒ†ã‚¹ãƒˆç”¨: å¸¸ã«æˆåŠŸ
            return {
                'success': True,
                'processed_data': data  # å®Ÿéš›ã®å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã‚‹
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Data processing failed: {str(e)}',
                'processing_stage': 'validation',
                'data_shape': getattr(data, 'shape', None),
                'validation_errors': [str(e)]
            }
    
    def _execute_lppl_fitting(self, data: Any, symbol: str, 
                            analysis_basis_date: str) -> Dict[str, Any]:
        """LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å®Ÿè¡Œï¼ˆãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
        
        print(f"   ğŸ¯ LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å®Ÿè¡Œ")
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ core.fitting.fitter ã‚’ä½¿ç”¨
        try:
            # ãƒ†ã‚¹ãƒˆç”¨ã®æˆåŠŸã‚±ãƒ¼ã‚¹
            if symbol in ['TEST_SUCCESS', 'BTC', 'ETH']:
                return {
                    'success': True,
                    'fit_result': {
                        'parameters': {
                            'tc': 1.5, 'beta': 0.8, 'omega': 12.0, 
                            'phi': 2.1, 'A': 5.2, 'B': -0.3, 'C': 0.02
                        },
                        'statistics': {'r_squared': 0.92, 'rmse': 0.015},
                        'predictions': {'crash_date': '2025-12-15', 'days_to_crash': 127}
                    }
                }
            
            # ãƒ†ã‚¹ãƒˆç”¨ã®å¤±æ•—ã‚±ãƒ¼ã‚¹
            elif symbol == 'TEST_OPTIMIZATION_FAIL':
                return {
                    'success': False,
                    'error': 'Optimization failed to converge',
                    'optimization_details': {
                        'algorithm': 'SLSQP',
                        'iterations': 100,
                        'convergence_status': 'max_iterations_reached'
                    },
                    'parameter_values': {'tc': 1.001, 'beta': 0.95},
                    'boundary_issues': ['tc_lower']
                }
            
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæˆåŠŸï¼ˆä½å“è³ªï¼‰
                return {
                    'success': True,
                    'fit_result': {
                        'parameters': {
                            'tc': 1.01, 'beta': 0.1, 'omega': 20.0, 
                            'phi': -3.14, 'A': 8.8, 'B': -0.29, 'C': 0.01
                        },
                        'statistics': {'r_squared': 0.65, 'rmse': 0.08},
                        'predictions': {'crash_date': '2025-09-01', 'days_to_crash': 22}
                    }
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'Fitting execution error: {str(e)}',
                'fitting_method': 'lppl',
                'exception_details': {'type': type(e).__name__, 'message': str(e)}
            }
    
    def _check_fitting_quality(self, fit_result: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
        
        print(f"   âœ… å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
        
        try:
            statistics = fit_result['statistics']
            r_squared = statistics.get('r_squared', 0)
            parameters = fit_result['parameters']
            
            # ç°¡å˜ãªå“è³ªãƒã‚§ãƒƒã‚¯
            failed_checks = []
            
            if r_squared < 0.8:
                failed_checks.append('r_squared_too_low')
            
            if parameters.get('tc', 0) < 1.05:
                failed_checks.append('tc_too_close_to_one')
            
            if parameters.get('omega', 0) >= 19.9:
                failed_checks.append('omega_at_upper_bound')
            
            quality_score = max(0, 1.0 - len(failed_checks) * 0.3)
            
            if quality_score < 0.5:
                return {
                    'success': False,
                    'error': 'Quality score too low',
                    'quality_score': quality_score,
                    'failed_checks': failed_checks,
                    'thresholds': {'min_r_squared': 0.8, 'min_quality': 0.5}
                }
            
            return {
                'success': True,
                'quality_score': quality_score,
                'passed_checks': True
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'Quality check error: {str(e)}',
                'quality_assessment': 'failed_to_evaluate'
            }
    
    def _save_analysis_result(self, fit_result: Dict[str, Any], 
                            data_info: Dict[str, Any], schedule_name: str,
                            backfill_batch_id: str) -> int:
        """åˆ†æçµæœã®ä¿å­˜ï¼ˆãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
        
        print(f"   ğŸ’¾ çµæœä¿å­˜")
        
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ analysis_results ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
        # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã§ã¯ä»®ã®IDã‚’è¿”ã™
        return 12345
    
    def _record_data_retrieval_failure(self, status_id: int, symbol: str,
                                     analysis_basis_date: str, data_result: Dict[str, Any],
                                     schedule_name: str, backfill_batch_id: str):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—ã®è¨˜éŒ²"""
        
        metadata = self.metadata_generator.create_data_retrieval_failure_metadata(
            api_provider=data_result.get('api_provider', 'unknown'),
            http_status=data_result.get('http_status'),
            request_details=data_result.get('request_details', {}),
            network_error=data_result.get('network_error')
        )
        
        self.failure_tracker.record_failure(
            status_id, symbol, analysis_basis_date,
            data_result.get('api_provider', 'unknown'),
            'data_retrieval', 'api_failure', data_result['error'],
            data_result, metadata, {'retrieval_success': False},
            schedule_name, backfill_batch_id
        )
    
    def _record_data_processing_failure(self, status_id: int, symbol: str,
                                      analysis_basis_date: str, processing_result: Dict[str, Any],
                                      data_info: Dict[str, Any], schedule_name: str,
                                      backfill_batch_id: str):
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†å¤±æ•—ã®è¨˜éŒ²"""
        
        metadata = self.metadata_generator.create_data_processing_failure_metadata(
            processing_stage=processing_result.get('processing_stage', 'unknown'),
            data_shape=processing_result.get('data_shape'),
            validation_errors=processing_result.get('validation_errors', [])
        )
        
        self.failure_tracker.record_failure(
            status_id, symbol, analysis_basis_date, data_info['source'],
            'data_processing', 'insufficient_data', processing_result['error'],
            processing_result, metadata, data_info, schedule_name, backfill_batch_id
        )
    
    def _record_fitting_failure(self, status_id: int, symbol: str,
                              analysis_basis_date: str, fitting_result: Dict[str, Any],
                              data_info: Dict[str, Any], schedule_name: str,
                              backfill_batch_id: str):
        """ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å¤±æ•—ã®è¨˜éŒ²"""
        
        opt_details = fitting_result.get('optimization_details', {})
        
        metadata = self.metadata_generator.create_fitting_failure_metadata(
            fitting_method='lppl',
            optimization_algorithm=opt_details.get('algorithm', 'unknown'),
            parameter_bounds={'tc': [1.0, 3.0], 'beta': [0.1, 1.0]},  # å®Ÿéš›ã®å€¤ã‚’ä½¿ç”¨
            attempted_iterations=opt_details.get('iterations', 0),
            convergence_status=opt_details.get('convergence_status', 'failed'),
            parameter_values=fitting_result.get('parameter_values'),
            boundary_issues=fitting_result.get('boundary_issues', [])
        )
        
        self.failure_tracker.record_failure(
            status_id, symbol, analysis_basis_date, data_info['source'],
            'fitting_execution', 'optimization_failed', fitting_result['error'],
            fitting_result, metadata, data_info, schedule_name, backfill_batch_id
        )
    
    def _record_quality_check_failure(self, status_id: int, symbol: str,
                                    analysis_basis_date: str, quality_result: Dict[str, Any],
                                    data_info: Dict[str, Any], schedule_name: str,
                                    backfill_batch_id: str):
        """å“è³ªãƒã‚§ãƒƒã‚¯å¤±æ•—ã®è¨˜éŒ²"""
        
        metadata = self.metadata_generator.create_quality_check_failure_metadata(
            quality_score=quality_result.get('quality_score', 0),
            failed_checks=quality_result.get('failed_checks', []),
            quality_thresholds=quality_result.get('thresholds', {}),
            statistical_values={}
        )
        
        self.failure_tracker.record_failure(
            status_id, symbol, analysis_basis_date, data_info['source'],
            'quality_check', 'quality_rejected', quality_result['error'],
            quality_result, metadata, data_info, schedule_name, backfill_batch_id
        )

def test_enhanced_runner():
    """å¼·åŒ–ã•ã‚ŒãŸåˆ†æå®Ÿè¡Œå™¨ã®ãƒ†ã‚¹ãƒˆ"""
    
    print("ğŸ§ª å¼·åŒ–ã•ã‚ŒãŸåˆ†æå®Ÿè¡Œå™¨ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    runner = EnhancedAnalysisRunner()
    
    test_cases = [
        ('TEST_SUCCESS', '2025-08-10', 'æˆåŠŸã‚±ãƒ¼ã‚¹'),
        ('TEST_RATE_LIMIT', '2025-08-10', 'ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼'),
        ('TEST_NOT_FOUND', '2025-08-10', 'ã‚·ãƒ³ãƒœãƒ«æœªç™ºè¦‹'),
        ('TEST_OPTIMIZATION_FAIL', '2025-08-10', 'æœ€é©åŒ–å¤±æ•—'),
        ('TEST_LOW_QUALITY', '2025-08-10', 'å“è³ªä¸è‰¯')
    ]
    
    results = []
    
    for symbol, date, description in test_cases:
        print(f"\\nğŸ§ª ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: {description}")
        print("-" * 40)
        
        result = runner.run_symbol_analysis(
            symbol, date, 'test_schedule', 'test_batch_001'
        )
        
        results.append((symbol, result['status'], description))
        print(f"çµæœ: {result['status']} - {result.get('reason', 'æˆåŠŸ')}")
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\\n" + "=" * 60)
    print("ğŸ§ª ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)
    
    for symbol, status, description in results:
        status_emoji = "âœ…" if status == "success" else "âŒ"
        print(f"{status_emoji} {description:20}: {status}")
    
    # å¤±æ•—çµ±è¨ˆç¢ºèª
    stats = runner.failure_tracker.get_failure_statistics('test_schedule')
    print(f"\\nğŸ“Š å¤±æ•—çµ±è¨ˆ:")
    print(f"  å¤±æ•—ã‚«ãƒ†ã‚´ãƒªåˆ¥: {stats['failure_by_category']}")
    print(f"  å¤±æ•—æ®µéšåˆ¥: {stats['failure_by_stage']}")
    
    return results

if __name__ == "__main__":
    test_enhanced_runner()