#!/usr/bin/env python3
"""
åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã¨å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ°¸ç¶šåŒ–
"""

import sqlite3
import json
import pandas as pd
from datetime import datetime
from typing import Dict, List, Optional, Any
import os
import base64
from pathlib import Path

class ResultsDatabase:
    """åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "results/analysis_results.db"):
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        
        Args:
            db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.db_path = db_path
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()
        
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åˆ†æçµæœãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    analysis_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    data_source TEXT,
                    data_period_start DATE,
                    data_period_end DATE,
                    data_points INTEGER,
                    
                    -- LPPLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                    tc REAL,
                    beta REAL,
                    omega REAL,
                    phi REAL,
                    A REAL,
                    B REAL,
                    C REAL,
                    
                    -- ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å“è³ª
                    r_squared REAL,
                    rmse REAL,
                    quality TEXT,
                    confidence REAL,
                    is_usable BOOLEAN,
                    
                    -- äºˆæ¸¬æƒ…å ±
                    predicted_crash_date DATE,
                    days_to_crash INTEGER,
                    
                    -- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
                    fitting_method TEXT,
                    window_days INTEGER,
                    total_candidates INTEGER,
                    successful_candidates INTEGER,
                    
                    -- JSONå½¢å¼ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿
                    quality_metadata TEXT,
                    selection_criteria TEXT
                )
            ''')
            
            # å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS visualizations (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id INTEGER,
                    chart_type TEXT NOT NULL,
                    chart_title TEXT,
                    file_path TEXT,
                    image_data BLOB,
                    creation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    description TEXT,
                    
                    FOREIGN KEY (analysis_id) REFERENCES analysis_results (id)
                )
            ''')
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS data_sources (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT NOT NULL,
                    source_name TEXT NOT NULL,
                    last_update TIMESTAMP,
                    data_points INTEGER,
                    date_range_start DATE,
                    date_range_end DATE,
                    api_status TEXT,
                    notes TEXT
                )
            ''')
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS schedule_config (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    schedule_name TEXT UNIQUE NOT NULL,
                    frequency TEXT NOT NULL,  -- 'daily', 'weekly', 'monthly'
                    day_of_week INTEGER,      -- 0=æœˆæ›œ, 6=æ—¥æ›œ (weeklyç”¨)
                    hour INTEGER DEFAULT 9,
                    minute INTEGER DEFAULT 0,
                    timezone TEXT DEFAULT 'UTC',
                    symbols TEXT,             -- JSONé…åˆ—å½¢å¼
                    enabled BOOLEAN DEFAULT 1,
                    last_run TIMESTAMP,
                    next_run TIMESTAMP,
                    created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    auto_backfill_limit INTEGER DEFAULT 30
                )
            ''')
            
            # analysis_resultsãƒ†ãƒ¼ãƒ–ãƒ«ã®æ‹¡å¼µï¼ˆæ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã«åˆ—è¿½åŠ ï¼‰
            self._add_column_if_not_exists(cursor, 'analysis_results', 'schedule_name', 'TEXT')
            self._add_column_if_not_exists(cursor, 'analysis_results', 'analysis_basis_date', 'DATE')
            self._add_column_if_not_exists(cursor, 'analysis_results', 'is_scheduled', 'BOOLEAN DEFAULT 0')
            self._add_column_if_not_exists(cursor, 'analysis_results', 'backfill_batch_id', 'TEXT')
            self._add_column_if_not_exists(cursor, 'analysis_results', 'is_expired', 'BOOLEAN DEFAULT 0')
            
            # æ›œæ—¥ãƒ¡ã‚¿æƒ…å ±ã®è¿½åŠ ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºæœ€é©åŒ–ï¼‰
            self._add_column_if_not_exists(cursor, 'analysis_results', 'basis_day_of_week', 'INTEGER')  # 0=æœˆæ›œ, 6=æ—¥æ›œ
            self._add_column_if_not_exists(cursor, 'analysis_results', 'analysis_frequency', 'TEXT')     # weekly, daily
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol_date ON analysis_results (symbol, analysis_date)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_quality ON analysis_results (quality, is_usable)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_schedule_enabled ON schedule_config (enabled)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_schedule_basis ON analysis_results (schedule_name, analysis_basis_date)')
            
            # åˆ†æåŸºæº–æ—¥ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæœ€é‡è¦ï¼šãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºæœ€é©åŒ–ï¼‰
            cursor.execute('CREATE UNIQUE INDEX IF NOT EXISTS unique_symbol_basis_date ON analysis_results (symbol, analysis_basis_date)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_symbol_basis_date ON analysis_results (symbol, analysis_basis_date DESC)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_frequency_basis ON analysis_results (analysis_frequency, analysis_basis_date DESC)')
            
            # é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ2025-08-11è¿½åŠ ï¼‰
            self._add_advanced_filtering_indexes(cursor)
            
            conn.commit()
            print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†: {self.db_path}")
    
    def _add_advanced_filtering_indexes(self, cursor):
        """
        é«˜åº¦ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã®ãŸã‚ã®æœ€é©åŒ–ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ 
        æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã«å½±éŸ¿ã‚’ä¸ãˆãªã„å®‰å…¨ãªå®Ÿè£… (2025-08-11è¿½åŠ )
        """
        advanced_indexes = [
            # 1. å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–ï¼ˆRÂ²ã€ä¿¡é ¼åº¦ã€ä½¿ç”¨å¯èƒ½æ€§ï¼‰
            ("idx_quality_comprehensive", 
             "CREATE INDEX IF NOT EXISTS idx_quality_comprehensive ON analysis_results (r_squared, confidence, is_usable, analysis_basis_date DESC)"),
            
            # 2. äºˆæ¸¬é–¢é€£ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–ï¼ˆäºˆæ¸¬æ—¥ã€åŸºæº–æ—¥ã€éŠ˜æŸ„ï¼‰  
            ("idx_prediction_filters",
             "CREATE INDEX IF NOT EXISTS idx_prediction_filters ON analysis_results (predicted_crash_date, analysis_basis_date, symbol)"),
            
            # 3. æ™‚ç³»åˆ—åˆ†ææ—¥æœ€é©åŒ–ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã§0.0172ç§’â†’0.0001ç§’æ”¹å–„ï¼‰
            ("idx_analysis_date_desc",
             "CREATE INDEX IF NOT EXISTS idx_analysis_date_desc ON analysis_results (analysis_date DESC)"),
            
            # 4. è¤‡åˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ€é©åŒ–ï¼ˆå“è³ª+äºˆæ¸¬ç¯„å›²ï¼‰
            ("idx_composite_filters",
             "CREATE INDEX IF NOT EXISTS idx_composite_filters ON analysis_results (is_usable, r_squared, predicted_crash_date)")
        ]
        
        print("ğŸ”§ é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ä¸­...")
        
        for idx_name, idx_sql in advanced_indexes:
            try:
                # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª
                cursor.execute("""
                    SELECT name FROM sqlite_master 
                    WHERE type='index' AND name=?
                """, (idx_name,))
                
                if cursor.fetchone():
                    print(f"  âšª {idx_name}: æ—¢ã«å­˜åœ¨")
                else:
                    cursor.execute(idx_sql)
                    print(f"  âœ… {idx_name}: ä½œæˆå®Œäº†")
                    
            except Exception as e:
                print(f"  âŒ {idx_name}: ã‚¨ãƒ©ãƒ¼ - {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦ã‚‚ä»–ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ç¶™ç¶šå‡¦ç†
                continue
    
    def _add_column_if_not_exists(self, cursor, table_name: str, column_name: str, column_def: str):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ã«åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿è¿½åŠ """
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = [row[1] for row in cursor.fetchall()]
        
        if column_name not in columns:
            cursor.execute(f"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_def}")
            print(f"  âœ… åˆ—è¿½åŠ : {table_name}.{column_name}")
    
    def save_analysis_result(self, result_data: Dict[str, Any]) -> int:
        """
        åˆ†æçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        
        Args:
            result_data: åˆ†æçµæœãƒ‡ãƒ¼ã‚¿
            
        Returns:
            int: ä¿å­˜ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ID
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            required_fields = ['symbol', 'tc', 'beta', 'omega', 'r_squared']
            for field in required_fields:
                if field not in result_data:
                    raise ValueError(f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ '{field}' ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            
            # ğŸ”§ Issue I048ä¿®æ­£: analysis_basis_date ã‚’è‡ªå‹•è¨­å®šï¼ˆdata_period_end ã‚’ä½¿ç”¨ï¼‰
            analysis_basis_date = result_data.get('analysis_basis_date') or result_data.get('data_period_end')
            
            # é‡è¤‡é˜²æ­¢: åŒä¸€éŠ˜æŸ„ãƒ»åŒä¸€åŸºæº–æ—¥ã¯æ›´æ–°ã€æ–°è¦ã¯æŒ¿å…¥ï¼ˆUPSERTï¼‰
            cursor.execute('''
                INSERT OR REPLACE INTO analysis_results (
                    symbol, data_source, data_period_start, data_period_end, data_points,
                    tc, beta, omega, phi, A, B, C,
                    r_squared, rmse, quality, confidence, is_usable,
                    predicted_crash_date, days_to_crash,
                    fitting_method, window_days, total_candidates, successful_candidates,
                    quality_metadata, selection_criteria, analysis_basis_date
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result_data['symbol'],
                result_data.get('data_source', 'unknown'),
                result_data.get('data_period_start'),
                result_data.get('data_period_end'),
                result_data.get('data_points', 0),
                result_data['tc'],
                result_data['beta'],
                result_data['omega'],
                result_data.get('phi', 0.0),
                result_data.get('A', 0.0),
                result_data.get('B', 0.0),
                result_data.get('C', 0.0),
                result_data['r_squared'],
                result_data.get('rmse', 0.0),
                result_data.get('quality', 'unknown'),
                result_data.get('confidence', 0.0),
                result_data.get('is_usable', False),
                result_data.get('predicted_crash_date'),
                result_data.get('days_to_crash'),
                result_data.get('fitting_method', 'multi_criteria'),
                result_data.get('window_days', 0),
                result_data.get('total_candidates', 0),
                result_data.get('successful_candidates', 0),
                json.dumps(result_data.get('quality_metadata', {})),
                json.dumps(result_data.get('selection_criteria', {})),
                analysis_basis_date
            ))
            
            analysis_id = cursor.lastrowid
            conn.commit()
            
            print(f"ğŸ“Š åˆ†æçµæœä¿å­˜å®Œäº†: ID={analysis_id}, Symbol={result_data['symbol']}")
            return analysis_id
    
    def save_visualization(self, analysis_id: int, chart_type: str, file_path: str, 
                          title: str = "", description: str = "") -> int:
        """
        å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        
        Args:
            analysis_id: é–¢é€£ã™ã‚‹åˆ†æçµæœID
            chart_type: ãƒãƒ£ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—
            file_path: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            title: ãƒãƒ£ãƒ¼ãƒˆã‚¿ã‚¤ãƒˆãƒ«
            description: èª¬æ˜
            
        Returns:
            int: ä¿å­˜ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ID
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’BLOBã¨ã—ã¦èª­ã¿è¾¼ã¿
            image_data = None
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    image_data = f.read()
            
            cursor.execute('''
                INSERT INTO visualizations (
                    analysis_id, chart_type, chart_title, file_path, 
                    image_data, description
                ) VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                analysis_id, chart_type, title, file_path, 
                image_data, description
            ))
            
            viz_id = cursor.lastrowid
            conn.commit()
            
            print(f"ğŸ“Š å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†: ID={viz_id}, Type={chart_type}")
            return viz_id
    
    def get_recent_analyses(self, limit: int = 50, symbol: str = None) -> pd.DataFrame:
        """
        æœ€è¿‘ã®åˆ†æçµæœã‚’å–å¾—
        
        Args:
            limit: å–å¾—ä»¶æ•°åˆ¶é™
            symbol: ç‰¹å®šéŠ˜æŸ„ã®ã¿å–å¾—ã™ã‚‹å ´åˆ
            
        Returns:
            DataFrame: åˆ†æçµæœ
        """
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT 
                    id, symbol, analysis_date, data_source,
                    data_period_start, data_period_end, data_points,
                    tc, beta, omega, phi, A, B, C,
                    r_squared, rmse, quality, confidence, is_usable,
                    predicted_crash_date, days_to_crash,
                    window_days, total_candidates, successful_candidates,
                    analysis_basis_date
                FROM analysis_results
            '''
            
            params = []
            if symbol:
                query += ' WHERE symbol = ?'
                params.append(symbol)
            
            # âš ï¸ CRITICAL: åˆ†æåŸºæº–æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆanalysis_dateã§ã¯ãªã„ï¼‰
            query += ' ORDER BY analysis_basis_date DESC, analysis_date DESC'
            
            # ğŸ”§ ä¿®æ­£: limit=Noneã®å ´åˆã¯LIMITå¥ã‚’è¿½åŠ ã—ãªã„ï¼ˆ2025-08-11ï¼‰
            if limit is not None:
                query += ' LIMIT ?'
                params.append(limit)
            
            return pd.read_sql_query(query, conn, params=params)
    
    def get_recent_analyses_by_frequency(self, symbol: str = None, frequency: str = 'weekly', limit: int = 50) -> pd.DataFrame:
        """
        é »åº¦åˆ¥æœ€è¿‘ã®åˆ†æçµæœã‚’å–å¾—ï¼ˆé€±æ¬¡ãƒ‡ãƒ¼ã‚¿å„ªå…ˆè¡¨ç¤ºï¼‰
        
        Args:
            symbol: ç‰¹å®šéŠ˜æŸ„ã®ã¿å–å¾—ã™ã‚‹å ´åˆ
            frequency: å–å¾—é »åº¦ ('weekly', 'daily', 'monthly')
            limit: å–å¾—ä»¶æ•°åˆ¶é™
            
        Returns:
            DataFrame: åˆ†æçµæœ
        """
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT 
                    id, symbol, analysis_date, data_source,
                    data_period_start, data_period_end, data_points,
                    tc, beta, omega, phi, A, B, C,
                    r_squared, rmse, quality, confidence, is_usable,
                    predicted_crash_date, days_to_crash,
                    window_days, total_candidates, successful_candidates,
                    schedule_name, analysis_basis_date, analysis_frequency,
                    basis_day_of_week
                FROM analysis_results
                WHERE analysis_frequency = ?
            '''
            params = [frequency]
            
            if symbol:
                query += ' AND symbol = ?'
                params.append(symbol)
            
            # âš ï¸ CRITICAL: åˆ†æåŸºæº–æ—¥ã§ã‚½ãƒ¼ãƒˆï¼ˆanalysis_dateã§ã¯ãªã„ï¼‰
            query += ' ORDER BY analysis_basis_date DESC, analysis_date DESC'
            
            # ğŸ”§ ä¿®æ­£: limit=Noneã®å ´åˆã¯LIMITå¥ã‚’è¿½åŠ ã—ãªã„ï¼ˆ2025-08-11ï¼‰
            if limit is not None:
                query += ' LIMIT ?'
                params.append(limit)
            
            return pd.read_sql_query(query, conn, params=params)
    
    def get_latest_analysis_per_frequency(self, symbol: str) -> pd.DataFrame:
        """
        éŠ˜æŸ„åˆ¥ãƒ»é »åº¦åˆ¥ã®æœ€æ–°åˆ†æçµæœã‚’å–å¾—
        
        Args:
            symbol: å¯¾è±¡éŠ˜æŸ„
            
        Returns:
            DataFrame: é »åº¦åˆ¥æœ€æ–°åˆ†æçµæœ
        """
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT DISTINCT
                    a1.id, a1.symbol, a1.analysis_date, a1.data_source,
                    a1.data_period_start, a1.data_period_end, a1.data_points,
                    a1.tc, a1.beta, a1.omega, a1.phi, a1.A, a1.B, a1.C,
                    a1.r_squared, a1.rmse, a1.quality, a1.confidence, a1.is_usable,
                    a1.predicted_crash_date, a1.days_to_crash,
                    a1.window_days, a1.total_candidates, a1.successful_candidates,
                    a1.schedule_name, a1.analysis_basis_date, a1.analysis_frequency,
                    a1.basis_day_of_week
                FROM analysis_results a1
                INNER JOIN (
                    SELECT analysis_frequency, MAX(analysis_basis_date) as max_basis_date
                    FROM analysis_results 
                    WHERE symbol = ? AND analysis_frequency IS NOT NULL
                    GROUP BY analysis_frequency
                ) a2 ON a1.analysis_frequency = a2.analysis_frequency 
                    AND a1.analysis_basis_date = a2.max_basis_date
                WHERE a1.symbol = ?
                ORDER BY a1.analysis_frequency, a1.analysis_basis_date DESC
            '''
            
            return pd.read_sql_query(query, conn, params=[symbol, symbol])
    
    def get_analysis_details(self, analysis_id: int) -> Dict[str, Any]:
        """
        ç‰¹å®šåˆ†æã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Args:
            analysis_id: åˆ†æID
            
        Returns:
            Dict: è©³ç´°æƒ…å ±
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åˆ†æçµæœå–å¾—
            cursor.execute('SELECT * FROM analysis_results WHERE id = ?', (analysis_id,))
            result = cursor.fetchone()
            
            if not result:
                return {}
            
            # ã‚«ãƒ©ãƒ åå–å¾—
            columns = [desc[0] for desc in cursor.description]
            analysis_data = dict(zip(columns, result))
            
            # JSON ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ‘ãƒ¼ã‚¹
            if analysis_data.get('quality_metadata'):
                analysis_data['quality_metadata'] = json.loads(analysis_data['quality_metadata'])
            
            if analysis_data.get('selection_criteria'):
                analysis_data['selection_criteria'] = json.loads(analysis_data['selection_criteria'])
            
            # é–¢é€£å¯è¦–åŒ–ãƒ‡ãƒ¼ã‚¿å–å¾—
            cursor.execute('''
                SELECT chart_type, chart_title, file_path, description, creation_date
                FROM visualizations 
                WHERE analysis_id = ?
                ORDER BY creation_date DESC
            ''', (analysis_id,))
            
            visualizations = []
            for viz in cursor.fetchall():
                viz_cols = ['chart_type', 'chart_title', 'file_path', 'description', 'creation_date']
                visualizations.append(dict(zip(viz_cols, viz)))
            
            analysis_data['visualizations'] = visualizations
            
            return analysis_data
    
    def get_visualization_image(self, analysis_id: int, chart_type: str) -> Optional[bytes]:
        """
        å¯è¦–åŒ–ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            analysis_id: åˆ†æID
            chart_type: ãƒãƒ£ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—
            
        Returns:
            bytes: ç”»åƒãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT image_data FROM visualizations 
                WHERE analysis_id = ? AND chart_type = ?
                ORDER BY creation_date DESC LIMIT 1
            ''', (analysis_id, chart_type))
            
            result = cursor.fetchone()
            return result[0] if result else None
    
    def get_summary_statistics(self) -> Dict[str, Any]:
        """
        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¦‚è¦çµ±è¨ˆã‚’å–å¾—
        
        Returns:
            Dict: çµ±è¨ˆæƒ…å ±
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # åŸºæœ¬çµ±è¨ˆ
            cursor.execute('SELECT COUNT(*) FROM analysis_results')
            total_analyses = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT symbol) FROM analysis_results')
            unique_symbols = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(*) FROM analysis_results WHERE is_usable = 1')
            usable_analyses = cursor.fetchone()[0]
            
            # æœ€æ–°åˆ†ææ—¥
            cursor.execute('SELECT MAX(analysis_date) FROM analysis_results')
            latest_analysis = cursor.fetchone()[0]
            
            # å“è³ªåˆ¥çµ±è¨ˆ
            cursor.execute('''
                SELECT quality, COUNT(*) 
                FROM analysis_results 
                GROUP BY quality
            ''')
            quality_stats = dict(cursor.fetchall())
            
            # RÂ²çµ±è¨ˆ
            cursor.execute('''
                SELECT 
                    AVG(r_squared) as avg_r_squared,
                    MIN(r_squared) as min_r_squared,
                    MAX(r_squared) as max_r_squared
                FROM analysis_results 
                WHERE r_squared IS NOT NULL
            ''')
            r_squared_stats = cursor.fetchone()
            
            return {
                'total_analyses': total_analyses,
                'unique_symbols': unique_symbols,
                'usable_analyses': usable_analyses,
                'usable_rate': usable_analyses / max(total_analyses, 1),
                'latest_analysis': latest_analysis,
                'quality_distribution': quality_stats,
                'r_squared_stats': {
                    'average': r_squared_stats[0] if r_squared_stats[0] else 0,
                    'minimum': r_squared_stats[1] if r_squared_stats[1] else 0,
                    'maximum': r_squared_stats[2] if r_squared_stats[2] else 0
                }
            }
    
    def get_filtered_analyses(self, 
                            symbol: str = None,
                            min_r_squared: float = None,
                            max_r_squared: float = None,
                            min_confidence: float = None,
                            max_confidence: float = None,
                            is_usable: bool = None,
                            basis_date_from: str = None,
                            basis_date_to: str = None,
                            predicted_crash_from: str = None,
                            predicted_crash_to: str = None,
                            quality_levels: List[str] = None,
                            sort_by: str = 'analysis_basis_date',
                            sort_order: str = 'DESC',
                            limit: int = 500) -> pd.DataFrame:
        """
        é«˜åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã«ã‚ˆã‚‹ã‚¯ã‚¨ãƒªæœ€é©åŒ–åˆ†æçµæœå–å¾—
        2025-08-11è¿½åŠ : ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å¤šæ¡ä»¶ANDæ¤œç´¢å¯¾å¿œ
        
        Args:
            symbol: ç‰¹å®šéŠ˜æŸ„ãƒ•ã‚£ãƒ«ã‚¿
            min_r_squared: RÂ²æœ€å°å€¤
            max_r_squared: RÂ²æœ€å¤§å€¤  
            min_confidence: ä¿¡é ¼åº¦æœ€å°å€¤
            max_confidence: ä¿¡é ¼åº¦æœ€å¤§å€¤
            is_usable: ä½¿ç”¨å¯èƒ½æ€§ãƒ•ã‚£ãƒ«ã‚¿
            basis_date_from: åˆ†æåŸºæº–æ—¥é–‹å§‹
            basis_date_to: åˆ†æåŸºæº–æ—¥çµ‚äº†
            predicted_crash_from: äºˆæ¸¬æ—¥é–‹å§‹
            predicted_crash_to: äºˆæ¸¬æ—¥çµ‚äº†
            quality_levels: å“è³ªãƒ¬ãƒ™ãƒ«ãƒªã‚¹ãƒˆ
            sort_by: ã‚½ãƒ¼ãƒˆåŸºæº–ï¼ˆanalysis_basis_date, r_squared, confidenceç­‰ï¼‰
            sort_order: ã‚½ãƒ¼ãƒˆé †åºï¼ˆASC, DESCï¼‰
            limit: çµæœä»¶æ•°åˆ¶é™
            
        Returns:
            DataFrame: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿åˆ†æçµæœ
        """
        with sqlite3.connect(self.db_path) as conn:
            query = '''
                SELECT 
                    id, symbol, analysis_date, data_source,
                    data_period_start, data_period_end, data_points,
                    tc, beta, omega, phi, A, B, C,
                    r_squared, rmse, quality, confidence, is_usable,
                    predicted_crash_date, days_to_crash,
                    window_days, total_candidates, successful_candidates,
                    analysis_basis_date, analysis_frequency
                FROM analysis_results
                WHERE 1=1
            '''
            params = []
            
            # å‹•çš„WHEREå¥æ§‹ç¯‰ï¼ˆANDæ¤œç´¢ï¼‰
            if symbol:
                query += ' AND symbol = ?'
                params.append(symbol)
                
            if min_r_squared is not None:
                query += ' AND r_squared >= ?'
                params.append(min_r_squared)
                
            if max_r_squared is not None:
                query += ' AND r_squared <= ?'
                params.append(max_r_squared)
                
            if min_confidence is not None:
                query += ' AND confidence >= ?'
                params.append(min_confidence)
                
            if max_confidence is not None:
                query += ' AND confidence <= ?'
                params.append(max_confidence)
                
            if is_usable is not None:
                query += ' AND is_usable = ?'
                params.append(is_usable)
                
            if basis_date_from:
                query += ' AND analysis_basis_date >= ?'
                params.append(basis_date_from)
                
            if basis_date_to:
                query += ' AND analysis_basis_date <= ?'
                params.append(basis_date_to)
                
            if predicted_crash_from:
                query += ' AND predicted_crash_date >= ?'
                params.append(predicted_crash_from)
                
            if predicted_crash_to:
                query += ' AND predicted_crash_date <= ?'
                params.append(predicted_crash_to)
                
            if quality_levels:
                placeholders = ','.join(['?' for _ in quality_levels])
                query += f' AND quality IN ({placeholders})'
                params.extend(quality_levels)
            
            # ã‚½ãƒ¼ãƒˆé †åºè¨­å®šï¼ˆåˆ†æåŸºæº–æ—¥å„ªå…ˆåŸå‰‡ï¼‰
            valid_sort_columns = ['analysis_basis_date', 'r_squared', 'confidence', 'symbol', 'predicted_crash_date']
            if sort_by not in valid_sort_columns:
                sort_by = 'analysis_basis_date'
            
            sort_order = sort_order.upper() if sort_order.upper() in ['ASC', 'DESC'] else 'DESC'
            query += f' ORDER BY {sort_by} {sort_order}, analysis_basis_date DESC'
            
            # çµæœä»¶æ•°åˆ¶é™
            if limit:
                query += ' LIMIT ?'
                params.append(limit)
            
            return pd.read_sql_query(query, conn, params=params)
    
    def get_filter_value_ranges(self) -> Dict[str, Any]:
        """
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç”¨ã®å€¤åŸŸæƒ…å ±ã‚’å–å¾—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ UI å‘ã‘ï¼‰
        2025-08-11è¿½åŠ : ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è‡ªå‹•è¨­å®š
        
        Returns:
            Dict: å„ã‚«ãƒ©ãƒ ã®æœ€å°ãƒ»æœ€å¤§å€¤æƒ…å ±
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # RÂ²å€¤åŸŸ
            cursor.execute('SELECT MIN(r_squared), MAX(r_squared) FROM analysis_results WHERE r_squared IS NOT NULL')
            r_squared_range = cursor.fetchone()
            
            # ä¿¡é ¼åº¦å€¤åŸŸ
            cursor.execute('SELECT MIN(confidence), MAX(confidence) FROM analysis_results WHERE confidence IS NOT NULL')
            confidence_range = cursor.fetchone()
            
            # åˆ†æåŸºæº–æ—¥å€¤åŸŸ
            cursor.execute('SELECT MIN(analysis_basis_date), MAX(analysis_basis_date) FROM analysis_results WHERE analysis_basis_date IS NOT NULL')
            basis_date_range = cursor.fetchone()
            
            # äºˆæ¸¬æ—¥å€¤åŸŸ
            cursor.execute('SELECT MIN(predicted_crash_date), MAX(predicted_crash_date) FROM analysis_results WHERE predicted_crash_date IS NOT NULL')
            predicted_crash_range = cursor.fetchone()
            
            # å“è³ªãƒ¬ãƒ™ãƒ«ä¸€è¦§
            cursor.execute('SELECT DISTINCT quality FROM analysis_results WHERE quality IS NOT NULL ORDER BY quality')
            quality_levels = [row[0] for row in cursor.fetchall()]
            
            # éŠ˜æŸ„ä¸€è¦§ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ï¼‰
            cursor.execute('SELECT DISTINCT symbol FROM analysis_results ORDER BY symbol')
            symbols = [row[0] for row in cursor.fetchall()]
            
            return {
                'r_squared': {
                    'min': r_squared_range[0] if r_squared_range[0] else 0.0,
                    'max': r_squared_range[1] if r_squared_range[1] else 1.0,
                },
                'confidence': {
                    'min': confidence_range[0] if confidence_range[0] else 0.0,
                    'max': confidence_range[1] if confidence_range[1] else 1.0,
                },
                'basis_date': {
                    'min': basis_date_range[0],
                    'max': basis_date_range[1],
                },
                'predicted_crash_date': {
                    'min': predicted_crash_range[0],
                    'max': predicted_crash_range[1],
                },
                'quality_levels': quality_levels,
                'symbols': symbols
            }
    
    def validate_database_compatibility(self) -> Dict[str, bool]:
        """
        æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§æ¤œè¨¼ï¼ˆ2025-08-11è¿½åŠ ï¼‰
        æ–°ã—ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ãŒæ—¢å­˜ãƒ¡ã‚½ãƒƒãƒ‰ã«å½±éŸ¿ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
        
        Returns:
            Dict: å„æ©Ÿèƒ½ã®å‹•ä½œçŠ¶æ³
        """
        compatibility_results = {}
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # 1. åŸºæœ¬ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ç¢ºèª
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                compatibility_results['basic_tables'] = 'analysis_results' in tables
                
                # 2. å¿…é ˆã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèª
                cursor.execute("PRAGMA table_info(analysis_results)")
                columns = [row[1] for row in cursor.fetchall()]
                required_columns = ['symbol', 'r_squared', 'confidence', 'analysis_basis_date']
                compatibility_results['required_columns'] = all(col in columns for col in required_columns)
                
                # 3. get_recent_analysesäº’æ›æ€§
                recent_df = self.get_recent_analyses(limit=1)
                compatibility_results['get_recent_analyses'] = len(recent_df) >= 0
                
                # 4. get_summary_statisticsäº’æ›æ€§
                stats = self.get_summary_statistics()
                compatibility_results['get_summary_statistics'] = 'total_analyses' in stats
                
                # 5. æ–°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å­˜åœ¨ç¢ºèª
                cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name='idx_quality_comprehensive'")
                compatibility_results['new_indexes'] = bool(cursor.fetchone())
                
                print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹äº’æ›æ€§æ¤œè¨¼å®Œäº†:")
                for feature, status in compatibility_results.items():
                    status_icon = "âœ…" if status else "âŒ"
                    print(f"  {status_icon} {feature}: {status}")
                    
        except Exception as e:
            print(f"âŒ äº’æ›æ€§æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            compatibility_results['error'] = str(e)
        
        return compatibility_results
    
    def get_filter_presets(self) -> Dict[str, Dict[str, Any]]:
        """
        ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆå®šç¾©ï¼ˆ2025-08-11è¿½åŠ ï¼‰
        ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
        
        Returns:
            Dict: ãƒ—ãƒªã‚»ãƒƒãƒˆåã‚’ã‚­ãƒ¼ã¨ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
        """
        return {
            "High Quality Only": {
                "min_r_squared": 0.85,
                "min_confidence": 0.80,
                "is_usable": True,
                "description": "RÂ² â‰¥ 0.85, Confidence â‰¥ 80%, Usable analyses only"
            },
            "Medium Quality+": {
                "min_r_squared": 0.70,
                "min_confidence": 0.60,
                "is_usable": True,
                "description": "RÂ² â‰¥ 0.70, Confidence â‰¥ 60%, Usable analyses only"
            },
            "Critical (30 days)": {
                "is_usable": True,
                "predicted_crash_from": datetime.now().strftime('%Y-%m-%d'),
                "predicted_crash_to": (datetime.now() + pd.DateOffset(days=30)).strftime('%Y-%m-%d'),
                "description": "Crash predictions within 30 days (Critical priority)"
            },
            "Near-term Predictions": {
                "is_usable": True,
                "predicted_crash_from": datetime.now().strftime('%Y-%m-%d'),
                "predicted_crash_to": (datetime.now() + pd.DateOffset(days=90)).strftime('%Y-%m-%d'),
                "description": "Crash predictions within 3 months (High priority)"
            },
            "Medium-term Predictions": {
                "is_usable": True,
                "predicted_crash_from": datetime.now().strftime('%Y-%m-%d'),
                "predicted_crash_to": (datetime.now() + pd.DateOffset(days=180)).strftime('%Y-%m-%d'),
                "description": "Crash predictions within 6 months (Medium priority)"
            },
            "Recent Analyses": {
                "basis_date_from": (datetime.now() - pd.DateOffset(days=30)).strftime('%Y-%m-%d'),
                "description": "Analyses performed in the last 30 days"
            },
            "Ultra-High Precision": {
                "min_r_squared": 0.90,
                "min_confidence": 0.85,
                "is_usable": True,
                "description": "Ultra-high precision analyses (RÂ² â‰¥ 0.90, Confidence â‰¥ 85%)"
            },
            "All Usable": {
                "is_usable": True,
                "description": "All analyses marked as usable"
            },
            "Latest 100": {
                "limit": 100,
                "description": "Latest 100 analyses by basis date"
            }
        }
    
    def apply_filter_preset(self, preset_name: str) -> pd.DataFrame:
        """
        ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨ã—ã¦çµæœå–å¾—
        2025-08-11è¿½åŠ : ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        
        Args:
            preset_name: ãƒ—ãƒªã‚»ãƒƒãƒˆå
            
        Returns:
            DataFrame: ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨æ¸ˆã¿åˆ†æçµæœ
        """
        presets = self.get_filter_presets()
        
        if preset_name not in presets:
            print(f"âŒ ãƒ—ãƒªã‚»ãƒƒãƒˆ '{preset_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return pd.DataFrame()
        
        preset_config = presets[preset_name]
        description = preset_config.pop('description', preset_name)
        
        print(f"ğŸ” ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨ä¸­: {preset_name} - {description}")
        
        return self.get_filtered_analyses(**preset_config)
    
    def cleanup_old_records(self, days_to_keep: int = 90):
        """
        å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            days_to_keep: ä¿æŒæ—¥æ•°
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            cursor.execute('''
                DELETE FROM analysis_results 
                WHERE analysis_date < datetime('now', '-{} days')
            '''.format(days_to_keep))
            
            deleted_count = cursor.rowcount
            conn.commit()
            
            print(f"ğŸ“Š å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰å‰Šé™¤: {deleted_count}ä»¶")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
def test_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
    db = ResultsDatabase("results/test_analysis.db")
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    sample_result = {
        'symbol': 'TEST_NASDAQ',
        'data_source': 'fred',
        'data_period_start': '2024-01-01',
        'data_period_end': '2024-12-31',
        'data_points': 250,
        'tc': 1.15,
        'beta': 0.33,
        'omega': 7.4,
        'phi': 0.1,
        'A': 10.0,
        'B': -1.0,
        'C': 0.1,
        'r_squared': 0.95,
        'rmse': 0.01,
        'quality': 'high_quality',
        'confidence': 0.92,
        'is_usable': True,
        'predicted_crash_date': '2025-01-15',
        'days_to_crash': 45,
        'fitting_method': 'multi_criteria',
        'window_days': 365,
        'total_candidates': 50,
        'successful_candidates': 35
    }
    
    # ä¿å­˜ãƒ†ã‚¹ãƒˆ
    analysis_id = db.save_analysis_result(sample_result)
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜: ID={analysis_id}")
    
    # å–å¾—ãƒ†ã‚¹ãƒˆ
    recent = db.get_recent_analyses(limit=5)
    print(f"âœ… æœ€è¿‘ã®åˆ†æå–å¾—: {len(recent)}ä»¶")
    
    # çµ±è¨ˆãƒ†ã‚¹ãƒˆ
    stats = db.get_summary_statistics()
    print(f"âœ… çµ±è¨ˆæƒ…å ±: ç·åˆ†ææ•°={stats['total_analyses']}, ä½¿ç”¨å¯èƒ½ç‡={stats['usable_rate']:.1%}")
    
    return db


if __name__ == "__main__":
    test_database()