#!/usr/bin/env python3
"""
è§£æé‡è¤‡é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ 
- åŒä¸€éŠ˜æŸ„ãƒ»åŒä¸€åŸºæº–æ—¥ã§ã®é‡è¤‡è§£æé˜²æ­¢
- å¤±æ•—è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
- backfillbatchå®Ÿè¡Œæ™‚ã®åŠ¹ç‡åŒ–
"""

import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from infrastructure.database.fitting_failure_tracker import FittingFailureTracker

class AnalysisDeduplicationSystem:
    """è§£æé‡è¤‡é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: Optional[str] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            db_path: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹
        """
        self.failure_tracker = FittingFailureTracker(db_path)
        self.db_path = self.failure_tracker.db_path
    
    def filter_analysis_candidates(self, symbol_date_pairs: List[Tuple[str, str]], 
                                 schedule_name: str = None,
                                 force_retry: bool = False,
                                 max_retry_count: int = 2) -> Dict[str, List[Tuple[str, str]]]:
        """
        è§£æå€™è£œã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡æ’é™¤ï¼‰
        
        Args:
            symbol_date_pairs: [(symbol, analysis_basis_date), ...] ã®ãƒªã‚¹ãƒˆ
            schedule_name: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å
            force_retry: å¤±æ•—ã—ãŸã‚±ãƒ¼ã‚¹ã‚‚å¼·åˆ¶å†å®Ÿè¡Œ
            max_retry_count: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
            
        Returns:
            dict: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
                - 'to_analyze': è§£æãŒå¿…è¦ãªéŠ˜æŸ„ãƒ»æ—¥ä»˜ãƒšã‚¢
                - 'to_skip': ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹éŠ˜æŸ„ãƒ»æ—¥ä»˜ãƒšã‚¢
                - 'to_retry': ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹éŠ˜æŸ„ãƒ»æ—¥ä»˜ãƒšã‚¢
                - 'already_successful': æ—¢ã«æˆåŠŸæ¸ˆã¿ã®éŠ˜æŸ„ãƒ»æ—¥ä»˜ãƒšã‚¢
        """
        
        print("ğŸ” è§£æå€™è£œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹")
        print("=" * 50)
        
        results = {
            'to_analyze': [],      # æ–°è¦è§£æãŒå¿…è¦
            'to_skip': [],         # ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæˆåŠŸæ¸ˆã¿ãƒ»æ°¸ç¶šçš„å¤±æ•—ï¼‰
            'to_retry': [],        # ãƒªãƒˆãƒ©ã‚¤å¯èƒ½ãªå¤±æ•—
            'already_successful': [] # æ—¢ã«æˆåŠŸæ¸ˆã¿
        }
        
        print(f"ğŸ“‹ å…¥åŠ›å€™è£œæ•°: {len(symbol_date_pairs)}")
        print(f"ğŸ“‹ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: {schedule_name or 'æœªæŒ‡å®š'}")
        print(f"ğŸ“‹ å¼·åˆ¶ãƒªãƒˆãƒ©ã‚¤: {'æœ‰åŠ¹' if force_retry else 'ç„¡åŠ¹'}")
        
        for i, (symbol, analysis_date) in enumerate(symbol_date_pairs, 1):
            print(f"\\n{i:3d}. {symbol:10} ({analysis_date})", end=" ")
            
            # è§£æå¿…è¦æ€§ãƒã‚§ãƒƒã‚¯
            check_result = self.failure_tracker.check_analysis_needed(
                symbol, analysis_date, schedule_name
            )
            
            action = check_result['action']
            reason = check_result['reason']
            status = check_result.get('status')
            
            if action == 'full_analysis':
                results['to_analyze'].append((symbol, analysis_date))
                print(f"â†’ ğŸ†• è§£æå¿…è¦ ({reason})")
                
            elif action == 'retry_fitting_only':
                if force_retry or check_result.get('retry_count', 0) < max_retry_count:
                    results['to_retry'].append((symbol, analysis_date))
                    retry_count = check_result.get('retry_count', 0)
                    print(f"â†’ ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ ({retry_count}å›ç›®)")
                else:
                    results['to_skip'].append((symbol, analysis_date))
                    print(f"â†’ â­ï¸ ã‚¹ã‚­ãƒƒãƒ— (ãƒªãƒˆãƒ©ã‚¤ä¸Šé™)")
                    
            elif action == 'skip':
                if reason == 'already_successful':
                    results['already_successful'].append((symbol, analysis_date))
                    print(f"â†’ âœ… æˆåŠŸæ¸ˆã¿")
                else:
                    results['to_skip'].append((symbol, analysis_date))
                    print(f"â†’ â­ï¸ ã‚¹ã‚­ãƒƒãƒ— ({reason})")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\\n" + "=" * 50)
        print("ğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        
        print(f"ğŸ†• æ–°è¦è§£æ: {len(results['to_analyze'])}ä»¶")
        print(f"ğŸ”„ ãƒªãƒˆãƒ©ã‚¤: {len(results['to_retry'])}ä»¶")
        print(f"âœ… æˆåŠŸæ¸ˆã¿: {len(results['already_successful'])}ä»¶")
        print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {len(results['to_skip'])}ä»¶")
        
        total_work = len(results['to_analyze']) + len(results['to_retry'])
        total_input = len(symbol_date_pairs)
        efficiency = (total_input - total_work) / total_input * 100 if total_input > 0 else 0
        
        print(f"\\nğŸ“Š åŠ¹ç‡åŒ–:")
        print(f"  å®Ÿè¡Œå¿…è¦: {total_work}/{total_input} ({total_work/total_input*100:.1f}%)")
        print(f"  é‡è¤‡æ’é™¤: {efficiency:.1f}%")
        
        return results
    
    def generate_backfill_execution_plan(self, symbols: List[str], 
                                       start_date: str = "2024-01-01",
                                       schedule_name: str = None,
                                       force_retry: bool = False) -> Dict[str, Any]:
        """
        backfillbatchå®Ÿè¡Œè¨ˆç”»ã®ç”Ÿæˆ
        
        Args:
            symbols: å¯¾è±¡éŠ˜æŸ„ãƒªã‚¹ãƒˆ
            start_date: é–‹å§‹æ—¥
            schedule_name: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å
            force_retry: å¼·åˆ¶ãƒªãƒˆãƒ©ã‚¤
            
        Returns:
            dict: å®Ÿè¡Œè¨ˆç”»
        """
        
        print("ğŸš€ Backfillbatchå®Ÿè¡Œè¨ˆç”»ç”Ÿæˆ")
        print("=" * 50)
        
        # é€±æ¬¡åŸºæº–æ—¥ã®ç”Ÿæˆï¼ˆé‡‘æ›œæ—¥åŸºæº–ï¼‰
        from datetime import datetime, timedelta
        
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.now()
        
        # æœ€åˆã®é‡‘æ›œæ—¥ã‚’è¦‹ã¤ã‘ã‚‹
        days_to_friday = (4 - start_dt.weekday()) % 7  # 4 = Friday (0=Monday)
        first_friday = start_dt + timedelta(days=days_to_friday)
        
        # é€±æ¬¡åŸºæº–æ—¥ãƒªã‚¹ãƒˆã®ç”Ÿæˆ
        basis_dates = []
        current_friday = first_friday
        while current_friday <= end_dt:
            basis_dates.append(current_friday.strftime('%Y-%m-%d'))
            current_friday += timedelta(weeks=1)
        
        print(f"ğŸ“… æœŸé–“: {start_date} ï½ {end_dt.strftime('%Y-%m-%d')}")
        print(f"ğŸ“… é€±æ¬¡åŸºæº–æ—¥: {len(basis_dates)}é€±åˆ†")
        print(f"ğŸ“‹ å¯¾è±¡éŠ˜æŸ„: {len(symbols)}éŠ˜æŸ„")
        
        # éŠ˜æŸ„Ã—åŸºæº–æ—¥ã®ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›
        all_combinations = []
        for symbol in symbols:
            for basis_date in basis_dates:
                all_combinations.append((symbol, basis_date))
        
        print(f"ğŸ“‹ ç·çµ„ã¿åˆã‚ã›æ•°: {len(all_combinations)}ä»¶")
        
        # é‡è¤‡æ’é™¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_results = self.filter_analysis_candidates(
            all_combinations, schedule_name, force_retry
        )
        
        # å®Ÿè¡Œè¨ˆç”»ã®æ§‹ç¯‰
        execution_plan = {
            'metadata': {
                'plan_generated_at': datetime.now().isoformat(),
                'target_symbols': symbols,
                'start_date': start_date,
                'end_date': end_dt.strftime('%Y-%m-%d'),
                'total_weeks': len(basis_dates),
                'schedule_name': schedule_name,
                'force_retry': force_retry
            },
            'statistics': {
                'total_combinations': len(all_combinations),
                'new_analyses': len(filtered_results['to_analyze']),
                'retries': len(filtered_results['to_retry']),
                'already_successful': len(filtered_results['already_successful']),
                'skipped': len(filtered_results['to_skip']),
                'efficiency_percent': (len(all_combinations) - len(filtered_results['to_analyze']) - len(filtered_results['to_retry'])) / len(all_combinations) * 100 if all_combinations else 0
            },
            'execution_batches': {
                'new_analyses': filtered_results['to_analyze'],
                'retries': filtered_results['to_retry']
            },
            'skip_details': {
                'already_successful': filtered_results['already_successful'],
                'permanent_skips': filtered_results['to_skip']
            }
        }
        
        # å®Ÿè¡Œå„ªå…ˆåº¦ä»˜ã‘ï¼ˆä¸»è¦éŠ˜æŸ„å„ªå…ˆã€æœ€æ–°ãƒ‡ãƒ¼ã‚¿å„ªå…ˆï¼‰
        major_symbols = {'BTC', 'ETH', 'SP500', 'NASDAQ', 'USDT', 'USDC'}
        
        def prioritize_analyses(analyses):
            """è§£æãƒªã‚¹ãƒˆã®å„ªå…ˆåº¦ä»˜ã‘"""
            priority_scores = []
            
            for symbol, date in analyses:
                score = 0
                
                # ä¸»è¦éŠ˜æŸ„ãƒœãƒ¼ãƒŠã‚¹
                if symbol in major_symbols:
                    score += 100
                
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã»ã©é«˜ã‚¹ã‚³ã‚¢
                date_dt = datetime.strptime(date, '%Y-%m-%d')
                days_old = (datetime.now() - date_dt).days
                score += max(0, 365 - days_old) / 365 * 50
                
                priority_scores.append((score, symbol, date))
            
            # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
            priority_scores.sort(reverse=True)
            return [(symbol, date) for _, symbol, date in priority_scores]
        
        execution_plan['execution_batches']['new_analyses'] = prioritize_analyses(
            filtered_results['to_analyze']
        )
        execution_plan['execution_batches']['retries'] = prioritize_analyses(
            filtered_results['to_retry']
        )
        
        return execution_plan
    
    def save_execution_plan(self, plan: Dict[str, Any], filename: str = None) -> str:
        """
        å®Ÿè¡Œè¨ˆç”»ã®ä¿å­˜
        
        Args:
            plan: å®Ÿè¡Œè¨ˆç”»
            filename: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæœªæŒ‡å®šæ™‚ã¯è‡ªå‹•ç”Ÿæˆï¼‰
            
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            schedule_name = plan['metadata'].get('schedule_name', 'unknown')
            filename = f"backfill_plan_{schedule_name}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(plan, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å®Ÿè¡Œè¨ˆç”»ä¿å­˜: {filename}")
        return filename
    
    def generate_execution_commands(self, plan: Dict[str, Any]) -> List[str]:
        """
        å®Ÿè¡Œè¨ˆç”»ã‹ã‚‰ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç”Ÿæˆ
        
        Args:
            plan: å®Ÿè¡Œè¨ˆç”»
            
        Returns:
            list: å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ãƒªã‚¹ãƒˆ
        """
        
        commands = []
        
        # æ–°è¦è§£æã‚³ãƒãƒ³ãƒ‰
        new_analyses = plan['execution_batches']['new_analyses']
        if new_analyses:
            # éŠ˜æŸ„ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            symbol_groups = {}
            for symbol, date in new_analyses:
                if symbol not in symbol_groups:
                    symbol_groups[symbol] = []
                symbol_groups[symbol].append(date)
            
            schedule_name = plan['metadata'].get('schedule_name', 'manual_backfill')
            
            for symbol, dates in symbol_groups.items():
                if len(dates) > 10:  # å¤šæ•°ã®å ´åˆã¯ç¯„å›²æŒ‡å®š
                    start_date = min(dates)
                    end_date = max(dates)
                    cmd = f"python entry_points/main.py scheduled-analysis backfill --symbols {symbol} --start {start_date} --end {end_date} --schedule-name {schedule_name}"
                else:  # å°‘æ•°ã®å ´åˆã¯å€‹åˆ¥æŒ‡å®š
                    dates_str = ','.join(dates)
                    cmd = f"python entry_points/main.py scheduled-analysis backfill --symbols {symbol} --dates {dates_str} --schedule-name {schedule_name}"
                
                commands.append(cmd)
        
        # ãƒªãƒˆãƒ©ã‚¤ã‚³ãƒãƒ³ãƒ‰
        retries = plan['execution_batches']['retries']
        if retries:
            retry_symbols = list(set([symbol for symbol, _ in retries]))
            symbols_str = ','.join(retry_symbols)
            cmd = f"python entry_points/main.py scheduled-analysis retry --symbols {symbols_str} --max-retries 3"
            commands.append(cmd)
        
        return commands

def test_deduplication_system():
    """é‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    print("ğŸ§ª è§£æé‡è¤‡é˜²æ­¢ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    dedup = AnalysisDeduplicationSystem()
    
    # ãƒ†ã‚¹ãƒˆç”¨éŠ˜æŸ„ï¼ˆä¸è¶³ã—ã¦ã„ã‚‹CoinGeckoéŠ˜æŸ„ã‚’å«ã‚€ï¼‰
    test_symbols = ['BTC', 'ETH', 'USDT', 'USDC', 'MATIC']
    
    print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆå¯¾è±¡éŠ˜æŸ„: {test_symbols}")
    
    # å®Ÿè¡Œè¨ˆç”»ç”Ÿæˆ
    plan = dedup.generate_backfill_execution_plan(
        symbols=test_symbols,
        start_date="2025-07-01",
        schedule_name="missing_coingecko_test",
        force_retry=False
    )
    
    # çµæœè¡¨ç¤º
    print(f"\\nğŸ“Š å®Ÿè¡Œè¨ˆç”»çµ±è¨ˆ:")
    stats = plan['statistics']
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ
    commands = dedup.generate_execution_commands(plan)
    print(f"\\nğŸš€ ç”Ÿæˆã•ã‚ŒãŸå®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:")
    for i, cmd in enumerate(commands, 1):
        print(f"  {i}. {cmd}")
    
    # è¨ˆç”»ä¿å­˜
    plan_file = dedup.save_execution_plan(plan)
    
    return plan, plan_file

if __name__ == "__main__":
    test_deduplication_system()