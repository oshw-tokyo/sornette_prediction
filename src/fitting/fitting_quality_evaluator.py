#!/usr/bin/env python3
"""
ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¢ƒç•Œå¼µã‚Šä»˜ãã‚„ãã®ä»–ã®ç•°å¸¸ã‚’æ¤œå‡ºã—ã€
ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®å“è³ªã‚’è©•ä¾¡ãƒ»åˆ†é¡ã™ã‚‹
"""

import numpy as np
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import warnings
warnings.filterwarnings('ignore')


class FittingQuality(Enum):
    """ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å“è³ªã®åˆ†é¡"""
    HIGH_QUALITY = "high_quality"              # é«˜å“è³ªãƒ•ã‚£ãƒƒãƒˆ
    ACCEPTABLE = "acceptable"                  # è¨±å®¹å¯èƒ½
    BOUNDARY_STUCK = "boundary_stuck"          # å¢ƒç•Œå¼µã‚Šä»˜ã
    POOR_CONVERGENCE = "poor_convergence"      # åæŸä¸è‰¯
    OVERFITTING = "overfitting"               # éå­¦ç¿’ç–‘ã„
    UNSTABLE = "unstable"                     # ä¸å®‰å®š
    FAILED = "failed"                         # å¤±æ•—
    CRITICAL_PROXIMITY = "critical_proximity"  # è‡¨ç•Œç‚¹æ¥µè¿‘ï¼ˆå®Ÿéš›ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ç›´å‰ï¼‰


@dataclass
class QualityAssessment:
    """å“è³ªè©•ä¾¡çµæœ"""
    quality: FittingQuality
    confidence: float  # 0-1ã®ä¿¡é ¼åº¦
    issues: List[str]  # æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã®ãƒªã‚¹ãƒˆ
    metadata: Dict[str, Any]  # è¿½åŠ ãƒ¡ã‚¿æƒ…å ±
    is_usable: bool  # äºˆæ¸¬ã«ä½¿ç”¨å¯èƒ½ã‹
    
    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'quality': self.quality.value,
            'confidence': self.confidence,
            'issues': self.issues,
            'metadata': self.metadata,
            'is_usable': self.is_usable
        }


class FittingQualityEvaluator:
    """ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å“è³ªè©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç†è«–çš„ç¯„å›²
        self.parameter_ranges = {
            'tc': {'min': 1.01, 'max': 3.0, 'theoretical_min': 1.05, 'theoretical_max': 2.0},
            'beta': {'min': 0.05, 'max': 1.0, 'theoretical_min': 0.2, 'theoretical_max': 0.7},
            'omega': {'min': 1.0, 'max': 20.0, 'theoretical_min': 3.0, 'theoretical_max': 15.0},
            'A': {'min': -10, 'max': 10},
            'B': {'min': -10, 'max': 10},
            'C': {'min': -1.0, 'max': 1.0},
            'phi': {'min': -2*np.pi, 'max': 2*np.pi}
        }
        
        # å¢ƒç•Œå¼µã‚Šä»˜ãåˆ¤å®šã®é–¾å€¤
        self.boundary_tolerance = 0.001  # å¢ƒç•Œã‹ã‚‰ã“ã®ç¯„å›²å†…ãªã‚‰å¼µã‚Šä»˜ãã¨åˆ¤å®š
        
        # å“è³ªåˆ¤å®šã®é–¾å€¤
        self.quality_thresholds = {
            'r_squared': {'high': 0.9, 'acceptable': 0.7, 'poor': 0.5},
            'rmse': {'high': 0.05, 'acceptable': 0.1, 'poor': 0.2},
            'parameter_stability': {'high': 0.9, 'acceptable': 0.7, 'poor': 0.5}
        }
    
    def evaluate_fitting(self, 
                        parameters: Dict[str, float],
                        statistics: Dict[str, float],
                        bounds: Optional[Tuple] = None,
                        initial_params: Optional[List[float]] = None,
                        convergence_info: Optional[Dict] = None) -> QualityAssessment:
        """
        ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®ç·åˆè©•ä¾¡
        
        Args:
            parameters: ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (tc, beta, omegaç­‰)
            statistics: çµ±è¨ˆé‡ (r_squared, rmseç­‰)
            bounds: ä½¿ç”¨ã•ã‚ŒãŸå¢ƒç•Œæ¡ä»¶
            initial_params: åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            convergence_info: åæŸæƒ…å ±
        
        Returns:
            QualityAssessment: å“è³ªè©•ä¾¡çµæœ
        """
        
        issues = []
        metadata = {}
        quality_scores = []
        
        # 1. å¢ƒç•Œå¼µã‚Šä»˜ããƒã‚§ãƒƒã‚¯
        boundary_check = self._check_boundary_sticking(parameters, bounds)
        if boundary_check['has_boundary_issues']:
            issues.extend(boundary_check['issues'])
            metadata['boundary_stuck_params'] = boundary_check['stuck_params']
        quality_scores.append(boundary_check['score'])
        
        # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        param_check = self._check_parameter_validity(parameters)
        if param_check['has_issues']:
            issues.extend(param_check['issues'])
            metadata['parameter_warnings'] = param_check['warnings']
        quality_scores.append(param_check['score'])
        
        # 3. çµ±è¨ˆçš„å“è³ªãƒã‚§ãƒƒã‚¯
        stat_check = self._check_statistical_quality(statistics)
        if stat_check['has_issues']:
            issues.extend(stat_check['issues'])
            metadata['statistical_metrics'] = stat_check['metrics']
        quality_scores.append(stat_check['score'])
        
        # 4. éå­¦ç¿’ãƒã‚§ãƒƒã‚¯
        overfit_check = self._check_overfitting(parameters, statistics)
        if overfit_check['is_overfitted']:
            issues.append("éå­¦ç¿’ã®ç–‘ã„")
            metadata['overfitting_indicators'] = overfit_check['indicators']
        quality_scores.append(overfit_check['score'])
        
        # 5. åæŸå“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆæƒ…å ±ãŒã‚ã‚‹å ´åˆï¼‰
        if convergence_info:
            conv_check = self._check_convergence_quality(convergence_info)
            if conv_check['has_issues']:
                issues.extend(conv_check['issues'])
                metadata['convergence_metrics'] = conv_check['metrics']
            quality_scores.append(conv_check['score'])
        
        # ç·åˆè©•ä¾¡
        overall_score = np.mean(quality_scores)
        quality = self._determine_quality_level(overall_score, issues, boundary_check)
        confidence = self._calculate_confidence(overall_score, quality, issues)
        is_usable = self._determine_usability(quality, parameters, statistics)
        
        # ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        if quality == FittingQuality.CRITICAL_PROXIMITY:
            metadata.update({
                'fitting_error_warning': True,
                'interpretation': 'ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ï¼štcä¸‹é™å¢ƒç•Œå¼µã‚Šä»˜ã',
                'primary_cause': 'ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã®æ•°å€¤çš„å•é¡Œ',
                'usage_note': 'äºˆæ¸¬ã«ã¯ä½¿ç”¨ä¸å¯ï¼šãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å¤±æ•—',
                'recommended_action': 'ç•°ãªã‚‹åˆæœŸå€¤ãƒ»æœŸé–“ãƒ»æ‰‹æ³•ã§ã®å†è©¦è¡Œ',
                'rare_possibility': 'æ¥µç¨€ã«å®Ÿéš›ã®è‡¨ç•Œç‚¹æ¥µè¿‘ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŒã€é€šå¸¸ã¯ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼'
            })
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        metadata.update({
            'overall_score': overall_score,
            'individual_scores': quality_scores,
            'parameter_values': parameters,
            'statistical_values': statistics
        })
        
        return QualityAssessment(
            quality=quality,
            confidence=confidence,
            issues=issues,
            metadata=metadata,
            is_usable=is_usable
        )
    
    def _check_boundary_sticking(self, parameters: Dict[str, float], 
                                bounds: Optional[Tuple] = None) -> Dict:
        """å¢ƒç•Œå¼µã‚Šä»˜ãã®ãƒã‚§ãƒƒã‚¯"""
        
        if bounds is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¢ƒç•Œã‚’ä½¿ç”¨
            lower_bounds = [self.parameter_ranges[p]['min'] for p in ['tc', 'beta', 'omega', 'phi', 'A', 'B', 'C']]
            upper_bounds = [self.parameter_ranges[p]['max'] for p in ['tc', 'beta', 'omega', 'phi', 'A', 'B', 'C']]
        else:
            lower_bounds, upper_bounds = bounds
        
        stuck_params = []
        issues = []
        
        param_order = ['tc', 'beta', 'omega', 'phi', 'A', 'B', 'C']
        
        for i, param_name in enumerate(param_order):
            if param_name in parameters:
                value = parameters[param_name]
                lower = lower_bounds[i] if i < len(lower_bounds) else self.parameter_ranges[param_name]['min']
                upper = upper_bounds[i] if i < len(upper_bounds) else self.parameter_ranges[param_name]['max']
                
                # å¢ƒç•Œã¨ã®è·é›¢ã‚’è¨ˆç®—
                lower_distance = abs(value - lower)
                upper_distance = abs(value - upper)
                range_size = upper - lower
                
                # ç›¸å¯¾çš„ãªè·é›¢ã§åˆ¤å®š
                relative_tolerance = self.boundary_tolerance * range_size
                
                if lower_distance < relative_tolerance:
                    stuck_params.append(f"{param_name}_lower")
                    issues.append(f"{param_name}ãŒä¸‹é™ã«å¼µã‚Šä»˜ã ({value:.6f} â‰ˆ {lower})")
                elif upper_distance < relative_tolerance:
                    stuck_params.append(f"{param_name}_upper")
                    issues.append(f"{param_name}ãŒä¸Šé™ã«å¼µã‚Šä»˜ã ({value:.6f} â‰ˆ {upper})")
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå¼µã‚Šä»˜ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«åŸºã¥ãï¼‰
        score = 1.0 - (len(stuck_params) / len(param_order))
        
        # tcå¼µã‚Šä»˜ãã¯ç‰¹ã«å•é¡Œ
        if any('tc' in sp for sp in stuck_params):
            score *= 0.5
            issues.insert(0, "âš ï¸ tcå€¤ã®å¢ƒç•Œå¼µã‚Šä»˜ãã¯æ·±åˆ»ãªå•é¡Œ")
        
        return {
            'has_boundary_issues': len(stuck_params) > 0,
            'stuck_params': stuck_params,
            'issues': issues,
            'score': score
        }
    
    def _check_parameter_validity(self, parameters: Dict[str, float]) -> Dict:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        
        issues = []
        warnings = []
        validity_scores = []
        
        for param_name, value in parameters.items():
            if param_name in self.parameter_ranges:
                ranges = self.parameter_ranges[param_name]
                
                # ç†è«–çš„ç¯„å›²ãƒã‚§ãƒƒã‚¯
                if 'theoretical_min' in ranges and 'theoretical_max' in ranges:
                    if value < ranges['theoretical_min'] or value > ranges['theoretical_max']:
                        warnings.append(f"{param_name}={value:.3f}ãŒç†è«–çš„ç¯„å›²å¤–")
                        validity_scores.append(0.5)
                    else:
                        # ç†è«–çš„ç¯„å›²å†…ã§ã®ç›¸å¯¾ä½ç½®
                        theoretical_range = ranges['theoretical_max'] - ranges['theoretical_min']
                        distance_from_center = abs(value - (ranges['theoretical_min'] + ranges['theoretical_max']) / 2)
                        validity_scores.append(1.0 - (distance_from_center / theoretical_range))
        
        # ç‰¹æ®Šãªãƒã‚§ãƒƒã‚¯
        if 'tc' in parameters:
            if parameters['tc'] > 2.5:
                issues.append("tcå€¤ãŒéç¾å®Ÿçš„ã«å¤§ãã„ï¼ˆ> 2.5ï¼‰")
            elif parameters['tc'] < 1.02:
                issues.append("tcå€¤ãŒéç¾å®Ÿçš„ã«å°ã•ã„ï¼ˆ< 1.02ï¼‰")
        
        if 'omega' in parameters:
            if parameters['omega'] < 2.0:
                warnings.append("Ï‰å€¤ãŒç•°å¸¸ã«å°ã•ã„ï¼ˆ< 2.0ï¼‰")
            elif parameters['omega'] > 15.0:
                warnings.append("Ï‰å€¤ãŒç•°å¸¸ã«å¤§ãã„ï¼ˆ> 15.0ï¼‰")
        
        score = np.mean(validity_scores) if validity_scores else 0.8
        
        return {
            'has_issues': len(issues) > 0,
            'issues': issues,
            'warnings': warnings,
            'score': score
        }
    
    def _check_statistical_quality(self, statistics: Dict[str, float]) -> Dict:
        """çµ±è¨ˆçš„å“è³ªã®ãƒã‚§ãƒƒã‚¯"""
        
        issues = []
        metrics = {}
        scores = []
        
        # RÂ²ãƒã‚§ãƒƒã‚¯
        if 'r_squared' in statistics:
            r2 = statistics['r_squared']
            metrics['r_squared'] = r2
            
            if r2 < self.quality_thresholds['r_squared']['poor']:
                issues.append(f"RÂ²ãŒéå¸¸ã«ä½ã„ ({r2:.3f})")
                scores.append(0.2)
            elif r2 < self.quality_thresholds['r_squared']['acceptable']:
                issues.append(f"RÂ²ãŒä½ã„ ({r2:.3f})")
                scores.append(0.5)
            elif r2 < self.quality_thresholds['r_squared']['high']:
                scores.append(0.8)
            else:
                scores.append(1.0)
        
        # RMSEãƒã‚§ãƒƒã‚¯
        if 'rmse' in statistics:
            rmse = statistics['rmse']
            metrics['rmse'] = rmse
            
            if rmse > self.quality_thresholds['rmse']['poor']:
                issues.append(f"RMSEãŒéå¸¸ã«é«˜ã„ ({rmse:.3f})")
                scores.append(0.2)
            elif rmse > self.quality_thresholds['rmse']['acceptable']:
                issues.append(f"RMSEãŒé«˜ã„ ({rmse:.3f})")
                scores.append(0.5)
            elif rmse > self.quality_thresholds['rmse']['high']:
                scores.append(0.8)
            else:
                scores.append(1.0)
        
        score = np.mean(scores) if scores else 0.5
        
        return {
            'has_issues': len(issues) > 0,
            'issues': issues,
            'metrics': metrics,
            'score': score
        }
    
    def _check_overfitting(self, parameters: Dict[str, float], 
                          statistics: Dict[str, float]) -> Dict:
        """éå­¦ç¿’ã®ãƒã‚§ãƒƒã‚¯"""
        
        indicators = []
        is_overfitted = False
        
        # é«˜RÂ²ã¨ç•°å¸¸ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›
        if 'r_squared' in statistics and statistics['r_squared'] > 0.95:
            # tcå¢ƒç•Œå¼µã‚Šä»˜ã
            if 'tc' in parameters and parameters['tc'] <= 1.01:
                indicators.append("é«˜RÂ²ã¨tcå¢ƒç•Œå¼µã‚Šä»˜ãã®çµ„ã¿åˆã‚ã›")
                is_overfitted = True
            
            # æ¥µç«¯ãªÎ²å€¤
            if 'beta' in parameters and (parameters['beta'] < 0.1 or parameters['beta'] > 0.9):
                indicators.append("é«˜RÂ²ã¨æ¥µç«¯ãªÎ²å€¤ã®çµ„ã¿åˆã‚ã›")
                is_overfitted = True
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨æ¯”è¼ƒã—ã¦ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ãŒå°‘ãªã„å ´åˆã®åˆ¤å®šã¯åˆ¥é€”å¿…è¦
        
        score = 0.2 if is_overfitted else 0.9
        
        return {
            'is_overfitted': is_overfitted,
            'indicators': indicators,
            'score': score
        }
    
    def _check_convergence_quality(self, convergence_info: Dict) -> Dict:
        """åæŸå“è³ªã®ãƒã‚§ãƒƒã‚¯"""
        
        issues = []
        metrics = {}
        
        # åæŸå›æ•°
        if 'iterations' in convergence_info:
            iterations = convergence_info['iterations']
            metrics['iterations'] = iterations
            
            if iterations > 900:
                issues.append(f"åæŸã«å¤šãã®åå¾©ãŒå¿…è¦ ({iterations}å›)")
        
        # åæŸåˆ¤å®š
        if 'converged' in convergence_info and not convergence_info['converged']:
            issues.append("åæŸåŸºæº–ã‚’æº€ãŸã•ãšçµ‚äº†")
        
        score = 0.5 if issues else 1.0
        
        return {
            'has_issues': len(issues) > 0,
            'issues': issues,
            'metrics': metrics,
            'score': score
        }
    
    def _determine_quality_level(self, overall_score: float, issues: List[str], 
                               boundary_check: Dict) -> FittingQuality:
        """å“è³ªãƒ¬ãƒ™ãƒ«ã®æ±ºå®š"""
        
        # å¢ƒç•Œå¼µã‚Šä»˜ããŒã‚ã‚‹å ´åˆã®è©³ç´°åˆ¤å®š
        if boundary_check['has_boundary_issues'] and 'tc' in str(boundary_check['stuck_params']):
            # tcä¸‹é™å¼µã‚Šä»˜ãã®å ´åˆã€è‡¨ç•Œç‚¹æ¥µè¿‘ã®å¯èƒ½æ€§ã‚’è€ƒæ…®
            if 'tc_lower' in boundary_check['stuck_params']:
                return FittingQuality.CRITICAL_PROXIMITY
            else:
                return FittingQuality.BOUNDARY_STUCK
        
        # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        if overall_score >= 0.9 and len(issues) == 0:
            return FittingQuality.HIGH_QUALITY
        elif overall_score >= 0.7 and len(issues) <= 2:
            return FittingQuality.ACCEPTABLE
        elif overall_score >= 0.5:
            if any("åæŸ" in issue for issue in issues):
                return FittingQuality.POOR_CONVERGENCE
            elif any("éå­¦ç¿’" in issue for issue in issues):
                return FittingQuality.OVERFITTING
            else:
                return FittingQuality.UNSTABLE
        else:
            return FittingQuality.FAILED
    
    def _calculate_confidence(self, overall_score: float, 
                            quality: FittingQuality, issues: List[str]) -> float:
        """ä¿¡é ¼åº¦ã®è¨ˆç®—"""
        
        base_confidence = overall_score
        
        # å“è³ªãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹èª¿æ•´
        quality_multipliers = {
            FittingQuality.HIGH_QUALITY: 1.0,
            FittingQuality.ACCEPTABLE: 0.8,
            FittingQuality.BOUNDARY_STUCK: 0.3,
            FittingQuality.POOR_CONVERGENCE: 0.4,
            FittingQuality.OVERFITTING: 0.2,
            FittingQuality.UNSTABLE: 0.3,
            FittingQuality.FAILED: 0.1,
            FittingQuality.CRITICAL_PROXIMITY: 0.95  # é«˜ä¿¡é ¼åº¦ã ãŒç‰¹æ®Šã‚±ãƒ¼ã‚¹
        }
        
        confidence = base_confidence * quality_multipliers.get(quality, 0.5)
        
        # å•é¡Œæ•°ã«ã‚ˆã‚‹è¿½åŠ æ¸›ç®—
        confidence *= (1.0 - 0.05 * len(issues))
        
        return max(0.0, min(1.0, confidence))
    
    def _determine_usability(self, quality: FittingQuality, 
                           parameters: Dict[str, float],
                           statistics: Dict[str, float]) -> bool:
        """äºˆæ¸¬ä½¿ç”¨å¯èƒ½æ€§ã®åˆ¤å®š"""
        
        # é€šå¸¸ä½¿ç”¨ä¸å¯ã®å“è³ªãƒ¬ãƒ™ãƒ«
        unusable_qualities = [
            FittingQuality.BOUNDARY_STUCK,
            FittingQuality.FAILED,
            FittingQuality.OVERFITTING
        ]
        
        # CRITICAL_PROXIMITYã¯ç‰¹æ®Šå‡¦ç†ï¼šé«˜ä¿¡é ¼åº¦ã ãŒäºˆæ¸¬ã¨ã—ã¦ã¯ä½¿ç”¨ä¸å¯
        if quality == FittingQuality.CRITICAL_PROXIMITY:
            return False  # è‡¨ç•Œç‚¹æ¥µè¿‘ã¯äºˆæ¸¬ã«ä½¿ç”¨ä¸å¯ï¼ˆä½†ã—é‡è¦ãªæƒ…å ±ï¼‰
        
        if quality in unusable_qualities:
            return False
        
        # æœ€ä½é™ã®çµ±è¨ˆçš„å“è³ª
        if 'r_squared' in statistics and statistics['r_squared'] < 0.5:
            return False
        
        # tcå€¤ã®å¦¥å½“æ€§
        if 'tc' in parameters and (parameters['tc'] <= 1.01 or parameters['tc'] > 3.0):
            return False
        
        return True


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
def example_usage():
    """ä½¿ç”¨ä¾‹"""
    evaluator = FittingQualityEvaluator()
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: å¢ƒç•Œå¼µã‚Šä»˜ã
    print("ğŸ” ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹1: å¢ƒç•Œå¼µã‚Šä»˜ã")
    params1 = {
        'tc': 1.001,  # ä¸‹é™ã«å¼µã‚Šä»˜ã
        'beta': 0.35,
        'omega': 6.5,
        'phi': 0.0,
        'A': 5.0,
        'B': 0.5,
        'C': 0.1
    }
    stats1 = {'r_squared': 0.95, 'rmse': 0.03}
    
    assessment1 = evaluator.evaluate_fitting(params1, stats1)
    print(f"   å“è³ª: {assessment1.quality.value}")
    print(f"   ä¿¡é ¼åº¦: {assessment1.confidence:.2%}")
    print(f"   ä½¿ç”¨å¯èƒ½: {assessment1.is_usable}")
    print(f"   å•é¡Œ: {assessment1.issues}")
    if assessment1.quality == FittingQuality.CRITICAL_PROXIMITY:
        print(f"   âš ï¸ è§£é‡ˆ: {assessment1.metadata.get('interpretation', '')}")
        print(f"   åŸå› : {assessment1.metadata.get('primary_cause', '')}")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: é«˜å“è³ªãƒ•ã‚£ãƒƒãƒˆ
    print("\nğŸ” ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹2: é«˜å“è³ªãƒ•ã‚£ãƒƒãƒˆ")
    params2 = {
        'tc': 1.25,
        'beta': 0.33,
        'omega': 6.36,
        'phi': 0.1,
        'A': 5.0,
        'B': 0.5,
        'C': 0.1
    }
    stats2 = {'r_squared': 0.92, 'rmse': 0.04}
    
    assessment2 = evaluator.evaluate_fitting(params2, stats2)
    print(f"   å“è³ª: {assessment2.quality.value}")
    print(f"   ä¿¡é ¼åº¦: {assessment2.confidence:.2%}")
    print(f"   ä½¿ç”¨å¯èƒ½: {assessment2.is_usable}")
    print(f"   å•é¡Œ: {assessment2.issues}")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: éå­¦ç¿’ç–‘ã„
    print("\nğŸ” ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹3: éå­¦ç¿’ç–‘ã„")
    params3 = {
        'tc': 1.01,
        'beta': 0.95,  # æ¥µç«¯ãªå€¤
        'omega': 18.0,  # æ¥µç«¯ãªå€¤
        'phi': 0.0,
        'A': 5.0,
        'B': 0.5,
        'C': 0.1
    }
    stats3 = {'r_squared': 0.98, 'rmse': 0.02}
    
    assessment3 = evaluator.evaluate_fitting(params3, stats3)
    print(f"   å“è³ª: {assessment3.quality.value}")
    print(f"   ä¿¡é ¼åº¦: {assessment3.confidence:.2%}")
    print(f"   ä½¿ç”¨å¯èƒ½: {assessment3.is_usable}")
    print(f"   å•é¡Œ: {assessment3.issues}")


if __name__ == "__main__":
    example_usage()