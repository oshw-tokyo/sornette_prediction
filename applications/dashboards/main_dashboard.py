#!/usr/bin/env python3
"""
Symbol-Based Market Analysis Dashboard

Restored implementation with symbol selection sidebar and category-based classification.
Displays analysis results for selected symbols in 3 tabs with trading position prioritization.
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import sys
import os
import json
import numpy as np
from typing import Dict, List, Optional, Tuple

# ãƒ‘ã‚¹ã®è¨­å®š
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

# .envãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•èª­ã¿è¾¼ã¿ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰
try:
    from dotenv import load_dotenv
    dotenv_path = os.path.join(os.path.dirname(__file__), '../../.env')
    if os.path.exists(dotenv_path):
        load_dotenv(dotenv_path)
        print("âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: .env ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
    else:
        print("âš ï¸ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
except ImportError:
    print("âš ï¸ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: python-dotenv ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")

from infrastructure.database.results_database import ResultsDatabase
from infrastructure.data_sources.unified_data_client import UnifiedDataClient

class SymbolAnalysisDashboard:
    """Symbol-Based Analysis Dashboard"""
    
    def __init__(self):
        self.db = ResultsDatabase()
        self.market_catalog = self.load_market_catalog()
        self.data_client = UnifiedDataClient()
        
        # ğŸ”§ APIåŠ¹ç‡åŒ–: ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å†…æœ‰åŠ¹ï¼‰
        if 'price_data_cache' not in st.session_state:
            st.session_state.price_data_cache = {}
        if 'cache_metadata' not in st.session_state:
            st.session_state.cache_metadata = {}
        
        # ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ2025-08-11è¿½åŠ ï¼‰
        if 'filter_presets_cache' not in st.session_state:
            st.session_state.filter_presets_cache = self.db.get_filter_presets()
    
    def _convert_date_columns_for_display(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºå°‚ç”¨ã®å®‰å…¨ãªæ—¥ä»˜å¤‰æ›
        ğŸ”’ ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä¿è­·: å¤‰æ›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯å¤–éƒ¨ã«æ¸¡ã•ãšã€è¡¨ç¤ºã®ã¿ã«ä½¿ç”¨
        æ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜ã•ã‚ŒãŸæ—¥ä»˜ã‚’Timestampå‹ã«å¤‰æ›ï¼ˆ2025-08-11è¿½åŠ ï¼‰
        """
        date_columns = [
            'predicted_crash_date', 'analysis_basis_date', 'data_period_start', 
            'data_period_end', 'analysis_date'
        ]
        
        # ğŸ”’ é‡è¦: å…ƒãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã›ãšã€ã‚³ãƒ”ãƒ¼ã§å¤‰æ›ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä¿è­·ï¼‰
        df_converted = df.copy()
        
        for col in date_columns:
            if col in df_converted.columns:
                try:
                    # æ–‡å­—åˆ—æ—¥ä»˜ã‚’Timestampå‹ã«å¤‰æ›ï¼ˆerrors='coerce'ã§ç„¡åŠ¹ãªå€¤ã¯NaTã«ï¼‰
                    df_converted[col] = pd.to_datetime(df_converted[col], errors='coerce')
                    # é™éŸ³ãƒ¢ãƒ¼ãƒ‰: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã€ãƒ­ã‚°å‡ºåŠ›ã‚’å‰Šé™¤
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ã¯é™éŸ³å‡¦ç†ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¸ã®å½±éŸ¿ã‚’å›é¿ï¼‰
                    pass
                    
        return df_converted

    def _ensure_date_string(self, date_value) -> str:
        """
        APIå‘¼ã³å‡ºã—ç”¨ã«Timestamp/datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ YYYY-MM-DD æ–‡å­—åˆ—ã«å®‰å…¨å¤‰æ›
        ğŸ”§ FRED APIä¿®æ­£: observation_start/end ã®æ–‡å­—åˆ—ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¿è¨¼
        """
        if pd.isna(date_value):
            return None
        
        if isinstance(date_value, str):
            # æ—¢ã«æ–‡å­—åˆ—ã®å ´åˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥ï¼‰
            if len(date_value) >= 10:  # YYYY-MM-DDå½¢å¼ç¢ºèª
                return date_value[:10]  # æ™‚é–“éƒ¨åˆ†ãŒã‚ã‚Œã°é™¤å»
            return date_value
        
        if hasattr(date_value, 'strftime'):
            # Timestamp/datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
            return date_value.strftime('%Y-%m-%d')
        
        # ãã®ä»–ã®å ´åˆï¼ˆäºˆæœŸã—ãªã„å‹ï¼‰
        return str(date_value)[:10] if str(date_value) else None
    
    def __post_init__(self):
        """Initialize Streamlit configuration after object creation"""
        # Page configuration
        st.set_page_config(
            page_title="Symbol Analysis Dashboard",
            page_icon="ğŸ“Š",
            layout="wide",
            initial_sidebar_state="expanded"
        )
    
    def load_market_catalog(self) -> Dict:
        """Load market catalog for symbol categorization"""
        try:
            catalog_path = "infrastructure/data_sources/market_data_catalog.json"
            with open(catalog_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            st.error(f"Failed to load market catalog: {str(e)}")
            return {"symbols": {}}
    
    def get_symbols_by_category(self) -> Dict[str, List[Dict]]:
        """Organize symbols by asset class and instrument type"""
        categorized = {}
        
        if not self.market_catalog.get("symbols"):
            return categorized
        
        for symbol, info in self.market_catalog["symbols"].items():
            asset_class = info.get("asset_class", "unknown")
            instrument_type = info.get("instrument_type", "unknown")
            
            # Create category key
            category_key = f"{asset_class}_{instrument_type}"
            
            if category_key not in categorized:
                categorized[category_key] = {
                    "display_name": f"{asset_class.title()} - {instrument_type}",
                    "symbols": []
                }
            
            categorized[category_key]["symbols"].append({
                "symbol": symbol,
                "display_name": info.get("display_name", symbol),
                "description": info.get("description", ""),
                "bubble_analysis_suitability": info.get("bubble_analysis_suitability", "unknown")
            })
        
        return categorized
    
    # DEPRECATED: tcå€¤ã‹ã‚‰ã®æ—¥æ™‚å¤‰æ›ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜æ™‚ã«å®Ÿè¡Œæ¸ˆã¿ã®ãŸã‚ä¸è¦
    # å°†æ¥çš„ã«å¿…è¦ã«ãªã‚‹å ´åˆã«å‚™ãˆã¦ä¿æŒï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
    # def tc_to_datetime_with_hours(self, tc: float, data_end_date: datetime, window_days: int) -> datetime:
    #     """
    #     tcå€¤ã‚’æ—¥æ™‚ã«å¤‰æ›ï¼ˆæ™‚é–“ç²¾åº¦ã¾ã§å«ã‚€ï¼‰
    #     NOTE: ã“ã®æ©Ÿèƒ½ã¯integration_helpers.pyã§å®Ÿè¡Œæ¸ˆã¿ã®ãŸã‚ç¾åœ¨ã¯ä¸è¦
    #     """
    #     # pandas.Timestampã®å ´åˆã¯python datetimeã«å¤‰æ›
    #     if hasattr(data_end_date, 'to_pydatetime'):
    #         data_end_date = data_end_date.to_pydatetime()
    #     
    #     if tc > 1.0:
    #         # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã‚’è¶…ãˆãŸäºˆæ¸¬
    #         days_beyond = (tc - 1.0) * window_days
    #         days_int = int(days_beyond)
    #         hours = (days_beyond - days_int) * 24
    #         return data_end_date + timedelta(days=days_int, hours=hours)
    #     else:
    #         # ãƒ‡ãƒ¼ã‚¿æœŸé–“å†…ã®äºˆæ¸¬
    #         days_from_start = tc * window_days
    #         days_int = int(days_from_start)
    #         hours = (days_from_start - days_int) * 24
    #         data_start_date = data_end_date - timedelta(days=window_days)
    #         return data_start_date + timedelta(days=days_int, hours=hours)
    
    def calculate_crash_date_convergence(self, crash_dates: List[datetime], basis_dates: List[datetime]) -> Tuple[str, bool]:
        """
        ã‚¯ãƒ©ãƒƒã‚·ãƒ¥äºˆæ¸¬æ—¥ã®åæŸå€¤ã‚’è¨ˆç®—
        
        Args:
            crash_dates: äºˆæ¸¬ã•ã‚ŒãŸã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥ã®ãƒªã‚¹ãƒˆ
            basis_dates: ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            Tuple[str, bool]: (åæŸæ—¥ã¾ãŸã¯"åæŸã—ãªã„", åæŸãƒ•ãƒ©ã‚°)
        """
        try:
            if not crash_dates or len(crash_dates) < 3:
                return "ãƒ‡ãƒ¼ã‚¿ä¸è¶³", False
            
            # ç¾åœ¨ã‹ã‚‰3å¹´ä»¥å†…ã®äºˆæ¸¬ã®ã¿ã‚’æœ‰åŠ¹ã¨ã™ã‚‹
            now = datetime.now()
            three_years_later = now + timedelta(days=3*365)
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šéå»ã®äºˆæ¸¬ã¨3å¹´ä»¥ä¸Šå…ˆã®äºˆæ¸¬ã‚’é™¤å¤–
            valid_predictions = []
            for crash_date, basis_date in zip(crash_dates, basis_dates):
                if crash_date > now and crash_date <= three_years_later:
                    valid_predictions.append((crash_date, basis_date))
            
            if len(valid_predictions) < 3:
                return "åæŸã—ãªã„", False
            
            # åŸºæº–æ—¥é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
            valid_predictions.sort(key=lambda x: x[1], reverse=True)
            recent_predictions = valid_predictions[:min(5, len(valid_predictions))]
            
            # æœ€æ–°5ä»¶ã®äºˆæ¸¬æ—¥ã®æ¨™æº–åå·®ã‚’è¨ˆç®—
            recent_crash_dates = [pred[0] for pred in recent_predictions]
            timestamps = [d.timestamp() for d in recent_crash_dates]
            
            if len(timestamps) < 3:
                return "ãƒ‡ãƒ¼ã‚¿ä¸è¶³", False
            
            # æ¨™æº–åå·®ã‚’æ—¥æ•°ã§è¨ˆç®—
            mean_timestamp = np.mean(timestamps)
            std_days = np.std(timestamps) / (24 * 3600)  # ç§’ã‚’æ—¥ã«å¤‰æ›
            
            # åæŸåˆ¤å®šï¼šæ¨™æº–åå·®ãŒ30æ—¥ä»¥å†…ãªã‚‰åæŸã¨ã¿ãªã™
            if std_days <= 30:
                convergence_date = datetime.fromtimestamp(mean_timestamp)
                
                # åæŸå‚¾å‘ã®ç¢ºèªï¼šæœ€æ–°3ä»¶ã®äºˆæ¸¬ãŒä¸€å®šæ–¹å‘ã«å‘ã‹ã£ã¦ã„ã‚‹ã‹
                if len(recent_crash_dates) >= 3:
                    trend_timestamps = [d.timestamp() for d in recent_crash_dates[:3]]
                    # æ™‚ç³»åˆ—çš„ã«å˜èª¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                    trend_variation = np.std(np.diff(trend_timestamps)) / (24 * 3600)
                    
                    if trend_variation <= 15:  # 15æ—¥ä»¥å†…ã®å¤‰å‹•ãªã‚‰å®‰å®š
                        return convergence_date.strftime('%Y-%m-%d'), True
                
                return convergence_date.strftime('%Y-%m-%d'), True
            else:
                return "åæŸã—ãªã„", False
                
        except Exception as e:
            return "è¨ˆç®—ã‚¨ãƒ©ãƒ¼", False
    
    def calculate_multi_method_convergence(self, period_data: pd.DataFrame) -> Dict:
        """
        Multiple methods for convergence analysis
        
        Args:
            period_data: DataFrame with prediction data for the period
            
        Returns:
            Dict: Convergence metrics using multiple methods
        """
        try:
            from scipy import stats
            import numpy as np
            
            # Prepare crash dates as numeric values (days from now)
            crash_dates = []
            fitting_dates = []
            r_squared_values = []
            
            for _, row in period_data.iterrows():
                crash_date = pd.to_datetime(row['predicted_crash_date'])
                fitting_date = pd.to_datetime(row['fitting_basis_date'])
                
                # Days from now to crash
                days_to_crash = (crash_date - datetime.now()).days
                crash_dates.append(days_to_crash)
                fitting_dates.append(fitting_date)
                r_squared_values.append(row.get('r_squared', 0.5))
            
            crash_dates = np.array(crash_dates)
            r_squared_values = np.array(r_squared_values)
            
            # 1. Standard deviation method
            std_deviation = np.std(crash_dates)
            mean_days = np.mean(crash_dates)
            
            # 2. Coefficient of variation
            coefficient_variation = std_deviation / abs(mean_days) if abs(mean_days) > 0 else float('inf')
            
            # 3. Range method  
            prediction_range = np.max(crash_dates) - np.min(crash_dates)
            
            # 4. Weighted standard deviation (recent data weighted more)
            # Create exponential weights (more recent = higher weight)
            sorted_indices = np.argsort([fd for fd in fitting_dates])[::-1]  # Most recent first
            weights = np.exp(-0.1 * np.arange(len(crash_dates)))  # Exponential decay
            
            # Apply weights according to fitting date order
            weighted_crash_dates = crash_dates[sorted_indices]
            weighted_mean = np.average(weighted_crash_dates, weights=weights)
            weighted_variance = np.average((weighted_crash_dates - weighted_mean)**2, weights=weights)
            weighted_std = np.sqrt(weighted_variance)
            
            # 5. Trend analysis
            # Linear regression of crash dates over fitting dates
            fitting_timestamps = [fd.timestamp() for fd in fitting_dates]
            if len(fitting_timestamps) >= 3:
                slope, intercept, r_value, p_value, std_err = stats.linregress(fitting_timestamps, crash_dates)
                trend_r_squared = r_value**2
                trend_slope = slope * (24 * 3600)  # Convert to days per day
            else:
                trend_slope = 0
                trend_r_squared = 0
            
            # 6. Consensus date (weighted average of recent predictions)
            consensus_days = weighted_mean
            consensus_date = datetime.now() + timedelta(days=consensus_days)
            
            # 7. Convergence status based on multiple criteria
            if std_deviation < 5 and coefficient_variation < 0.05:
                convergence_status = "Excellent"
            elif std_deviation < 10 and coefficient_variation < 0.10:
                convergence_status = "Good"
            elif std_deviation < 20 and coefficient_variation < 0.20:
                convergence_status = "Moderate" 
            else:
                convergence_status = "Poor"
            
            return {
                'std_deviation': std_deviation,
                'coefficient_variation': coefficient_variation,
                'prediction_range': prediction_range,
                'weighted_std': weighted_std,
                'trend_slope': trend_slope,
                'trend_r_squared': trend_r_squared,
                'consensus_date': consensus_date,
                'convergence_status': convergence_status,
                'mean_days_to_crash': mean_days,
                'data_count': len(crash_dates)
            }
            
        except Exception as e:
            # Return default values on error
            return {
                'std_deviation': 999.0,
                'coefficient_variation': 999.0,
                'prediction_range': 999.0,
                'weighted_std': 999.0,
                'trend_slope': 0.0,
                'trend_r_squared': 0.0,
                'consensus_date': datetime.now(),
                'convergence_status': 'Error',
                'mean_days_to_crash': 0.0,
                'data_count': 0
            }
    
    def create_convergence_plot(self, period_data: pd.DataFrame, period_name: str, convergence_results: Dict):
        """
        Create a detailed convergence plot for a specific period
        
        Args:
            period_data: DataFrame with prediction data
            period_name: Name of the time period
            convergence_results: Results from calculate_multi_method_convergence
            
        Returns:
            plotly.graph_objects.Figure: Convergence analysis plot
        """
        try:
            fig = go.Figure()
            
            # Prepare data
            fitting_dates = []
            crash_dates = []
            r_squared_values = []
            quality_values = []
            
            for _, row in period_data.iterrows():
                fitting_dates.append(pd.to_datetime(row['fitting_basis_date']))
                crash_dates.append(pd.to_datetime(row['predicted_crash_date']))
                r_squared_values.append(row.get('r_squared', 0.5))
                quality_values.append(row.get('quality', 'unknown'))
            
            # Main scatter plot: fitting dates vs crash predictions
            fig.add_trace(go.Scatter(
                x=fitting_dates,
                y=crash_dates,
                mode='markers',
                marker=dict(
                    size=12,
                    color=r_squared_values,
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="RÂ² Score", x=1.02)
                ),
                name='Predictions',
                text=[f"RÂ²: {r2:.3f}<br>Quality: {q}<br>Days to crash: {(pd.to_datetime(cd) - datetime.now()).days if pd.notna(cd) else 'N/A'}" 
                      for cd, r2, q in zip(crash_dates, r_squared_values, quality_values)],
                hovertemplate='Fitting Date: %{x}<br>Predicted Crash: %{y}<br>%{text}<extra></extra>'
            ))
            
            # Add consensus crash date line
            consensus_date = convergence_results['consensus_date']
            fig.add_hline(
                y=consensus_date,
                line_dash="dash",
                line_color="red",
                annotation_text=f"Consensus: {consensus_date.strftime('%Y-%m-%d')}",
                annotation_position="bottom right"
            )
            
            # Add convergence bands
            std_dev_days = convergence_results['std_deviation']
            upper_band = consensus_date + timedelta(days=std_dev_days)
            lower_band = consensus_date - timedelta(days=std_dev_days)
            
            # Add upper and lower bands
            fig.add_hline(
                y=upper_band,
                line_dash="dot",
                line_color="orange",
                opacity=0.5,
                annotation_text=f"+1Ïƒ ({std_dev_days:.0f}d)",
                annotation_position="top right"
            )
            fig.add_hline(
                y=lower_band,
                line_dash="dot", 
                line_color="orange",
                opacity=0.5,
                annotation_text=f"-1Ïƒ",
                annotation_position="bottom right"
            )
            
            # Trend line if significant
            if convergence_results['trend_r_squared'] > 0.5:
                # Add trend line
                x_trend = [min(fitting_dates), max(fitting_dates)]
                trend_slope_per_sec = convergence_results['trend_slope'] / (24 * 3600)
                
                # Calculate y values for trend line
                y_start = consensus_date
                days_span = (max(fitting_dates) - min(fitting_dates)).days
                y_end = consensus_date + timedelta(days=trend_slope_per_sec * days_span)
                
                fig.add_trace(go.Scatter(
                    x=x_trend,
                    y=[y_start, y_end],
                    mode='lines',
                    line=dict(color='purple', width=2, dash='dot'),
                    name=f'Trend (RÂ²={convergence_results["trend_r_squared"]:.2f})',
                    hovertemplate='Trend Line<extra></extra>'
                ))
            
            # Layout
            fig.update_layout(
                title=f"{period_name} Convergence Analysis - Status: {convergence_results['convergence_status']}",
                xaxis_title="Fitting Basis Date",
                yaxis_title="Predicted Crash Date", 
                height=500,
                hovermode='closest',
                plot_bgcolor='rgba(240, 240, 240, 0.8)',
                showlegend=True
            )
            
            return fig
            
        except Exception as e:
            # Return empty figure on error
            fig = go.Figure()
            fig.update_layout(
                title=f"{period_name} - Plot Error: {str(e)}",
                height=400
            )
            return fig
    
    def calculate_trading_priority_score(self, predicted_crash_date: datetime) -> float:
        """
        äºˆæ¸¬ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥æ™‚ã«åŸºã¥ã„ã¦ãƒˆãƒ¬ãƒ¼ãƒ‰å„ªå…ˆåº¦ã‚’è¨ˆç®—
        
        Args:
            predicted_crash_date: äºˆæ¸¬ã•ã‚ŒãŸã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥æ™‚
            
        Returns:
            float: å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©ç·Šæ€¥ï¼‰
        """
        try:
            if pd.isna(predicted_crash_date):
                return 0.0
            
            now = datetime.now()
            days_to_crash = (predicted_crash_date - now).days
            
            # æ—¥æ•°ã«åŸºã¥ãå„ªå…ˆåº¦ã‚¹ã‚³ã‚¢
            if days_to_crash <= 0:
                return 100  # æ—¢ã«éãã¦ã„ã‚‹å ´åˆã¯æœ€é«˜å„ªå…ˆåº¦
            elif days_to_crash <= 30:
                return 90   # 1ãƒ¶æœˆä»¥å†…
            elif days_to_crash <= 90:
                return 70   # 3ãƒ¶æœˆä»¥å†…
            elif days_to_crash <= 180:
                return 50   # 6ãƒ¶æœˆä»¥å†…
            elif days_to_crash <= 365:
                return 30   # 1å¹´ä»¥å†…
            else:
                return 10   # 1å¹´ä»¥ä¸Šå…ˆ
                
        except Exception:
            return 0.0
    
    def get_symbol_analysis_data(self, symbol: str, limit: int = 50, 
                                 period_selection: Optional[Dict] = None) -> pd.DataFrame:
        """
        Get analysis data for specific symbol with predicted crash dates
        
        v2æ›´æ–°ï¼ˆ2025-08-11ï¼‰: Symbolé¸æŠå¾Œã¯å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ã—ã€Displaying Periodã®ã¿ã§ãƒ•ã‚£ãƒ«ã‚¿
        """
        try:
            # é¸æŠéŠ˜æŸ„ã®å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆSymbol Filterså½±éŸ¿ãªã—ï¼‰
            analyses = self.db.get_recent_analyses(symbol=symbol, limit=None)
            
            if analyses.empty:
                return pd.DataFrame()
            
            # ğŸ”§ è¡¨ç¤ºå°‚ç”¨ã®æ—¥ä»˜ã‚«ãƒ©ãƒ å¤‰æ›ï¼ˆãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ä¿è­·ï¼‰
            analyses = self._convert_date_columns_for_display(analyses)
            
            # Apply period filtering based on analysis basis date
            if period_selection:
                start_date = period_selection.get('start_date')
                end_date = period_selection.get('end_date')
                
                if start_date and end_date:
                    # Convert dates to datetime for filtering
                    start_datetime = pd.to_datetime(start_date)
                    end_datetime = pd.to_datetime(end_date) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)  # End of day
                    
                    # Filter based on analysis_basis_date (priority) or data_period_end (fallback)
                    filtered_analyses = []
                    for _, row in analyses.iterrows():
                        basis_date = None
                        if pd.notna(row.get('analysis_basis_date')):
                            basis_date = pd.to_datetime(row['analysis_basis_date'])
                        elif pd.notna(row.get('data_period_end')):
                            basis_date = pd.to_datetime(row['data_period_end'])
                        
                        if basis_date and start_datetime <= basis_date <= end_datetime:
                            filtered_analyses.append(row)
                    
                    if filtered_analyses:
                        analyses = pd.DataFrame(filtered_analyses)
                    else:
                        return pd.DataFrame()  # No data in selected period
            
            # Use database-stored predicted crash dates (no recalculation needed)
            # Convert string dates to datetime objects for processing
            if 'predicted_crash_date' in analyses.columns:
                analyses['predicted_crash_date'] = pd.to_datetime(analyses['predicted_crash_date'], errors='coerce')
            else:
                # Fallback: if column missing, create empty datetime column
                analyses['predicted_crash_date'] = pd.NaT
            
            # Calculate trading priority scores based on predicted dates
            analyses['trading_priority'] = analyses['predicted_crash_date'].apply(
                self.calculate_trading_priority_score
            )
            
            # Sort by analysis basis date (newest first) for temporal consistency
            # This ensures the chronological order is maintained after filtering
            basis_date_col = []
            for _, row in analyses.iterrows():
                if pd.notna(row.get('analysis_basis_date')):
                    basis_date_col.append(pd.to_datetime(row['analysis_basis_date']))
                elif pd.notna(row.get('data_period_end')):
                    basis_date_col.append(pd.to_datetime(row['data_period_end']))
                else:
                    basis_date_col.append(pd.NaT)
            
            analyses['sort_basis_date'] = basis_date_col
            analyses = analyses.sort_values('sort_basis_date', ascending=False)
            analyses = analyses.drop('sort_basis_date', axis=1)
            
            return analyses
            
        except Exception as e:
            st.error(f"Error retrieving analysis data: {str(e)}")
            return pd.DataFrame()
    
    def render_sidebar_v2(self):
        """
        æ–°ã—ã„ã‚µã‚¤ãƒ‰ãƒãƒ¼å®Ÿè£…ï¼ˆ2025-08-11ï¼‰
        - Symbol Filters: éŠ˜æŸ„é¸æŠãƒªã‚¹ãƒˆã®ã¿å½±éŸ¿
        - Displaying Period: ãƒ—ãƒ­ãƒƒãƒˆç¯„å›²ã®ã¿åˆ¶å¾¡
        - Apply ãƒœã‚¿ãƒ³: æ˜ç¤ºçš„æ›´æ–°åˆ¶å¾¡
        """
        with st.sidebar:
            st.title("ğŸ” Analysis Controls")
            
            # === 1. Symbol Filters ===
            st.subheader("ğŸ›ï¸ Symbol Filters")
            
            # Get categorized symbols
            categorized_symbols = self.get_symbols_by_category()
            
            # Asset Category (æœ€ä¸Šæ®µãƒ»ç‹¬ç«‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³)
            st.markdown("#### ğŸ·ï¸ Asset Category")
            selected_category = st.selectbox(
                "Select Asset Category",
                ["All Symbols"] + list(categorized_symbols.keys()),
                format_func=lambda x: "All Symbols" if x == "All Symbols" 
                else categorized_symbols[x]["display_name"] if x in categorized_symbols 
                else x,
                help="Filter symbols by asset category"
            )
            
            # Filter Conditions (ãƒ—ãƒªã‚»ãƒƒãƒˆ/ã‚«ã‚¹ã‚¿ãƒ )
            st.markdown("#### ğŸ” Filter Conditions")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆ
            filter_presets = st.session_state.filter_presets_cache
            preset_options = ["User Defined"] + list(filter_presets.keys())
            selected_preset = st.selectbox(
                "Filter Presets",
                preset_options,
                help="Pre-defined filter configurations for common use cases"
            )
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã®åé›†
            custom_filters = {}
            preset_config = None
            
            if selected_preset != "User Defined":
                # ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠæ™‚
                preset_config = filter_presets[selected_preset].copy()
                preset_config.pop('description', None)
                st.info(f"**Applied**: {filter_presets[selected_preset].get('description', '')}")
            else:
                # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šæ™‚
                with st.expander("ğŸ›ï¸ Custom Filters", expanded=True):
                    # RÂ²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    col1, col2 = st.columns(2)
                    with col1:
                        min_r_squared = st.number_input("Min RÂ²", 0.0, 1.0, 0.0, 0.01, format="%.2f")
                        if min_r_squared > 0:
                            custom_filters['min_r_squared'] = min_r_squared
                    with col2:
                        max_r_squared = st.number_input("Max RÂ²", 0.0, 1.0, 1.0, 0.01, format="%.2f")
                        if max_r_squared < 1.0:
                            custom_filters['max_r_squared'] = max_r_squared
                    
                    # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    col1, col2 = st.columns(2)
                    with col1:
                        min_confidence = st.number_input("Min Confidence", 0.0, 1.0, 0.0, 0.01, format="%.2f")
                        if min_confidence > 0:
                            custom_filters['min_confidence'] = min_confidence
                    with col2:
                        max_confidence = st.number_input("Max Confidence", 0.0, 1.0, 1.0, 0.01, format="%.2f")
                        if max_confidence < 1.0:
                            custom_filters['max_confidence'] = max_confidence
                    
                    # ä½¿ç”¨å¯èƒ½æ€§
                    usable_only = st.checkbox("Usable Only", help="Show only usable analyses")
                    if usable_only:
                        custom_filters['is_usable'] = True
                    
                    # äºˆæ¸¬æ—¥ç¯„å›²
                    st.markdown("**Predicted Crash Date Range**")
                    col1, col2 = st.columns(2)
                    with col1:
                        crash_from = st.date_input(
                            "From",
                            value=(datetime.now() - timedelta(days=365)).date(),
                            help="Crash predictions after this date"
                        )
                        if crash_from:
                            custom_filters['predicted_crash_from'] = crash_from.strftime('%Y-%m-%d')
                    with col2:
                        crash_to = st.date_input(
                            "To",
                            value=(datetime.now() + timedelta(days=730)).date(),
                            help="Crash predictions before this date"
                        )
                        if crash_to:
                            custom_filters['predicted_crash_to'] = crash_to.strftime('%Y-%m-%d')
            
            # === 2. Symbol Selection (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°) ===
            st.subheader("ğŸ“ˆ Select Symbol")
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ã—ã¦åˆ©ç”¨å¯èƒ½ãªéŠ˜æŸ„ã‚’å–å¾—
            try:
                available_symbols = self._get_filtered_symbols(
                    selected_category, selected_preset, preset_config, custom_filters
                )
                
                if not available_symbols:
                    st.warning("No symbols match current filters")
                    return None
                
                # Symbolé¸æŠ - NASDAQCOMã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«è¨­å®šï¼ˆ2025-08-12ï¼‰
                default_symbol = "NASDAQCOM" if "NASDAQCOM" in available_symbols else available_symbols[0] if available_symbols else None
                selected_symbol = st.selectbox(
                    "Choose Symbol",
                    available_symbols,
                    index=available_symbols.index(default_symbol) if default_symbol and default_symbol in available_symbols else 0,
                    help="Select a symbol from filtered results"
                )
                
                # ğŸ†• Symbolé¸æŠå¾Œã«Currently Selected Symbolã‚’æ›´æ–°ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—ä¿®æ­£ï¼‰
                if selected_symbol != st.session_state.get('selected_symbol_temp'):
                    st.session_state.selected_symbol_temp = selected_symbol
                    # st.rerun()ã‚’å‰Šé™¤ã—ã¦ç„¡é™ãƒ«ãƒ¼ãƒ—ã‚’é˜²æ­¢
                
            except Exception as e:
                st.error(f"Failed to load symbols: {str(e)}")
                return None
            
            # === 3. Currently Selected Symbol ===
            st.markdown("#### ğŸ¯ Currently Selected")
            # ğŸ”§ ä¿®æ­£ï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é¸æŠçŠ¶æ³ã‚’åæ˜ ï¼ˆ2025-08-11ï¼‰
            temp_symbol = st.session_state.get('selected_symbol_temp')
            current_symbol = st.session_state.get('current_symbol')
            
            if temp_symbol and temp_symbol != current_symbol:
                st.info(f"**{temp_symbol}** (Ready to Apply)")
            elif current_symbol:
                st.success(f"**{current_symbol}** (Active)")
            else:
                st.info("*No symbol selected yet*")
            
            # === 4. Apply Symbol Selection ===
            st.markdown("---")
            # ğŸ”§ ä¿®æ­£ï¼šSymbolé¸æŠçŠ¶æ…‹ã«å¿œã˜ãŸãƒœã‚¿ãƒ³è¡¨ç¤ºï¼ˆ2025-08-11ï¼‰
            apply_disabled = not temp_symbol
            apply_button_text = "ğŸ“ˆ **Select Symbol**" if temp_symbol else "âŒ **Choose Symbol First**"
            
            if st.button(apply_button_text, type="primary", use_container_width=True, disabled=apply_disabled):
                if temp_symbol:
                    # é¸æŠã•ã‚ŒãŸéŠ˜æŸ„ã‚’ç¢ºå®š
                    st.session_state.current_symbol = temp_symbol
                    st.session_state.apply_clicked = True
                    st.success(f"Selected symbol: {temp_symbol}!")
            
            # è¿”å´å€¤ - Display Periodã¯å„ã‚¿ãƒ–ã§å€‹åˆ¥ç®¡ç†
            current_symbol = st.session_state.get('current_symbol')
            if current_symbol and st.session_state.get('apply_clicked', False):
                # apply_clickedãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆæ¬¡å›ã®ãŸã‚ã«ï¼‰ - ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ãƒ«ãƒ¼ãƒ—é˜²æ­¢
                # st.session_state.apply_clicked = False  
                return current_symbol  # period_selectionã¨filtersã¯å‰Šé™¤
            else:
                return None
    
    def _get_filtered_symbols(self, category, preset, preset_config, custom_filters):
        """Symbol Filtersé©ç”¨ã—ã¦åˆ©ç”¨å¯èƒ½ãªéŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if category != "All Symbols":
            categorized_symbols = self.get_symbols_by_category()
            category_symbols = [
                s["symbol"] for s in categorized_symbols.get(category, {}).get("symbols", [])
            ]
        else:
            category_symbols = None
        
        # ãƒ—ãƒªã‚»ãƒƒãƒˆ/ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if preset != "User Defined" and preset_config:
            filtered_data = self.db.apply_filter_preset(preset)
        elif custom_filters:
            filtered_data = self.db.get_filtered_analyses(**custom_filters)
        else:
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãªã—
            all_analyses = self.db.get_recent_analyses(limit=100)
            filtered_data = all_analyses
        
        if filtered_data.empty:
            return []
        
        # éŠ˜æŸ„ãƒªã‚¹ãƒˆå–å¾—
        available_symbols = sorted(filtered_data['symbol'].unique().tolist())
        
        # ã‚«ãƒ†ã‚´ãƒªã§ã•ã‚‰ã«çµã‚Šè¾¼ã¿
        if category_symbols:
            available_symbols = [s for s in available_symbols if s in category_symbols]
        
        return available_symbols
    
    def _get_period_selection(self, symbol):
        """
        Displaying Periodè¨­å®šã‚’å–å¾—
        ä¿®æ­£ï¼šSymbolæœªé¸æŠæ™‚ã®å®‰å…¨ãªãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆ2025-08-11ï¼‰
        """
        if not symbol:
            st.info("ğŸ“ Please select a symbol to configure the displaying period.")
            return None
            
        try:
            # é¸æŠéŠ˜æŸ„ã®å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆSymbol Filtersç„¡è¦–ï¼‰
            all_analyses = self.db.get_recent_analyses(symbol=symbol, limit=None)
            if all_analyses.empty:
                st.warning(f"No analysis data found for {symbol}")
                return None
            
            # åŸºæº–æ—¥ç¯„å›²ã‚’è¨ˆç®—
            basis_dates = []
            for _, row in all_analyses.iterrows():
                if pd.notna(row.get('analysis_basis_date')):
                    basis_dates.append(pd.to_datetime(row['analysis_basis_date']))
                elif pd.notna(row.get('data_period_end')):
                    basis_dates.append(pd.to_datetime(row['data_period_end']))
            
            if not basis_dates:
                st.warning(f"No valid analysis dates found for {symbol}")
                return None
            
            basis_dates = sorted(basis_dates)
            min_date = basis_dates[0].date()
            max_date = basis_dates[-1].date()
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ - å¤‰æ›´: æœ€å¤ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¡¨ç¤ºï¼ˆ2025-08-12ï¼‰
            default_end = max_date
            default_start = min_date  # æœ€å¤ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¡¨ç¤º
            
            # æœŸé–“é¸æŠUI
            col1, col2 = st.columns(2)
            with col1:
                start_date = st.date_input(
                    "From", value=default_start,
                    min_value=min_date, max_value=max_date,
                    help="Start of analysis period"
                )
            with col2:
                end_date = st.date_input(
                    "To", value=default_end,
                    min_value=min_date, max_value=max_date,
                    help="End of analysis period"
                )
            
            if start_date > end_date:
                st.error("Start date must be earlier than end date")
                return None
            
            return {'start_date': start_date, 'end_date': end_date}
            
        except Exception as e:
            st.error(f"Error loading period data: {str(e)}")
            return None
    
    def render_sidebar(self):
        """Render symbol selection sidebar with categorization"""
        
        with st.sidebar:
            st.title("ğŸ” Analysis Controls")
            
            # ğŸ†• ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ã‚’æœ€ä¸Šéƒ¨ã«é…ç½®ï¼ˆ2025-08-11æ”¹å–„: Symbol Filtersã«æ”¹åï¼‰
            st.subheader("ğŸ›ï¸ Symbol Filters")
            
            # Get categorized symbols first
            categorized_symbols = self.get_symbols_by_category()
            
            # Asset Category ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆSymbol Filtersæœ€ä¸Šæ®µãƒ»ç‹¬ç«‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
            with st.expander("ğŸ·ï¸ Asset Category", expanded=True):
                category_options = ["All Symbols"] + list(categorized_symbols.keys())
                selected_category = st.selectbox(
                    "Select Asset Category",
                    category_options,
                    format_func=lambda x: "All Symbols" if x == "All Symbols" 
                    else categorized_symbols[x]["display_name"] if x in categorized_symbols 
                    else x,
                    help="Filter symbols by asset category"
                )
            
            # Filter Conditions ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ—¢å­˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ»ãƒ—ãƒªã‚»ãƒƒãƒˆåˆ©ç”¨å¯èƒ½ï¼‰
            with st.expander("ğŸ” Filter Conditions", expanded=True):
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãƒ—ãƒªã‚»ãƒƒãƒˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰é«˜é€Ÿå–å¾—ï¼‰
                filter_presets = st.session_state.filter_presets_cache
                preset_options = ["User Defined"] + list(filter_presets.keys())
                selected_preset = st.selectbox(
                    "Filter Presets",
                    preset_options,
                    help="Pre-defined filter configurations for common use cases"
                )
                
                # ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®è©³ç´°è¡¨ç¤ºï¼†ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
                custom_filters = {}
                preset_config = None
            
            if selected_preset != "User Defined":
                # ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠæ™‚ï¼šè¨­å®šå†…å®¹ã‚’è¡¨ç¤ºï¼ˆç·¨é›†ä¸å¯ï¼‰
                preset_config = filter_presets[selected_preset].copy()
                preset_config.pop('description', None)  # èª¬æ˜æ–‡ã‚’é™¤å»
                
                st.info(f"**Applied Settings**: {filter_presets[selected_preset].get('description', '')}")
                
                # ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã®è©³ç´°ã‚’å±•é–‹è¡¨ç¤ºï¼ˆã‚°ãƒ¬ãƒ¼ã‚¢ã‚¦ãƒˆï¼‰
                with st.expander("ğŸ“‹ Current Filter Settings (Applied)", expanded=False):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if 'min_r_squared' in preset_config:
                            st.text_input("Min RÂ²", value=f"{preset_config['min_r_squared']:.2f}", disabled=True)
                        if 'min_confidence' in preset_config:
                            st.text_input("Min Confidence", value=f"{preset_config['min_confidence']:.2f}", disabled=True)
                        if 'predicted_crash_from' in preset_config:
                            st.text_input("Crash Date From", value=preset_config['predicted_crash_from'], disabled=True)
                    
                    with col2:
                        if 'is_usable' in preset_config:
                            st.checkbox("Usable Only", value=preset_config['is_usable'], disabled=True)
                        if 'limit' in preset_config:
                            st.text_input("Result Limit", value=str(preset_config['limit']), disabled=True)
                        if 'predicted_crash_to' in preset_config:
                            st.text_input("Crash Date To", value=preset_config['predicted_crash_to'], disabled=True)
                        if 'basis_date_from' in preset_config:
                            st.text_input("Analysis From", value=preset_config['basis_date_from'], disabled=True)
            else:
                # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šæ™‚ï¼šãƒ•ãƒ«æ©Ÿèƒ½æä¾›
                with st.expander("ğŸ›ï¸ Quality Filters", expanded=True):
                    # RÂ²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    col1, col2 = st.columns(2)
                    with col1:
                        min_r_squared = st.number_input(
                            "Min RÂ²", min_value=0.0, max_value=1.0, 
                            step=0.01, format="%.2f", help="Minimum R-squared value threshold"
                        )
                        if min_r_squared > 0:
                            custom_filters['min_r_squared'] = min_r_squared
                    
                    with col2:
                        max_r_squared = st.number_input(
                            "Max RÂ²", min_value=0.0, max_value=1.0, 
                            value=1.0, step=0.01, format="%.2f"
                        )
                        if max_r_squared < 1.0:
                            custom_filters['max_r_squared'] = max_r_squared
                    
                    # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    col1, col2 = st.columns(2)
                    with col1:
                        min_confidence = st.number_input(
                            "Min Confidence", min_value=0.0, max_value=1.0,
                            step=0.01, format="%.2f"
                        )
                        if min_confidence > 0:
                            custom_filters['min_confidence'] = min_confidence
                    
                    with col2:
                        max_confidence = st.number_input(
                            "Max Confidence", min_value=0.0, max_value=1.0,
                            value=1.0, step=0.01, format="%.2f"
                        )
                        if max_confidence < 1.0:
                            custom_filters['max_confidence'] = max_confidence
                    
                    # ä½¿ç”¨å¯èƒ½æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                    usable_only = st.checkbox("Usable Only", help="Show only analyses marked as usable")
                    if usable_only:
                        custom_filters['is_usable'] = True
                
                with st.expander("ğŸ“… Date Range Filters", expanded=False):
                    # äºˆæ¸¬æ—¥ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šãƒ»æ”¹å–„ç‰ˆï¼‰
                    st.markdown("**Predicted Crash Date Range**")
                    
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šï¼ˆ1å¹´å‰ã€œ2å¹´å…ˆï¼‰
                    default_crash_from = (datetime.now() - timedelta(days=365)).date()  # 1å¹´å‰
                    default_crash_to = (datetime.now() + timedelta(days=730)).date()    # 2å¹´å…ˆ
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        crash_from = st.date_input(
                            "Crash From", 
                            value=default_crash_from,
                            help="Show analyses predicting crashes after this date"
                        )
                        if crash_from:
                            custom_filters['predicted_crash_from'] = crash_from.strftime('%Y-%m-%d')
                    
                    with col2:
                        crash_to = st.date_input(
                            "Crash To",
                            value=default_crash_to,
                            help="Show analyses predicting crashes before this date"
                        )
                        if crash_to:
                            custom_filters['predicted_crash_to'] = crash_to.strftime('%Y-%m-%d')
                
                with st.expander("ğŸ”¢ Sort Options", expanded=False):
                    # ã‚½ãƒ¼ãƒˆè¨­å®šï¼ˆRÂ²å„ªå…ˆã€é †åºå¤‰æ›´ï¼‰
                    sort_options = {
                        'r_squared': 'RÂ² Value',
                        'confidence': 'Confidence', 
                        'predicted_crash_date': 'Predicted Crash Date',
                        'analysis_basis_date': 'Analysis Date',
                        'symbol': 'Symbol Name'
                    }
                    sort_by = st.selectbox(
                        "Sort By", options=list(sort_options.keys()),
                        format_func=lambda x: sort_options[x],
                        index=0  # RÂ²ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠï¼ˆãƒªã‚¹ãƒˆã®æœ€åˆï¼‰
                    )
                    custom_filters['sort_by'] = sort_by
                    
                    # ã‚½ãƒ¼ãƒˆé †åºï¼ˆæ”¹å–„ç‰ˆï¼‰
                    sort_order_options = {
                        'DESC': 'Highest First (High to Low)',
                        'ASC': 'Lowest First (Low to High)'
                    }
                    sort_order = st.selectbox(
                        "Sort Order", 
                        options=list(sort_order_options.keys()),
                        format_func=lambda x: sort_order_options[x],
                        index=0
                    )
                    custom_filters['sort_order'] = sort_order
                    
                    # çµæœä»¶æ•°åˆ¶é™ï¼ˆæœ€å¾Œã«é…ç½®ï¼‰
                    result_limit = st.number_input(
                        "Result Limit", min_value=10, max_value=1000, 
                        value=500, step=50, help="Maximum number of results to display"
                    )
                    custom_filters['limit'] = result_limit
            
            # Get available symbols from database (ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è€ƒæ…®ãƒ»æ”¹å–„ç‰ˆ)
            try:
                if selected_preset != "User Defined":
                    # ãƒ—ãƒªã‚»ãƒƒãƒˆé©ç”¨æ™‚ï¼šãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éŠ˜æŸ„å–å¾—
                    filtered_data = self.db.apply_filter_preset(selected_preset)
                    if not filtered_data.empty:
                        available_symbols = sorted(filtered_data['symbol'].unique().tolist())
                    else:
                        available_symbols = []
                elif custom_filters and any(
                    v is not None and v != "" and v != 0 and v is not False 
                    for k, v in custom_filters.items() 
                    if k not in ['limit', 'sort_by', 'sort_order']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯é™¤å¤–
                ):
                    # ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨æ™‚ï¼ˆå®Ÿéš›ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                    filtered_data = self.db.get_filtered_analyses(**custom_filters)
                    if not filtered_data.empty:
                        available_symbols = sorted(filtered_data['symbol'].unique().tolist())
                    else:
                        available_symbols = []
                else:
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãªã—ï¼ˆåˆæœŸçŠ¶æ…‹ã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ ã ãŒè¨­å®šãªã—ï¼‰
                    # ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: éŠ˜æŸ„ãƒªã‚¹ãƒˆã®ã¿å–å¾—ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä¸è¦ï¼‰
                    if 'available_symbols_cache' not in st.session_state:
                        # åˆå›ã®ã¿ï¼šéŠ˜æŸ„ãƒªã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                        all_analyses = self.db.get_recent_analyses(limit=100)  # æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿
                        if all_analyses.empty:
                            st.warning("No analysis data available")
                            return None
                        st.session_state.available_symbols_cache = sorted(all_analyses['symbol'].unique().tolist())
                    available_symbols = st.session_state.available_symbols_cache
                    
            except Exception as e:
                st.error(f"Failed to load analysis data: {str(e)}")
                return None
            
            if not available_symbols:
                st.warning("No symbols match current filter criteria")
                return None
            
            # Get categorized symbols
            categorized_symbols = self.get_symbols_by_category()
            
            # Symbol selection with categories
            st.subheader("ğŸ“ˆ Select Symbol")
            
            # Create category options
            category_options = ["All Symbols"] + list(categorized_symbols.keys())
            selected_category = st.selectbox(
                "Asset Category",
                category_options,
                format_func=lambda x: "All Symbols" if x == "All Symbols" 
                else categorized_symbols[x]["display_name"] if x in categorized_symbols 
                else x
            )
            
            # Filter symbols based on category
            if selected_category == "All Symbols":
                symbol_options = available_symbols
            else:
                category_symbols = [
                    s["symbol"] for s in categorized_symbols.get(selected_category, {}).get("symbols", [])
                ]
                symbol_options = [s for s in available_symbols if s in category_symbols]
            
            if not symbol_options:
                st.warning("No symbols available in selected category")
                return None
            
            # Symbol selection
            selected_symbol = st.selectbox(
                "Symbol",
                symbol_options,
                format_func=lambda x: f"{x} - {self.market_catalog.get('symbols', {}).get(x, {}).get('display_name', x)}"
            )
            
            # Display symbol info
            if selected_symbol in self.market_catalog.get("symbols", {}):
                symbol_info = self.market_catalog["symbols"][selected_symbol]
                
                st.subheader("ğŸ“Š Symbol Information")
                st.info(f"**{symbol_info.get('display_name', selected_symbol)}**")
                st.write(f"**Type**: {symbol_info.get('instrument_type', 'Unknown')}")
                st.write(f"**Asset Class**: {symbol_info.get('asset_class', 'Unknown')}")
                st.write(f"**Analysis Suitability**: {symbol_info.get('bubble_analysis_suitability', 'Unknown')}")
                
                if symbol_info.get('description'):
                    with st.expander("â„¹ï¸ Description"):
                        st.write(symbol_info['description'])
            
            # è¡¨ç¤ºæœŸé–“è¨­å®šï¼ˆæ”¹å–„ç‰ˆï¼‰
            st.subheader("ğŸ“… Displaying Period")
            
            # åˆ†æåŸºæº–æ—¥ã®ç¯„å›²ã‚’å–å¾—
            try:
                # ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: å¿…è¦æœ€å°é™ã®ãƒ‡ãƒ¼ã‚¿å–å¾—
                all_analyses = self.db.get_recent_analyses(symbol=selected_symbol, limit=100)
                if not all_analyses.empty:
                    # åˆ†æåŸºæº–æ—¥ã®å–å¾—ï¼ˆå„ªå…ˆé †ä½: analysis_basis_date > data_period_endï¼‰
                    basis_dates = []
                    for _, row in all_analyses.iterrows():
                        if pd.notna(row.get('analysis_basis_date')):
                            basis_dates.append(pd.to_datetime(row['analysis_basis_date']))
                        elif pd.notna(row.get('data_period_end')):
                            basis_dates.append(pd.to_datetime(row['data_period_end']))
                    
                    if basis_dates:
                        basis_dates = sorted(basis_dates)
                        min_date = basis_dates[0].date()
                        max_date = basis_dates[-1].date()
                        
                        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨ˆç®—
                        default_end = max_date
                        default_start = max(min_date, max_date - timedelta(days=120))  # 4ãƒ¶æœˆå‰
                        
                        # æœŸé–“é¸æŠUIï¼ˆæ”¹å–„ç‰ˆï¼‰
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            start_date = st.date_input(
                                "From",
                                value=default_start,
                                min_value=min_date,
                                max_value=max_date,
                                help="Start date for analysis period (oldest fitting basis date to include)"
                            )
                            st.caption("*Start of analysis basis date range*")
                        
                        with col2:
                            end_date = st.date_input(
                                "To",
                                value=default_end,
                                min_value=min_date,
                                max_value=max_date,
                                help="End date for analysis period (newest fitting basis date to include)"
                            )
                            st.caption("*End of analysis basis date range*")
                        
                        # æ—¥ä»˜ç¯„å›²ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
                        if start_date > end_date:
                            st.error("Start date must be earlier than end date")
                            start_date, end_date = default_start, default_end
                        
                        period_selection = {
                            'start_date': start_date,
                            'end_date': end_date
                        }
                    else:
                        st.warning("No valid basis dates found")
                        period_selection = None
                else:
                    st.warning("No analysis data for period selection")
                    period_selection = None
            except Exception as e:
                st.error(f"Error loading period data: {str(e)}")
                period_selection = None
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®šã‚’ã¾ã¨ã‚ã¦è¿”å´ï¼ˆLegacy Priority Filterå‰Šé™¤ï¼‰
            filter_settings = {
                'preset': preset_config,
                'preset_name': selected_preset if selected_preset != "User Defined" else None,
                'custom': custom_filters if selected_preset == "User Defined" else {},
                'legacy_priority': None  # å»ƒæ­¢æ¸ˆã¿
            }
            
            return selected_symbol, period_selection, filter_settings
    
    def get_symbol_price_data(self, symbol: str, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """Get symbol price data with caching for API efficiency"""
        try:
            # ğŸ”§ APIåŠ¹ç‡åŒ–: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ
            cache_key = f"{symbol}_{start_date}_{end_date}"
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
            if cache_key in st.session_state.price_data_cache:
                cached_data = st.session_state.price_data_cache[cache_key]
                cache_info = st.session_state.cache_metadata.get(cache_key, {})
                print(f"ğŸ”„ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—: {symbol} ({cache_info.get('source', 'unknown')}) - {len(cached_data)}æ—¥åˆ†")
                return cached_data
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€æ–°ã®åˆ†æçµæœã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ç‰¹å®š
            latest_analysis = self.db.get_recent_analyses(symbol=symbol, limit=1)
            preferred_source = None
            if not latest_analysis.empty:
                data_source = latest_analysis.iloc[0].get('data_source')
                if data_source:
                    # å®‰å®šç‰ˆv1.0: FREDå„ªå…ˆ â†’ Twelve Dataè£œå®Œ
                    preferred_source = 'fred' if data_source == 'fred' else 'twelvedata'
            
            # çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãã§ãƒ‡ãƒ¼ã‚¿å–å¾—
            data, source_used = self.data_client.get_data_with_fallback(
                symbol, start_date, end_date, preferred_source=preferred_source
            )
            
            if data is not None and len(data) > 0:
                # ğŸ”§ APIåŠ¹ç‡åŒ–: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                st.session_state.price_data_cache[cache_key] = data
                st.session_state.cache_metadata[cache_key] = {
                    'source': source_used,
                    'cached_at': pd.Timestamp.now(),
                    'size': len(data)
                }
                print(f"âœ… APIå–å¾—æˆåŠŸãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {symbol} ({source_used}) - {len(data)}æ—¥åˆ†")
                return data
            else:
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {symbol}")
                return None
            
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def compute_extended_lppl_fit(self, prices: pd.Series, params: Dict, basis_date: pd.Timestamp, 
                                  target_date: pd.Timestamp) -> Dict:
        """Generate extended LPPL fit for Future Period display"""
        try:
            # å…ƒãƒ‡ãƒ¼ã‚¿ã®æœŸé–“
            data_start = prices.index[0]
            data_end = prices.index[-1]
            
            # Future Periodç”¨ã®æ—¥ä»˜ç¯„å›²ã‚’ç”Ÿæˆ
            future_dates = pd.date_range(start=basis_date, end=target_date, freq='D')
            future_dates = future_dates[future_dates > basis_date]  # åŸºæº–æ—¥ã¯é™¤å¤–
            
            if len(future_dates) == 0:
                return None
            
            # æ­£è¦åŒ–ã•ã‚ŒãŸæ™‚é–“è»¸ã‚’è¨ˆç®—
            total_days = (data_end - data_start).days
            future_days = [(date - data_start).days for date in future_dates]
            t_future = np.array(future_days) / total_days
            
            # LPPLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            tc = params['tc']
            beta = params['beta']
            omega = params['omega']
            phi = params['phi']
            A = params['A']
            B = params['B']
            C = params['C']
            
            # Future Periodç”¨ã®LPPLè¨ˆç®—
            tau_future = tc - t_future
            tau_power_beta = np.power(np.abs(tau_future), beta)
            
            with np.errstate(divide='ignore', invalid='ignore'):
                log_term = np.log(np.abs(tau_future))
                oscillation = np.cos(omega * log_term + phi)
            
            fitted_log_prices = A + B * tau_power_beta + C * tau_power_beta * oscillation
            fitted_prices = np.exp(fitted_log_prices)
            
            # æ­£è¦åŒ–ï¼ˆå…ƒã®ä¾¡æ ¼ç¯„å›²ãƒ™ãƒ¼ã‚¹ï¼‰
            price_min, price_max = prices.min(), prices.max()
            normalized_fitted = (fitted_prices - price_min) / (price_max - price_min)
            
            return {
                'future_dates': future_dates,
                'fitted_prices': fitted_prices,
                'normalized_fitted': normalized_fitted
            }
            
        except Exception as e:
            print(f"âš ï¸ Extended LPPL calculation error: {str(e)}")
            return None

    def compute_lppl_fit(self, prices: pd.Series, params: Dict) -> Dict:
        """Compute LPPL model fit and normalized data for visualization"""
        try:
            # ä¾¡æ ¼ã‚’å¯¾æ•°å¤‰æ›
            log_prices = np.log(prices)
            N = len(prices)
            
            # æ™‚é–“é…åˆ—ã‚’æ­£è¦åŒ–ï¼ˆ0-1ï¼‰
            t = np.linspace(0, 1, N)
            
            # LPPLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            tc = params['tc']
            beta = params['beta'] 
            omega = params['omega']
            phi = params['phi']
            A = params['A']
            B = params['B']
            C = params['C']
            
            # LPPLé–¢æ•°ã®è¨ˆç®—
            # log(p(t)) = A + B*(tc-t)^Î² + C*(tc-t)^Î² * cos(Ï‰*ln(tc-t) + Ï†)
            tau = tc - t
            tau_power_beta = np.power(np.abs(tau), beta)
            
            # è² ã®å€¤ã‚’é¿ã‘ã‚‹ãŸã‚ã«çµ¶å¯¾å€¤ã‚’ä½¿ç”¨
            with np.errstate(divide='ignore', invalid='ignore'):
                log_term = np.log(np.abs(tau))
                oscillation = np.cos(omega * log_term + phi)
                
            fitted_log_prices = A + B * tau_power_beta + C * tau_power_beta * oscillation
            fitted_prices = np.exp(fitted_log_prices)
            
            # æ­£è¦åŒ–ãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—ï¼ˆè«–æ–‡å†ç¾ãƒ†ã‚¹ãƒˆã®å³ä¸Šã‚°ãƒ©ãƒ•ç›¸å½“ï¼‰
            # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’0-1ã«æ­£è¦åŒ–
            price_min, price_max = prices.min(), prices.max()
            normalized_prices = (prices - price_min) / (price_max - price_min)
            normalized_fitted = (fitted_prices - price_min) / (price_max - price_min)
            
            return {
                'fitted_prices': fitted_prices,
                'fitted_log_prices': fitted_log_prices,
                'normalized_prices': normalized_prices,
                'normalized_fitted': normalized_fitted,
                'time_normalized': t
            }
            
        except Exception as e:
            st.error(f"LPPLè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
    
    def convert_tc_to_real_date(self, tc: float, data_start_date: str, data_end_date: str) -> datetime:
        """Convert tc value to actual prediction date"""
        try:
            start_dt = pd.to_datetime(data_start_date)
            end_dt = pd.to_datetime(data_end_date)
            
            # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã®æ—¥æ•°ã‚’è¨ˆç®—
            total_days = (end_dt - start_dt).days
            
            # tcå€¤ã‚’å®Ÿéš›ã®æ—¥ä»˜ã«å¤‰æ›
            # tc > 1ã®å ´åˆã¯æœªæ¥ã®æ—¥ä»˜
            if tc > 1:
                days_beyond_end = (tc - 1) * total_days
                prediction_date = end_dt + timedelta(days=days_beyond_end)
            else:
                # tc < 1ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿æœŸé–“å†…
                days_from_start = tc * total_days
                prediction_date = start_dt + timedelta(days=days_from_start)
            
            return prediction_date
            
        except Exception as e:
            st.error(f"æ—¥ä»˜å¤‰æ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return datetime.now() + timedelta(days=30)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def render_price_predictions_tab(self, symbol: str, analysis_data: pd.DataFrame):
        """Tab 3: LPPL Fitting Plot - Visual analysis of LPPL model fitting results"""
        
        st.header(f"ğŸ“ˆ {symbol} - LPPL Fitting Plot")
        
        if analysis_data.empty:
            st.warning("No analysis data available for this symbol")
            return
        
        # Analysis Data Period - Enhanced version matching clustering tab
        st.subheader("ğŸ“… Analysis Data Period")
        st.caption("è§£æãƒ‡ãƒ¼ã‚¿ã®å¯¾è±¡ã¨ã™ã‚‹æœŸé–“")
        
        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®è¨ˆç®—ï¼ˆFrom/Toã®ä¸‹ã«è¡¨ç¤ºç”¨ï¼‰
        original_data = self.get_symbol_analysis_data(symbol, limit=1000)  # ãƒ•ã‚£ãƒ«ã‚¿å‰ã®å…¨ãƒ‡ãƒ¼ã‚¿
        if not original_data.empty:
            original_data['analysis_basis_date'] = pd.to_datetime(original_data['analysis_basis_date'])
            full_min_date = original_data['analysis_basis_date'].min()
            full_max_date = original_data['analysis_basis_date'].max()
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            if 'lppl_from_date' not in st.session_state:
                st.session_state.lppl_from_date = analysis_data['analysis_basis_date'].min().date()
            from_date = st.date_input("From", st.session_state.lppl_from_date, key='lppl_from_date_input')
            st.session_state.lppl_from_date = from_date
            # Oldest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Oldest Analysis: {full_min_date.strftime('%Y-%m-%d')}")
            
        with col2:
            if 'lppl_to_date' not in st.session_state:
                st.session_state.lppl_to_date = analysis_data['analysis_basis_date'].max().date()
            to_date = st.date_input("To", st.session_state.lppl_to_date, key='lppl_to_date_input')
            st.session_state.lppl_to_date = to_date
            # Latest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Latest Analysis: {full_max_date.strftime('%Y-%m-%d')}")
        
        # ğŸ“Š é¸æŠæœŸé–“ã®æƒ…å ±è¡¨ç¤º
        if not original_data.empty:
            selected_min = pd.to_datetime(from_date)
            selected_max = pd.to_datetime(to_date)
            
            # æœŸé–“ã®å‰²åˆè¨ˆç®—
            total_days = (full_max_date - full_min_date).days
            selected_duration = (selected_max - selected_min).days
            start_offset = (selected_min - full_min_date).days if total_days > 0 else 0
            selected_ratio = selected_duration / total_days if total_days > 0 else 1.0
            
            # ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§æœŸé–“æƒ…å ±ã‚’è¡¨ç¤º
            st.info(f"ğŸ“… **Analysis Period Summary**: {selected_duration} days selected ({selected_ratio*100:.1f}% of available data) | Position: Day {start_offset+1}-{start_offset+selected_duration} of {total_days} total days")
        
        st.markdown("---")
        
        # Display Period ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        analysis_data['analysis_basis_date'] = pd.to_datetime(analysis_data['analysis_basis_date'])
        from_datetime = pd.to_datetime(from_date)
        to_datetime = pd.to_datetime(to_date)
        
        date_mask = (analysis_data['analysis_basis_date'] >= from_datetime) & (analysis_data['analysis_basis_date'] <= to_datetime)
        analysis_data = analysis_data[date_mask].copy()
        
        if len(analysis_data) == 0:
            st.warning(f"No data available for selected period: {from_date} to {to_date}")
            return
        
        st.markdown("---")
        st.info(f"**LPPL Analysis Settings**: Period: {from_date} to {to_date} ({len(analysis_data)} analyses)")
        st.markdown("---")
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ—ãƒ­ãƒƒãƒˆåˆ†å‰²ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        debug_mode = st.checkbox("ğŸ” Debug Mode: Split Integrated Plot into Two Separate Views", 
                                 value=False, 
                                 help="åˆ†æãƒ—ãƒ­ãƒƒãƒˆï¼šæœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ç¢ºèªã€çµ±åˆãƒ—ãƒ­ãƒƒãƒˆï¼šæœŸé–“ç¯„å›²ã®è¤‡æ•°äºˆæ¸¬è¡¨ç¤º")
        
        # æœ€æ–°ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        latest = analysis_data.iloc[0]
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # æœ€æ–°ã®äºˆæ¸¬ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥ã‚’è¡¨ç¤ºï¼ˆtcå€¤ã‹ã‚‰å¤‰æ›ï¼‰
            if pd.notna(latest.get('tc')):
                tc = latest['tc']
                data_start = latest.get('data_period_start')
                data_end = latest.get('data_period_end')
                
                if data_start and data_end:
                    pred_date = self.convert_tc_to_real_date(tc, data_start, data_end)
                    st.metric(
                        "Latest Crash Prediction",
                        pred_date.strftime('%Y-%m-%d'),
                        help=f"Converted from tc={tc:.4f}"
                    )
                else:
                    st.metric("Latest Crash Prediction", "N/A")
            else:
                st.metric("Latest Crash Prediction", "N/A")
        
        with col2:
            st.metric(
                "Model Fit (RÂ²)",
                f"{latest['r_squared']:.4f}" if pd.notna(latest.get('r_squared')) else "N/A",
                help="Goodness of fit for LPPL model"
            )
        
        with col3:
            st.metric(
                "Quality",
                latest.get('quality', 'N/A'),
                help="Analysis quality assessment"
            )
        
        # æœ€æ–°ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        if pd.notna(latest.get('data_period_start')) and pd.notna(latest.get('data_period_end')):
            # ğŸ”§ FRED APIä¿®æ­£: Timestamp ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å®‰å…¨å¤‰æ›
            data_start = self._ensure_date_string(latest['data_period_start'])
            data_end = self._ensure_date_string(latest['data_period_end'])
            
            # è¡¨ç¤ºæ•°ã‚’äº‹å‰ã«å®šç¾©
            display_count = len(analysis_data)
            
            # ğŸ”§ APIåŠ¹ç‡åŒ–æ”¹å–„: å…¨ã¦ã®åˆ†ææœŸé–“ã‚’ã‚«ãƒãƒ¼ã™ã‚‹å®Œå…¨ãªæœŸé–“ã‚’è¨ˆç®—
            # Individual Analysis ã§å¿…è¦ãªå…¨æœŸé–“ã‚’äº‹å‰ã«æŠŠæ¡
            min_data_start = data_start
            max_pred_date = data_end
            
            for _, row in analysis_data.head(display_count).iterrows():
                # å„åˆ†æã®é–‹å§‹æ—¥ã‚’å«ã‚ã‚‹
                row_start = self._ensure_date_string(row.get('data_period_start'))
                if row_start and row_start < min_data_start:
                    min_data_start = row_start
                    
                # å„åˆ†æã®äºˆæ¸¬æ—¥ã‚’å«ã‚ã‚‹
                if pd.notna(row.get('tc')):
                    # ğŸ”§ FRED APIä¿®æ­£: æ—¥ä»˜å¤‰æ›é–¢æ•°é©ç”¨
                    row_start = self._ensure_date_string(row.get('data_period_start', data_start))
                    row_end = self._ensure_date_string(row.get('data_period_end', data_end))
                    if row_start and row_end:
                        pred_date = self.convert_tc_to_real_date(row.get('tc'), row_start, row_end)
                        if pred_date > pd.to_datetime(max_pred_date):
                            max_pred_date = pred_date.strftime('%Y-%m-%d')
            
            # æœ€å°é–‹å§‹æ—¥ã‚’ä½¿ç”¨ï¼ˆå…¨æœŸé–“ã‚’ã‚«ãƒãƒ¼ï¼‰
            data_start = min_data_start
            print(f"ğŸ”§ å…¨æœŸé–“ã‚«ãƒãƒ¼ç¯„å›²: {data_start} to {max_pred_date} (å…¨{display_count}ä»¶å¯¾å¿œ)")
            
            # Future Periodè¡¨ç¤ºã®ãŸã‚ã«ã•ã‚‰ã«æœŸé–“ã‚’æ‹¡å¼µï¼ˆäºˆæ¸¬æ—¥+60æ—¥ï¼‰
            # ğŸ”§ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ: æ¥µç«¯ã«é ã„äºˆæ¸¬æ—¥ã‚’åˆ¶é™
            max_pred_dt = pd.to_datetime(max_pred_date)
            max_allowed_dt = datetime.now() + timedelta(days=365)  # æœ€å¤§1å¹´å…ˆã¾ã§åˆ¶é™
            
            if max_pred_dt > max_allowed_dt:
                print(f"âš ï¸ äºˆæ¸¬æ—¥åˆ¶é™: {max_pred_dt.date()} â†’ {max_allowed_dt.date()} (ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ)")
                max_pred_dt = max_allowed_dt
            
            extended_end = (max_pred_dt + timedelta(days=60)).strftime('%Y-%m-%d')
            print(f"ğŸ” Getting extended price data for {symbol}: {data_start} to {extended_end}")
            
            # å®Ÿéš›ã®ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæ‹¡å¼µæœŸé–“ï¼‰
            price_data = self.get_symbol_price_data(symbol, data_start, extended_end)
            
            if price_data is not None and not price_data.empty and 'Close' in price_data.columns:
                print(f"âœ… Price data retrieved successfully: {len(price_data)} days for {symbol}")
                print(f"   Period: {price_data.index.min()} to {price_data.index.max()}")
                print(f"   Price range: ${price_data['Close'].min():.0f} - ${price_data['Close'].max():.0f}")
                # LPPLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                lppl_params = {
                    'tc': latest.get('tc', 1.0),
                    'beta': latest.get('beta', 0.33),
                    'omega': latest.get('omega', 6.0),
                    'phi': latest.get('phi', 0.0),
                    'A': latest.get('A', 0.0),
                    'B': latest.get('B', 0.0),
                    'C': latest.get('C', 0.0)
                }
                
                # LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’è¨ˆç®—
                lppl_results = self.compute_lppl_fit(price_data['Close'], lppl_params)
                
                if lppl_results:
                    # è«–æ–‡å†ç¾ãƒ†ã‚¹ãƒˆå³ä¸Šã‚°ãƒ©ãƒ•ã«ç›¸å½“ã™ã‚‹æ­£è¦åŒ–è¡¨ç¤ºã‚’ä½œæˆ
                    fig = go.Figure()
                    
                    # æ­£è¦åŒ–ã•ã‚ŒãŸå®Ÿãƒ‡ãƒ¼ã‚¿
                    fig.add_trace(go.Scatter(
                        x=price_data.index,
                        y=lppl_results['normalized_prices'],
                        mode='lines',
                        name='Normalized Market Data',
                        line=dict(color='blue', width=2),
                        opacity=0.8
                    ))
                    
                    # æœ€æ–°ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã‚’å–å¾—
                    latest_fitting_basis = latest.get('analysis_basis_date', data_end)
                    latest_fitting_basis_dt = pd.to_datetime(latest_fitting_basis)
                    
                    # LPPLãƒ•ã‚£ãƒƒãƒˆï¼ˆåŸºæº–æ—¥ã¾ã§ï¼‰- æœ€æ–°ãƒ—ãƒ­ãƒƒãƒˆã®ã¿ã«é©ç”¨
                    basis_mask = price_data.index <= latest_fitting_basis_dt
                    fig.add_trace(go.Scatter(
                        x=price_data.index[basis_mask],
                        y=lppl_results['normalized_fitted'][basis_mask],
                        mode='lines',
                        name='LPPL Fit (Basis Period)',
                        line=dict(color='red', width=2.5)
                    ))
                    
                    # LPPLãƒ•ã‚£ãƒƒãƒˆï¼ˆåŸºæº–æ—¥ä»¥é™ï¼‰- Future Periodè¡¨ç¤º
                    # Individual Analysisã¨åŒã˜æ–¹å¼ã§å®Ÿè£…
                    if pd.notna(latest.get('tc')):
                        # Individual Analysisã¨åŒã˜æ–¹å¼ã§tcå¤‰æ›
                        integrated_pred_date = self.convert_tc_to_real_date(
                            latest['tc'], data_start, data_end)
                        
                        # Individual Analysisã¨åŒã˜æ–¹å¼ã§extended LPPLè¨ˆç®—
                        extended_lppl = self.compute_extended_lppl_fit(
                            price_data['Close'], lppl_params, 
                            latest_fitting_basis_dt, integrated_pred_date + timedelta(days=30))
                        
                        if extended_lppl and len(extended_lppl['future_dates']) > 0:
                            fig.add_trace(go.Scatter(
                                x=extended_lppl['future_dates'],
                                y=extended_lppl['normalized_fitted'],
                                mode='lines',
                                name='LPPL Fit (Future Period)',
                                line=dict(color='orange', width=2.5, dash='dot'),
                                opacity=0.8
                            ))
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæº–æ—¥ä»¥é™ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ã¿
                            future_mask = price_data.index > latest_fitting_basis_dt
                            if future_mask.any():
                                fig.add_trace(go.Scatter(
                                    x=price_data.index[future_mask],
                                    y=lppl_results['normalized_fitted'][future_mask],
                                    mode='lines',
                                    name='LPPL Fit (Future Period)',
                                    line=dict(color='orange', width=2.5, dash='dot')
                                ))
                    
                    # Number of Results ã§æŒ‡å®šã•ã‚ŒãŸæ•°ã®äºˆæ¸¬ç·šã‚’ç¸¦ç·šã§è¡¨ç¤º
                    # Analysis Period Selectionã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ã•ã‚ŒãŸanalysis_dataã®ä»¶æ•°ã‚’ä½¿ç”¨
                    display_count = len(analysis_data)  # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿å…¨ä»¶è¡¨ç¤º
                    
                    # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã®å–å¾—ã¨ã‚½ãƒ¼ãƒˆï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
                    basis_dates = []
                    for _, pred in analysis_data.head(display_count).iterrows():
                        if pd.notna(pred.get('analysis_basis_date')):
                            basis_dates.append(pd.to_datetime(pred['analysis_basis_date']))
                        elif pd.notna(pred.get('data_period_end')):
                            basis_dates.append(pd.to_datetime(pred['data_period_end']))
                        else:
                            basis_dates.append(pd.to_datetime('1900-01-01'))  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    
                    # åŸºæº–æ—¥ã®ç¯„å›²ã‚’è¨ˆç®—ï¼ˆã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
                    if len([d for d in basis_dates if d.year > 1900]) > 1:
                        valid_dates = [d for d in basis_dates if d.year > 1900]
                        min_date = min(valid_dates)
                        max_date = max(valid_dates)
                        date_range = (max_date - min_date).days
                    else:
                        date_range = 0
                    
                    for i, (_, pred) in enumerate(analysis_data.head(display_count).iterrows()):
                        if pd.notna(pred.get('tc')):
                            pred_tc = pred['tc']
                            pred_start = pred.get('data_period_start', data_start)
                            pred_end = pred.get('data_period_end', data_end)
                            
                            if pred_start and pred_end:
                                pred_date = self.convert_tc_to_real_date(pred_tc, pred_start, pred_end)
                                
                                # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã‚’å–å¾—
                                fitting_basis_date = basis_dates[i] if i < len(basis_dates) else pd.to_datetime('1900-01-01')
                                
                                # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è‰²ã®è¨ˆç®—ï¼ˆåŸºæº–æ—¥ãƒ™ãƒ¼ã‚¹ï¼‰
                                if date_range > 0 and fitting_basis_date.year > 1900:
                                    days_from_oldest = (fitting_basis_date - min_date).days
                                    gradient_ratio = days_from_oldest / date_range
                                    # å¤ã„äºˆæ¸¬ï¼šé’ç³»ã€æ–°ã—ã„äºˆæ¸¬ï¼šèµ¤ç³»
                                    red = int(50 + gradient_ratio * 200)  # 50-250
                                    green = int(50 + (1-gradient_ratio) * 100)  # 50-150
                                    blue = int(250 - gradient_ratio * 200)  # 50-250
                                    alpha = 0.8
                                else:
                                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è‰²
                                    red, green, blue = 255, 150, 150
                                    alpha = 0.7
                                
                                # äºˆæ¸¬ç·šã‚’ç¸¦ç·šã§è¡¨ç¤º
                                fig.add_shape(
                                    type="line",
                                    x0=pred_date,
                                    x1=pred_date,
                                    y0=0,
                                    y1=1,
                                    line=dict(
                                        color=f'rgba({red}, {green}, {blue}, {alpha})', 
                                        width=2, 
                                        dash="dash"
                                    )
                                )
                                
                                # äºˆæ¸¬ç·šã®ãƒ©ãƒ™ãƒ«ï¼ˆæœˆæ—¥ã®ã¿è¡¨ç¤ºï¼‰
                                label_text = pred_date.strftime('%m/%d')
                                fig.add_annotation(
                                    x=pred_date,
                                    y=0.95 - i * 0.05,
                                    text=label_text,
                                    showarrow=False,
                                    font=dict(size=10, color='white'),
                                    bgcolor=f"rgba(0, 0, 0, 0.7)",  # é»’ç³»èƒŒæ™¯
                                    bordercolor=f'rgba({red}, {green}, {blue}, 0.8)',
                                    borderwidth=1
                                )
                    
                    # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å‡¡ä¾‹ã‚’è¿½åŠ ï¼ˆæ•£å¸ƒå›³ã¨ã—ã¦è¡¨ç¤ºï¼‰
                    if date_range > 0 and len([d for d in basis_dates if d.year > 1900]) > 1:
                        # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
                        legend_dates = pd.date_range(min_date, max_date, periods=5)
                        legend_y = [0.85, 0.82, 0.79, 0.76, 0.73]  # Yåº§æ¨™
                        legend_colors = []
                        
                        for j, legend_date in enumerate(legend_dates):
                            days_from_oldest = (legend_date - min_date).days
                            gradient_ratio = days_from_oldest / date_range
                            red = int(50 + gradient_ratio * 200)
                            green = int(50 + (1-gradient_ratio) * 100)
                            blue = int(250 - gradient_ratio * 200)
                            legend_colors.append(f'rgb({red}, {green}, {blue})')
                        
                        # å‡¡ä¾‹ç”¨ã®æ•£å¸ƒå›³ã‚’è¿½åŠ 
                        fig.add_trace(go.Scatter(
                            x=[price_data.index.min()] * len(legend_dates),
                            y=legend_y,
                            mode='markers+text',
                            marker=dict(size=15, color=legend_colors),
                            text=[f"{date.strftime('%Y-%m')}: {(date-min_date).days}d" for date in legend_dates],
                            textposition='middle right',
                            textfont=dict(color='white', size=9),
                            showlegend=False,
                            hoverinfo='skip'
                        ))
                    
                    # ã‚°ãƒ©ãƒ•ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
                    fig.update_layout(
                        title=f"{symbol} - Normalized Price Data with LPPL Predictions",
                        xaxis_title="Date",
                        yaxis_title="Normalized Price",
                        height=600,
                        hovermode='x unified',
                        showlegend=True,
                        # èƒŒæ™¯è‰²ã‚’é»’ç³»ã«å¤‰æ›´
                        plot_bgcolor='rgba(20, 20, 30, 0.95)',
                        paper_bgcolor='rgba(15, 15, 25, 0.95)',
                        font=dict(color='white'),
                        # xè»¸ã®ç¯„å›²ã‚’äºˆæ¸¬ç·šã¾ã§æ‹¡å¼µ
                        xaxis=dict(
                            range=[
                                price_data.index.min(),
                                max(price_data.index.max(), 
                                    max([self.convert_tc_to_real_date(row.get('tc', 1.0), 
                                                                     row.get('data_period_start', data_start),
                                                                     row.get('data_period_end', data_end)) 
                                         for _, row in analysis_data.head(display_count).iterrows() 
                                         if pd.notna(row.get('tc'))], default=price_data.index.max()))
                            ],
                            gridcolor='rgba(100, 100, 100, 0.2)',
                            showgrid=True,
                            gridwidth=1
                        ),
                        yaxis=dict(
                            gridcolor='rgba(100, 100, 100, 0.2)',
                            showgrid=True,
                            gridwidth=1
                        )
                    )
                    
                    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼š2ã¤ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ç¸¦ä¸¦ã³ã§è¡¨ç¤º
                    st.markdown("---")
                    st.subheader("ğŸ“Š Analysis Views: Latest & Integrated Predictions")
                    
                    # Latest Analysis Detailsï¼ˆä¸Šéƒ¨ï¼‰- å¸¸ã«æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
                    st.markdown("**ğŸ” Latest Analysis Details**")
                    st.caption("Display Periodç¯„å›²å†…ã§ã®æœ€æ–°ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã®è©³ç´°è¡¨ç¤º")
                    
                    # ğŸ”§ Display Periodé€£æºä¿®æ­£: ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿analysis_dataç¯„å›²å†…ã§æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ2025-08-11ï¼‰
                    # Display Periodã§ãƒ•ã‚£ãƒ«ã‚¿ã•ã‚ŒãŸanalysis_dataå†…ã®æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                    if not analysis_data.empty:
                        absolute_latest = analysis_data.iloc[0]  # analysis_dataã¯æ—¢ã«ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿
                    else:
                        absolute_latest = latest  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    
                    # æœ€æ–°åˆ†æãƒ—ãƒ­ãƒƒãƒˆä½œæˆï¼ˆabsolute_latestã‚’ä½¿ç”¨ï¼‰
                    latest_fig = go.Figure()
                    
                    # çµ¶å¯¾æœ€æ–°ãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨äºˆæ¸¬æ—¥ã‚’è¨ˆç®—
                    # ğŸ”§ FRED APIä¿®æ­£: Timestamp ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å®‰å…¨å¤‰æ›
                    absolute_latest_data_start = self._ensure_date_string(absolute_latest.get('data_period_start'))
                    absolute_latest_data_end = self._ensure_date_string(absolute_latest.get('data_period_end'))
                    absolute_latest_fitting_basis = absolute_latest.get('analysis_basis_date', absolute_latest_data_end)
                    absolute_latest_fitting_basis_dt = pd.to_datetime(absolute_latest_fitting_basis)
                    
                    # çµ¶å¯¾æœ€æ–°ã®äºˆæ¸¬æ—¥è¨ˆç®—
                    if pd.notna(absolute_latest.get('tc')):
                        absolute_latest_pred_date = self.convert_tc_to_real_date(
                            absolute_latest['tc'], absolute_latest_data_start, absolute_latest_data_end)
                    else:
                        absolute_latest_pred_date = None
                    
                    # çµ¶å¯¾æœ€æ–°ç”¨ã®LPPLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                    absolute_latest_lppl_params = {
                        'tc': absolute_latest.get('tc', 1.0),
                        'beta': absolute_latest.get('beta', 0.33),
                        'omega': absolute_latest.get('omega', 6.0),
                        'phi': absolute_latest.get('phi', 0.0),
                        'A': absolute_latest.get('A', 0.0),
                        'B': absolute_latest.get('B', 0.0),
                        'C': absolute_latest.get('C', 0.0)
                    }
                    
                    # çµ¶å¯¾æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãprice_dataã¨LPPLçµæœã‚’å–å¾—
                    absolute_latest_price_data = self.get_symbol_price_data(symbol, absolute_latest_data_start, absolute_latest_data_end)
                    absolute_latest_lppl_results = None
                    latest_extended_lppl = None
                    
                    if absolute_latest_price_data is not None:
                        absolute_latest_lppl_results = self.compute_lppl_fit(absolute_latest_price_data['Close'], absolute_latest_lppl_params)
                        
                        if absolute_latest_lppl_results:
                            # çµ¶å¯¾æœ€æ–°ã®ç”Ÿãƒ‡ãƒ¼ã‚¿
                            latest_fig.add_trace(go.Scatter(
                                x=absolute_latest_price_data.index,
                                y=absolute_latest_lppl_results['normalized_prices'],
                                mode='lines',
                                name='Market Data',
                                line=dict(color='lightblue', width=2)
                            ))
                            
                            # çµ¶å¯¾æœ€æ–°ã®LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆåŸºæº–æ—¥ã¾ã§ï¼‰
                            basis_mask = absolute_latest_price_data.index <= absolute_latest_fitting_basis_dt
                            latest_fig.add_trace(go.Scatter(
                                x=absolute_latest_price_data.index[basis_mask],
                                y=absolute_latest_lppl_results['normalized_fitted'][basis_mask],
                                mode='lines',
                                name='LPPL Fit (Basis Period)',
                                line=dict(color='red', width=2.5)
                            ))
                            
                            # çµ¶å¯¾æœ€æ–°ã®LPPL Future Period
                            if absolute_latest_pred_date is not None:
                                latest_extended_lppl = self.compute_extended_lppl_fit(
                                    absolute_latest_price_data['Close'], absolute_latest_lppl_params, 
                                    absolute_latest_fitting_basis_dt, absolute_latest_pred_date + timedelta(days=30))
                    
                            # Future Periodè¡¨ç¤º
                            if latest_extended_lppl and len(latest_extended_lppl['future_dates']) > 0:
                                latest_fig.add_trace(go.Scatter(
                                    x=latest_extended_lppl['future_dates'],
                                    y=latest_extended_lppl['normalized_fitted'],
                                    mode='lines',
                                    name='LPPL Fit (Future Period)',
                                    line=dict(color='orange', width=2.5, dash='dot'),
                                    opacity=0.8
                                ))
                            
                            # çµ¶å¯¾æœ€æ–°ã®äºˆæ¸¬æ—¥ç¸¦ç·šï¼ˆæœ€å¾Œã«æç”»ã—ã¦Future Periodã‚ˆã‚Šä¸Šã«è¡¨ç¤ºï¼‰
                            if absolute_latest_pred_date is not None:
                                # Yè»¸ã®å®Ÿéš›ã®ç¯„å›²ã‚’è¨ˆç®—
                                y_min = absolute_latest_lppl_results['normalized_prices'].min()
                                y_max = absolute_latest_lppl_results['normalized_prices'].max()
                                
                                # LPPLãƒ•ã‚£ãƒƒãƒˆã®ç¯„å›²ã‚‚è€ƒæ…®
                                y_min = min(y_min, absolute_latest_lppl_results['normalized_fitted'].min())
                                y_max = max(y_max, absolute_latest_lppl_results['normalized_fitted'].max())
                                
                                # Future PeriodãŒã‚ã‚‹å ´åˆã¯ãã®ç¯„å›²ã‚‚è€ƒæ…®
                                if latest_extended_lppl and len(latest_extended_lppl['future_dates']) > 0:
                                    y_max = max(y_max, max(latest_extended_lppl['normalized_fitted']))
                                    y_min = min(y_min, min(latest_extended_lppl['normalized_fitted']))
                                
                                # å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
                                y_range = y_max - y_min
                                y_min_extended = y_min - y_range * 0.02
                                y_max_extended = y_max + y_range * 0.02
                                
                                latest_fig.add_shape(
                                    type="line",
                                    x0=absolute_latest_pred_date, x1=absolute_latest_pred_date,
                                    y0=y_min_extended, y1=y_max_extended,
                                    line=dict(color='red', width=3, dash="dash"),  # èµ¤ç³»ã«å¤‰æ›´
                                    layer='above'  # ä»–ã®è¦ç´ ã‚ˆã‚Šä¸Šã«æç”»
                                )
                                latest_fig.add_annotation(
                                    x=absolute_latest_pred_date, 
                                    y=y_max_extended * 0.95,  # å®Ÿéš›ã®ç¯„å›²ã®ä¸Šéƒ¨ã«é…ç½®
                                    text=f"Latest Prediction\n{absolute_latest_pred_date.strftime('%m/%d')}",
                                    showarrow=False, font=dict(color='red', size=11),
                                    bgcolor="rgba(255, 200, 200, 0.3)"  # èµ¤ç³»ã®èƒŒæ™¯
                                )
                            
                            # Xè»¸ç¯„å›²ã‚’çµ¶å¯¾æœ€æ–°ã®äºˆæ¸¬æ—¥+30æ—¥ã¾ã§æ‹¡å¼µ
                            x_range_end = absolute_latest_pred_date + timedelta(days=30) if absolute_latest_pred_date else absolute_latest_price_data.index.max()
                            latest_fig.update_layout(
                                title="Latest Analysis (Most Recent - Absolute)",
                                height=400,
                                plot_bgcolor='rgba(20, 30, 40, 0.95)',
                                paper_bgcolor='rgba(15, 25, 35, 0.95)',
                                font=dict(color='white', size=10),
                                xaxis=dict(
                                    gridcolor='rgba(100, 100, 100, 0.2)',
                                    showgrid=True,
                                    gridwidth=1,
                                    range=[absolute_latest_price_data.index.min(), x_range_end]
                                ),
                                yaxis=dict(
                                    gridcolor='rgba(100, 100, 100, 0.2)',
                                    showgrid=True,
                                    gridwidth=1
                                )
                            )
                    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã„å ´åˆï¼‰
                    if len(latest_fig.data) == 0:
                        latest_fig.update_layout(
                            title="Latest Analysis - Data Not Available",
                            height=400,
                            plot_bgcolor='rgba(20, 30, 40, 0.95)',
                            paper_bgcolor='rgba(15, 25, 35, 0.95)',
                            font=dict(color='white', size=10)
                        )
                    
                    # Latest Analysis ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º
                    st.plotly_chart(latest_fig, use_container_width=True)
                    
                    # Integrated Predictionsï¼ˆçµ±åˆäºˆæ¸¬è¡¨ç¤ºï¼‰
                    st.markdown("**ğŸ“ˆ Integrated Predictions**")
                    st.caption("çµ±åˆäºˆæ¸¬è¡¨ç¤º - Display Periodç¯„å›²å†…ã®å…¨åˆ†æçµæœã«ã‚ˆã‚‹äºˆæ¸¬çµ±åˆ")
                    
                    # Latest AnalysisåŸºæº–ã§ã®æ–°ã—ã„Integrated Predictions
                    if absolute_latest_price_data is not None:
                        integrated_fig = go.Figure()
                        
                        # Latest AnalysisåŸºæº–ã§ã®å¸‚å ´ãƒ‡ãƒ¼ã‚¿
                        integrated_fig.add_trace(go.Scatter(
                            x=absolute_latest_price_data.index,
                            y=absolute_latest_lppl_results['normalized_prices'],
                            mode='lines',
                            name='Market Data (Latest Basis)',
                            line=dict(color='lightblue', width=2)
                        ))
                        
                        # æœŸé–“å†…ã®è¤‡æ•°äºˆæ¸¬æ—¥ã‚’åé›†ï¼ˆå¾Œã§æç”»ï¼‰
                        prediction_colors = ['red', 'orange', 'green', 'purple', 'brown', 'cyan', 'magenta', 'yellow', 'lime', 'pink']
                        prediction_count = 0
                        prediction_lines = []  # å¾Œã§æç”»ã™ã‚‹ãŸã‚ã®ç¸¦ç·šæƒ…å ±ã‚’ä¿å­˜
                        
                        for i, (_, analysis) in enumerate(analysis_data.iterrows()):
                            if pd.notna(analysis.get('tc')):
                                # å„åˆ†æã®äºˆæ¸¬æ—¥ã‚’è¨ˆç®—
                                analysis_data_start = analysis.get('data_period_start', absolute_latest_data_start)
                                analysis_data_end = analysis.get('data_period_end', absolute_latest_data_end)
                                analysis_pred_date = self.convert_tc_to_real_date(
                                    analysis['tc'], analysis_data_start, analysis_data_end)
                                
                                color = prediction_colors[prediction_count % len(prediction_colors)]
                                # ç¸¦ç·šæƒ…å ±ã‚’ä¿å­˜ï¼ˆå¾Œã§æç”»ï¼‰
                                prediction_lines.append({
                                    'date': analysis_pred_date,
                                    'color': color,
                                    'index': prediction_count
                                })
                                
                                prediction_count += 1
                        
                        # Latest AnalysisåŸºæº–ã§ã®LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
                        if absolute_latest_lppl_results:
                            # Basis Period
                            basis_mask = absolute_latest_price_data.index <= absolute_latest_fitting_basis_dt
                            integrated_fig.add_trace(go.Scatter(
                                x=absolute_latest_price_data.index[basis_mask],
                                y=absolute_latest_lppl_results['normalized_fitted'][basis_mask],
                                mode='lines',
                                name='LPPL Fit (Latest Basis)',
                                line=dict(color='red', width=2.5)
                            ))
                            
                            # Future Period
                            if latest_extended_lppl and len(latest_extended_lppl['future_dates']) > 0:
                                integrated_fig.add_trace(go.Scatter(
                                    x=latest_extended_lppl['future_dates'],
                                    y=latest_extended_lppl['normalized_fitted'],
                                    mode='lines',
                                    name='LPPL Fit (Future Period)',
                                    line=dict(color='orange', width=2.5, dash='dot'),
                                    opacity=0.8
                                ))
                        
                        # ç¸¦ç·šã‚’æœ€å¾Œã«æç”»ï¼ˆFuture Periodã‚ˆã‚Šä¸Šã«è¡¨ç¤ºï¼‰
                        if absolute_latest_lppl_results:
                            # Yè»¸ã®å®Ÿéš›ã®ç¯„å›²ã‚’è¨ˆç®—
                            y_min = absolute_latest_lppl_results['normalized_prices'].min()
                            y_max = absolute_latest_lppl_results['normalized_prices'].max()
                            y_min = min(y_min, absolute_latest_lppl_results['normalized_fitted'].min())
                            y_max = max(y_max, absolute_latest_lppl_results['normalized_fitted'].max())
                            
                            if latest_extended_lppl and len(latest_extended_lppl['future_dates']) > 0:
                                y_max = max(y_max, max(latest_extended_lppl['normalized_fitted']))
                                y_min = min(y_min, min(latest_extended_lppl['normalized_fitted']))
                            
                            y_range = y_max - y_min
                            y_min_extended = y_min - y_range * 0.02
                            y_max_extended = y_max + y_range * 0.02
                            
                            # ä¿å­˜ã—ãŸç¸¦ç·šæƒ…å ±ã‚’æç”»
                            for pred_info in prediction_lines:
                                integrated_fig.add_shape(
                                    type="line",
                                    x0=pred_info['date'], x1=pred_info['date'],
                                    y0=y_min_extended, y1=y_max_extended,
                                    line=dict(color=pred_info['color'], width=2, dash="dash"),
                                    layer='above'  # ä»–ã®è¦ç´ ã‚ˆã‚Šä¸Šã«æç”»
                                )
                                
                                # ãƒ©ãƒ™ãƒ«ã‚‚è¿½åŠ 
                                y_pos = y_max_extended * (0.95 - (pred_info['index'] % 10) * 0.03)
                                integrated_fig.add_annotation(
                                    x=pred_info['date'], 
                                    y=y_pos,
                                    text=f"{pred_info['date'].strftime('%m/%d')}",
                                    showarrow=False, 
                                    font=dict(color='white', size=9),
                                    bgcolor="rgba(0, 0, 0, 0.8)"
                                )
                        
                        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
                        x_range_end = absolute_latest_pred_date + timedelta(days=60) if absolute_latest_pred_date else absolute_latest_price_data.index.max() + timedelta(days=30)
                        integrated_fig.update_layout(
                            title="Integrated Predictions (Latest Analysis Basis)",
                            height=400,
                            plot_bgcolor='rgba(20, 30, 40, 0.95)',
                            paper_bgcolor='rgba(15, 25, 35, 0.95)',
                            font=dict(color='white', size=10),
                            xaxis=dict(
                                gridcolor='rgba(100, 100, 100, 0.2)',
                                showgrid=True,
                                gridwidth=1,
                                range=[absolute_latest_price_data.index.min(), x_range_end]
                            ),
                            yaxis=dict(
                                gridcolor='rgba(100, 100, 100, 0.2)',
                                showgrid=True,
                                gridwidth=1
                            )
                        )
                        
                        st.plotly_chart(integrated_fig, use_container_width=True)
                        
                        # èª¬æ˜
                        st.info(f"ğŸ“Š Showing latest analysis basis with {prediction_count} integrated predictions from selected period")
                    
                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼šè©³ç´°åˆ†æãƒ—ãƒ­ãƒƒãƒˆè¡¨ç¤º
                    if debug_mode:
                        st.markdown("---")
                        st.subheader("ğŸ” Debug Mode: Additional Detailed Analysis Plots")
                        
                        # Plot 1: æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°åˆ†æãƒ—ãƒ­ãƒƒãƒˆ
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown("**ğŸ“Š Debug Plot 1: Alternative Latest View**")
                            st.caption("ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæœ€æ–°ãƒãƒ¼ã‚±ãƒƒãƒˆç”Ÿãƒ‡ãƒ¼ã‚¿ + æœ€æ–°LPPL + æœ€æ–°äºˆæ¸¬æ—¥ï¼ˆåˆ¥å®Ÿè£…ç¢ºèªï¼‰")
                            
                            latest_fig = go.Figure()
                            
                            # æœ€æ–°ã®ç”Ÿãƒ‡ãƒ¼ã‚¿
                            latest_fig.add_trace(go.Scatter(
                                x=price_data.index,
                                y=lppl_results['normalized_prices'],
                                mode='lines',
                                name='Latest Market Data',
                                line=dict(color='cyan', width=2)
                            ))
                            
                            # æœ€æ–°ã®LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆå…¨æœŸé–“ï¼‰
                            latest_fig.add_trace(go.Scatter(
                                x=price_data.index,
                                y=lppl_results['normalized_fitted'],
                                mode='lines',
                                name='Latest LPPL Fit (Full)',
                                line=dict(color='magenta', width=2.5)
                            ))
                            
                            # æœ€æ–°ã®äºˆæ¸¬æ—¥
                            if pd.notna(latest.get('tc')):
                                latest_pred_date = self.convert_tc_to_real_date(
                                    latest['tc'], data_start, data_end)
                                latest_fig.add_shape(
                                    type="line",
                                    x0=latest_pred_date, x1=latest_pred_date,
                                    y0=0, y1=1,
                                    line=dict(color='yellow', width=3, dash="solid")
                                )
                                latest_fig.add_annotation(
                                    x=latest_pred_date, y=0.9,
                                    text=f"Latest Prediction\n{latest_pred_date.strftime('%m/%d')}",
                                    showarrow=False, font=dict(color='yellow', size=12),
                                    bgcolor="rgba(255, 255, 0, 0.3)"
                                )
                            
                            latest_fig.update_layout(
                                title="Latest Analysis (Most Recent Fitting)",
                                height=400,
                                plot_bgcolor='rgba(20, 30, 20, 0.95)',
                                font=dict(color='white', size=10)
                            )
                            
                            st.plotly_chart(latest_fig, use_container_width=True)
                        
                        with col2:
                            st.markdown("**ğŸ“Š Debug Plot 2: Alternative Integration View**")
                            st.caption("ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šæœŸé–“ç¯„å›²ãƒ‡ãƒ¼ã‚¿ + æœ€è¿‘LPPL + è¤‡æ•°äºˆæ¸¬æ—¥ï¼ˆåˆ¥å®Ÿè£…ç¢ºèªï¼‰")
                            
                            integration_fig = go.Figure()
                            
                            # æœŸé–“ç¯„å›²ã®ç”Ÿãƒ‡ãƒ¼ã‚¿
                            integration_fig.add_trace(go.Scatter(
                                x=price_data.index,
                                y=lppl_results['normalized_prices'],
                                mode='lines',
                                name='Period Market Data',
                                line=dict(color='lightblue', width=2)
                            ))
                            
                            # ã‚µã‚¤ãƒ‰ãƒãƒ¼æœŸé–“å†…ã®æœ€ã‚‚æœ€è¿‘ã®ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
                            recent_analysis = analysis_data.iloc[0]  # æœ€ã‚‚æœ€è¿‘ã®ã‚‚ã®
                            if pd.notna(recent_analysis.get('tc')):
                                recent_params = {
                                    'tc': recent_analysis.get('tc', 1.0),
                                    'beta': recent_analysis.get('beta', 0.33),
                                    'omega': recent_analysis.get('omega', 6.0),
                                    'phi': recent_analysis.get('phi', 0.0),
                                    'A': recent_analysis.get('A', 0.0),
                                    'B': recent_analysis.get('B', 0.0),
                                    'C': recent_analysis.get('C', 0.0)
                                }
                                recent_lppl = self.compute_lppl_fit(price_data['Close'], recent_params)
                                
                                if recent_lppl:
                                    integration_fig.add_trace(go.Scatter(
                                        x=price_data.index,
                                        y=recent_lppl['normalized_fitted'],
                                        mode='lines',
                                        name='Recent Period LPPL Fit',
                                        line=dict(color='orange', width=2.5)
                                    ))
                            
                            # æœŸé–“å†…ã®è¤‡æ•°äºˆæ¸¬ç·š
                            display_count = min(len(analysis_data), 5)
                            for i, (_, pred) in enumerate(analysis_data.head(display_count).iterrows()):
                                if pd.notna(pred.get('tc')):
                                    pred_date = self.convert_tc_to_real_date(
                                        pred['tc'], pred.get('data_period_start', data_start), 
                                        pred.get('data_period_end', data_end))
                                    
                                    color_alpha = 0.8 - i * 0.15
                                    integration_fig.add_shape(
                                        type="line",
                                        x0=pred_date, x1=pred_date,
                                        y0=0, y1=1,
                                        line=dict(color=f'rgba(255, 100, 100, {color_alpha})', 
                                                 width=2, dash="dash")
                                    )
                            
                            integration_fig.update_layout(
                                title="Integration Analysis (Period Range)",
                                height=400,
                                plot_bgcolor='rgba(30, 20, 20, 0.95)',
                                font=dict(color='white', size=10)
                            )
                            
                            st.plotly_chart(integration_fig, use_container_width=True)
                        
                        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
                        with st.expander("ğŸ” Debug Information"):
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write("**Latest Analysis Info:**")
                                st.write(f"- Analysis Basis Date: {latest.get('analysis_basis_date', 'N/A')}")
                                st.write(f"- Data Period: {data_start} to {data_end}")
                                st.write(f"- TC Value: {latest.get('tc', 'N/A')}")
                                st.write(f"- Extended End: {extended_end}")
                                
                            with col2:
                                st.write("**Price Data Info:**")
                                st.write(f"- Price Data Range: {price_data.index.min()} to {price_data.index.max()}")
                                st.write(f"- Data Points: {len(price_data)}")
                                st.write(f"- Max Prediction Date: {max_pred_date}")
                    
                    # äºˆæ¸¬ã‚µãƒãƒªãƒ¼ã‚’è¡¨å½¢å¼ã§è¡¨ç¤º
                    st.subheader("ğŸ”® Prediction Summary")
                    
                    # è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                    summary_data = []
                    valid_predictions = analysis_data.head(display_count)
                    
                    for i, (_, pred) in enumerate(valid_predictions.iterrows()):
                        if pd.notna(pred.get('tc')):
                            pred_tc = pred['tc']
                            pred_start = pred.get('data_period_start', data_start)
                            pred_end = pred.get('data_period_end', data_end)
                            
                            if pred_start and pred_end:
                                pred_date = self.convert_tc_to_real_date(pred_tc, pred_start, pred_end)
                                # å®‰å…¨ãªæ—¥ä»˜æ¯”è¼ƒï¼ˆpred_dateãŒã™ã§ã«datetimeå‹ã®ãŸã‚ä¿®æ­£ä¸è¦ã ãŒå¿µã®ãŸã‚ç¢ºèªï¼‰
                                if isinstance(pred_date, str):
                                    pred_date = pd.to_datetime(pred_date)
                                days_from_today = (pred_date - datetime.now()).days
                                
                                # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã‚’è¡¨ç¤º
                                fitting_basis = pred.get('analysis_basis_date', pred.get('data_period_end', 'N/A'))
                                if fitting_basis != 'N/A':
                                    fitting_basis_dt = pd.to_datetime(fitting_basis)
                                    fitting_basis_str = fitting_basis_dt.strftime('%Y-%m-%d')
                                    days_from_basis = (pred_date - fitting_basis_dt).days
                                else:
                                    fitting_basis_str = 'N/A'
                                    days_from_basis = None
                                
                                # 2ã¤ã®æ—¥æ•°æŒ‡æ¨™ã‚’æº–å‚™
                                days_from_today_str = f"{days_from_today:+d}"
                                if days_from_basis is not None:
                                    days_from_basis_str = f"{days_from_basis:+d}"
                                else:
                                    days_from_basis_str = "N/A"
                                
                                summary_data.append({
                                    'Fitting Basis Date': fitting_basis_str,
                                    'Predicted Crash Date': pred_date.strftime('%Y-%m-%d'),
                                    'Days from Today': days_from_today_str,
                                    'Days from Basis': days_from_basis_str,
                                    'tc Value': f"{pred_tc:.4f}",
                                    'Î² (Beta)': f"{pred.get('beta', 0):.4f}",
                                    'Ï‰ (Omega)': f"{pred.get('omega', 0):.2f}",
                                    'RÂ² Score': f"{pred.get('r_squared', 0):.4f}",
                                    'Quality': pred.get('quality', 'N/A')
                                })
                    
                    if summary_data:
                        # DataFrameã«å¤‰æ›ã—ã¦è¡¨ç¤º
                        summary_df = pd.DataFrame(summary_data)
                        st.dataframe(
                            summary_df,
                            use_container_width=True,
                            hide_index=True,
                            column_config={
                                "Fitting Basis Date": st.column_config.TextColumn("Fitting Basis Date", help="Date when the fitting was performed"),
                                "Predicted Crash Date": st.column_config.TextColumn("Predicted Crash Date", help="Date when crash is predicted"),
                                "Days from Today": st.column_config.TextColumn("Days from Today", help="Days from current date to predicted crash"),
                                "Days from Basis": st.column_config.TextColumn("Days from Basis", help="Days from fitting basis date to predicted crash (prediction horizon)"),
                                "tc Value": st.column_config.TextColumn("tc Value", help="Critical time parameter"),
                                "Î² (Beta)": st.column_config.TextColumn("Î² (Beta)", help="Critical exponent (typically ~0.33)"),
                                "Ï‰ (Omega)": st.column_config.TextColumn("Ï‰ (Omega)", help="Angular frequency of oscillations"),
                                "RÂ² Score": st.column_config.TextColumn("RÂ² Score", help="Goodness of fit (higher is better)"),
                                "Quality": st.column_config.TextColumn("Quality", help="Overall analysis quality assessment")
                            }
                        )
                        st.caption(f"ğŸ“Š Showing {len(summary_data)} prediction results from the selected analysis period")
                    else:
                        st.warning("No valid predictions found in the selected period")
                    
                    # å€‹åˆ¥ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœè¡¨ç¤ºæ©Ÿèƒ½ã‚’è¿½åŠ 
                    st.subheader("ğŸ“Š Individual Fitting Results")
                    
                    # è¡¨ç¤ºæ•°ã‚’çµ±åˆãƒ—ãƒ­ãƒƒãƒˆã¨ä¸€è‡´ã•ã›ã‚‹ï¼ˆAnalysis Period Selectionã¨é€£å‹•ï¼‰
                    individual_display_count = len(analysis_data)  # æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿å…¨ä»¶è¡¨ç¤º
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®ã§ä¸Šé™è¨­å®šï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿æ™‚ï¼‰
                    is_limited = individual_display_count > 20
                    if is_limited:
                        individual_display_count = 20
                        st.markdown(f"*Displaying latest {individual_display_count} results out of {len(analysis_data)} total analyses (performance optimization)*")
                        # ä¸Šéƒ¨ã«è­¦å‘Šè¡¨ç¤º
                        st.warning(f"âš ï¸ **Performance Note**: Showing the most recent {individual_display_count} individual analyses from the selected period for optimal performance. To view older analyses, please adjust the period selection in the sidebar.")
                    else:
                        st.markdown(f"*Displaying all {individual_display_count} individual analyses from the selected period*")
                    
                    st.caption("Each plot shows an individual analysis with its own fitting period and prediction")
                    
                    for i, (_, individual) in enumerate(analysis_data.head(individual_display_count).iterrows()):
                        if pd.notna(individual.get('tc')):
                            ind_tc = individual['tc']
                            # ğŸ”§ FRED APIä¿®æ­£: Timestamp ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å®‰å…¨å¤‰æ›
                            ind_start = self._ensure_date_string(individual.get('data_period_start'))
                            ind_end = self._ensure_date_string(individual.get('data_period_end'))
                            
                            if ind_start and ind_end:
                                # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã‚’å–å¾—
                                fitting_basis_date = individual.get('analysis_basis_date', ind_end)
                                fitting_basis_dt = pd.to_datetime(fitting_basis_date)
                                
                                st.markdown(f"---")
                                st.markdown(f"**Analysis #{i+1} - Fitting Basis: {fitting_basis_dt.strftime('%Y-%m-%d')}**")
                                
                                # ğŸ”§ APIåŠ¹ç‡åŒ–: æ—¢ã«å–å¾—æ¸ˆã¿ã®æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¿…è¦æœŸé–“ã‚’æŠ½å‡º
                                if price_data is not None and not price_data.empty:
                                    # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©²å½“æœŸé–“ã‚’æŠ½å‡º
                                    ind_start_dt = pd.to_datetime(ind_start)
                                    ind_end_dt = pd.to_datetime(ind_end)
                                    
                                    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆå¤šå°‘ã®ä½™è£•ã‚’æŒã£ã¦åˆ¤å®šï¼‰
                                    data_start_dt = price_data.index.min()
                                    data_end_dt = price_data.index.max()
                                    
                                    # ğŸ”§ APIåŠ¹ç‡åŒ–æ”¹å–„: å°‘ã—ã§ã‚‚é‡è¤‡ãŒã‚ã‚Œã°æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                                    available_data_in_range = price_data.loc[
                                        (price_data.index >= ind_start_dt) & (price_data.index <= ind_end_dt)
                                    ]
                                    
                                    if len(available_data_in_range) >= 30:  # æœ€ä½30æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ä½¿ç”¨
                                        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœŸé–“æŠ½å‡ºï¼ˆAPIå‘¼ã³å‡ºã—ä¸è¦ï¼‰
                                        individual_data = available_data_in_range.copy()
                                        print(f"ğŸ”„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœŸé–“æŠ½å‡º: {symbol} {ind_start} to {ind_end} - {len(individual_data)}æ—¥åˆ†")
                                    else:
                                        # ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®ã¿APIå‘¼ã³å‡ºã—
                                        individual_data = self.get_symbol_price_data(symbol, ind_start, ind_end)
                                        print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚APIå‘¼ã³å‡ºã—: {symbol} (æ—¢å­˜:{len(available_data_in_range)}æ—¥ < 30æ—¥)")
                                else:
                                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ‹¡å¼µãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—æ™‚ã®ã¿APIå‘¼ã³å‡ºã—
                                    individual_data = self.get_symbol_price_data(symbol, ind_start, ind_end)
                                
                                if individual_data is not None and not individual_data.empty and 'Close' in individual_data.columns:
                                    # LPPLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                                    individual_params = {
                                        'tc': individual.get('tc', 1.0),
                                        'beta': individual.get('beta', 0.33),
                                        'omega': individual.get('omega', 6.0),
                                        'phi': individual.get('phi', 0.0),
                                        'A': individual.get('A', 0.0),
                                        'B': individual.get('B', 0.0),
                                        'C': individual.get('C', 0.0)
                                    }
                                    
                                    # LPPLãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’è¨ˆç®—
                                    individual_lppl = self.compute_lppl_fit(individual_data['Close'], individual_params)
                                    
                                    if individual_lppl:
                                        # å€‹åˆ¥ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
                                        individual_fig = go.Figure()
                                        
                                        # å®Ÿãƒ‡ãƒ¼ã‚¿
                                        individual_fig.add_trace(go.Scatter(
                                            x=individual_data.index,
                                            y=individual_lppl['normalized_prices'],
                                            mode='lines',
                                            name='Market Data',
                                            line=dict(color='lightblue', width=2)
                                        ))
                                        
                                        # LPPLãƒ•ã‚£ãƒƒãƒˆï¼ˆåŸºæº–æ—¥ã¾ã§ï¼‰
                                        basis_mask = individual_data.index <= fitting_basis_dt
                                        individual_fig.add_trace(go.Scatter(
                                            x=individual_data.index[basis_mask],
                                            y=individual_lppl['normalized_fitted'][basis_mask],
                                            mode='lines',
                                            name='LPPL Fit (Basis Period)',
                                            line=dict(color='red', width=2.5)
                                        ))
                                        
                                        # LPPLãƒ•ã‚£ãƒƒãƒˆï¼ˆåŸºæº–æ—¥ä»¥é™ï¼‰- æ‹¡å¼µç‰ˆFuture Period
                                        future_mask = individual_data.index > fitting_basis_dt
                                        
                                        # æ‹¡å¼µFuture Periodè¨ˆç®—
                                        individual_pred_date = self.convert_tc_to_real_date(ind_tc, ind_start, ind_end)
                                        extended_individual_lppl = self.compute_extended_lppl_fit(
                                            individual_data['Close'], individual_params, 
                                            fitting_basis_dt, individual_pred_date + timedelta(days=30))
                                        
                                        if extended_individual_lppl and len(extended_individual_lppl['future_dates']) > 0:
                                            # æ‹¡å¼µFuture Periodè¡¨ç¤º
                                            individual_fig.add_trace(go.Scatter(
                                                x=extended_individual_lppl['future_dates'],
                                                y=extended_individual_lppl['normalized_fitted'],
                                                mode='lines',
                                                name='LPPL Fit (Future Period)',
                                                line=dict(color='orange', width=2.5, dash='dot'),
                                                opacity=0.8
                                            ))
                                        elif future_mask.any():
                                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®Future Period
                                            individual_fig.add_trace(go.Scatter(
                                                x=individual_data.index[future_mask],
                                                y=individual_lppl['normalized_fitted'][future_mask],
                                                mode='lines',
                                                name='LPPL Fit (Future Period)',
                                                line=dict(color='orange', width=2.5, dash='dot')
                                            ))
                                        
                                        # äºˆæ¸¬ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥ã®ç¸¦ç·šï¼ˆãƒ‡ãƒ¼ã‚¿ç¯„å›²å…¨ä½“ã«è¡¨ç¤ºï¼‰
                                        # Yè»¸ã®ç¯„å›²ã‚’å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã‚‹
                                        y_min = min(individual_lppl['normalized_prices'].min(), 
                                                   individual_lppl['normalized_fitted'].min())
                                        y_max = max(individual_lppl['normalized_prices'].max(), 
                                                   individual_lppl['normalized_fitted'].max())
                                        
                                        # Future Periodã®ãƒ‡ãƒ¼ã‚¿ã‚‚è€ƒæ…®
                                        if extended_individual_lppl and len(extended_individual_lppl['future_dates']) > 0:
                                            y_max = max(y_max, max(extended_individual_lppl['normalized_fitted']))
                                        
                                        # ç¸¦ç·šã‚’æœ€å¾Œã«æç”»ï¼ˆä»–ã®ãƒ—ãƒ­ãƒƒãƒˆã‚ˆã‚Šä¸Šã«è¡¨ç¤ºï¼‰
                                        individual_fig.add_shape(
                                            type="line",
                                            x0=individual_pred_date,
                                            x1=individual_pred_date,
                                            y0=y_min * 0.98,  # å°‘ã—ä¸‹ã‹ã‚‰
                                            y1=y_max * 1.02,  # å°‘ã—ä¸Šã¾ã§
                                            line=dict(color='rgba(255, 100, 100, 0.8)', width=3, dash="dash"),
                                            layer='above'  # ä»–ã®è¦ç´ ã‚ˆã‚Šä¸Šã«æç”»
                                        )
                                        
                                        individual_fig.add_annotation(
                                            x=individual_pred_date,
                                            y=0.9,
                                            text=individual_pred_date.strftime('%m/%d'),
                                            showarrow=False,
                                            font=dict(size=10, color='white'),
                                            bgcolor="rgba(0, 0, 0, 0.7)"
                                        )
                                        
                                        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆXè»¸ç¯„å›²ã‚’äºˆæ¸¬æ—¥+30æ—¥ã¾ã§æ‹¡å¼µï¼‰
                                        x_range_end = max(individual_data.index.max(), 
                                                        individual_pred_date + timedelta(days=30))
                                        individual_fig.update_layout(
                                            title=f"Individual Analysis - Fitted on {fitting_basis_dt.strftime('%Y-%m-%d')}",
                                            height=400,
                                            plot_bgcolor='rgba(20, 20, 30, 0.95)',
                                            paper_bgcolor='rgba(15, 15, 25, 0.95)',
                                            font=dict(color='white'),
                                            xaxis=dict(
                                                gridcolor='rgba(100, 100, 100, 0.2)',
                                                showgrid=True,
                                                gridwidth=1,
                                                range=[individual_data.index.min(), x_range_end]
                                            ),
                                            yaxis=dict(
                                                gridcolor='rgba(100, 100, 100, 0.2)',
                                                showgrid=True,
                                                gridwidth=1
                                            )
                                        )
                                        
                                        st.plotly_chart(individual_fig, use_container_width=True, key=f"individual_pred_{symbol}_{i}")
                                        
                                        # å€‹åˆ¥çµæœã®çµ±è¨ˆ
                                        col1, col2, col3, col4 = st.columns(4)
                                        with col1:
                                            st.metric("Predicted Crash", individual_pred_date.strftime('%Y-%m-%d'))
                                        with col2:
                                            st.metric("RÂ² Score", f"{individual.get('r_squared', 0):.4f}")
                                        with col3:
                                            st.metric("Quality", individual.get('quality', 'N/A'))
                                        with col4:
                                            st.metric("tc Value", f"{ind_tc:.4f}")
                                    else:
                                        st.error(f"LPPL calculation failed for analysis #{i+1}")
                                else:
                                    st.warning(f"Unable to retrieve data for analysis #{i+1}")
                    
                    # ä¸‹éƒ¨ã«ã‚‚è­¦å‘Šè¡¨ç¤ºï¼ˆ20ä»¶åˆ¶é™ãŒã‚ã‚‹å ´åˆï¼‰
                    if is_limited:
                        st.markdown("---")
                        st.warning(f"âš ï¸ **Performance Note**: You have reached the display limit of {individual_display_count} analyses. There are {len(analysis_data) - individual_display_count} additional older analyses available. To view these, please adjust the period selection in the sidebar to focus on a different time range.")
                else:
                    st.error("LPPL ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ")
            else:
                # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¡¨ç¤º
                if price_data is None:
                    st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {symbol} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    st.info(f"æœŸé–“: {data_start} ã‹ã‚‰ {data_end}")
                    st.info("UnifiedDataClientã®åˆæœŸåŒ–ã¾ãŸã¯APIèªè¨¼ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                elif price_data.empty:
                    st.warning(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™: {symbol} ã®æŒ‡å®šæœŸé–“ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    st.info(f"æœŸé–“: {data_start} ã‹ã‚‰ {data_end}")
                elif 'Close' not in price_data.columns:
                    st.warning(f"âš ï¸ ä¾¡æ ¼åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {symbol} ãƒ‡ãƒ¼ã‚¿ã«'Close'åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
                    st.info(f"åˆ©ç”¨å¯èƒ½ãªåˆ—: {list(price_data.columns)}")
                else:
                    st.warning("â“ ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚‚è¡¨ç¤º
                st.subheader("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±")
                st.json({
                    "Symbol": symbol,
                    "Data Source": latest.get('data_source', 'N/A'),
                    "Period Start": data_start,
                    "Period End": data_end,
                    "Data Points": latest.get('data_points', 'N/A'),
                    "Analysis Basis Date": latest.get('analysis_basis_date', 'N/A')
                })
        else:
            st.error("âŒ ãƒ‡ãƒ¼ã‚¿æœŸé–“æƒ…å ±ãŒä¸å®Œå…¨ã§ã™")
            st.info("data_period_start ã¾ãŸã¯ data_period_end ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            st.subheader("ğŸ“Š åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
            debug_info = {}
            for col in ['data_period_start', 'data_period_end', 'data_source', 'analysis_basis_date', 'data_points']:
                val = latest.get(col)
                debug_info[col] = str(val) if val is not None else "None"
            
            st.json(debug_info)
    
    def render_prediction_data_tab(self, symbol: str, analysis_data: pd.DataFrame):
        """Tab 1: Crash Prediction Data Visualization"""
        
        st.header(f"ğŸ“Š {symbol} - Crash Prediction Data")
        
        if analysis_data.empty:
            st.warning("No analysis data available for this symbol")
            return
        
        valid_data = analysis_data.dropna(subset=['predicted_crash_date'])
        
        if valid_data.empty:
            st.warning("No valid prediction data available")
            return
        
        # Add fitting_basis_date column to valid_data for multi-period analysis
        fitting_basis_dates_valid = []
        for _, row in valid_data.iterrows():
            # å„ªå…ˆé †ä½: analysis_basis_date > data_period_end > analysis_date
            for col in ['analysis_basis_date', 'data_period_end', 'data_end', 'end_date', 'analysis_date']:
                if col in valid_data.columns and pd.notna(row.get(col)):
                    fitting_basis_dates_valid.append(pd.to_datetime(row[col]))
                    break
            else:
                fitting_basis_dates_valid.append(pd.to_datetime(row.get('analysis_date', datetime.now())))
        
        valid_data = valid_data.copy()  # Make a copy to avoid modifying the original
        valid_data['fitting_basis_date'] = fitting_basis_dates_valid
        
        # Analysis Data Period functionality
        st.subheader("ğŸ“… Analysis Data Period")
        st.caption("è§£æãƒ‡ãƒ¼ã‚¿ã®å¯¾è±¡ã¨ã™ã‚‹æœŸé–“")
        
        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®è¨ˆç®—ï¼ˆFrom/Toã®ä¸‹ã«è¡¨ç¤ºç”¨ï¼‰
        original_data = self.get_symbol_analysis_data(symbol, limit=1000)  # ãƒ•ã‚£ãƒ«ã‚¿å‰ã®å…¨ãƒ‡ãƒ¼ã‚¿
        if not original_data.empty:
            original_data['analysis_basis_date'] = pd.to_datetime(original_data['analysis_basis_date'])
            full_min_date = original_data['analysis_basis_date'].min()
            full_max_date = original_data['analysis_basis_date'].max()
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            if 'prediction_data_from_date' not in st.session_state:
                st.session_state.prediction_data_from_date = valid_data['fitting_basis_date'].min().date()
            from_date = st.date_input("From", st.session_state.prediction_data_from_date, key='prediction_data_from_date_input')
            st.session_state.prediction_data_from_date = from_date
            # Oldest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Oldest Analysis: {full_min_date.strftime('%Y-%m-%d')}")
            
        with col2:
            if 'prediction_data_to_date' not in st.session_state:
                st.session_state.prediction_data_to_date = valid_data['fitting_basis_date'].max().date()
            to_date = st.date_input("To", st.session_state.prediction_data_to_date, key='prediction_data_to_date_input')
            st.session_state.prediction_data_to_date = to_date
            # Latest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Latest Analysis: {full_max_date.strftime('%Y-%m-%d')}")
        
        # ğŸ“Š é¸æŠæœŸé–“ã®è¦–è¦šè¡¨ç¤ºï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã¿ãƒ»ã‚ˆã‚Šå¹…åºƒãï¼‰
        if not original_data.empty:
            selected_min = pd.to_datetime(from_date)
            selected_max = pd.to_datetime(to_date)
            
            # æœŸé–“ã®å‰²åˆè¨ˆç®—
            total_days = (full_max_date - full_min_date).days
            selected_duration = (selected_max - selected_min).days
            
            # é¸æŠæœŸé–“ã®é–‹å§‹ä½ç½®ã¨é•·ã•ã‚’è¨ˆç®—
            start_offset = (selected_min - full_min_date).days if total_days > 0 else 0
            selected_ratio = selected_duration / total_days if total_days > 0 else 1.0
        
        st.markdown("---")
        
        # Period ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_data['fitting_basis_date'] = pd.to_datetime(valid_data['fitting_basis_date'])
        from_datetime = pd.to_datetime(from_date)
        to_datetime = pd.to_datetime(to_date)
        
        date_mask = (valid_data['fitting_basis_date'] >= from_datetime) & (valid_data['fitting_basis_date'] <= to_datetime)
        valid_data = valid_data[date_mask].copy()
        
        if len(valid_data) == 0:
            st.warning(f"No data available for selected period: {from_date} to {to_date}")
            return
        
        # Main scatter plot: Analysis date vs Predicted crash date
        st.subheader("ğŸ“Š Crash Prediction Data")
        
        fig = go.Figure()
        
        # Prepare data for plotting
        plot_data = valid_data.copy()
        
        # Get fitting basis dates (ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã‚’å„ªå…ˆçš„ã«å–å¾—)
        fitting_basis_dates = []
        for _, row in plot_data.iterrows():
            # å„ªå…ˆé †ä½: analysis_basis_date > data_period_end > analysis_date
            for col in ['analysis_basis_date', 'data_period_end', 'data_end', 'end_date', 'analysis_date']:
                if col in plot_data.columns and pd.notna(row.get(col)):
                    fitting_basis_dates.append(pd.to_datetime(row[col]))
                    break
            else:
                fitting_basis_dates.append(pd.to_datetime(row.get('analysis_date', datetime.now())))
        
        plot_data['fitting_basis_date'] = fitting_basis_dates
        
        # Convert predicted crash datesï¼ˆå®‰å…¨ãªå¤‰æ›ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¿½åŠ ï¼‰
        crash_dates = []
        for date in plot_data['predicted_crash_date']:
            try:
                if pd.isna(date) or date is None:
                    crash_dates.append(None)
                elif hasattr(date, 'to_pydatetime'):
                    crash_dates.append(date.to_pydatetime())
                else:
                    # æ–‡å­—åˆ—ã‚„ä»–ã®å½¢å¼ã‚’å®‰å…¨ã«Timestampã«å¤‰æ›
                    converted_date = pd.to_datetime(date)
                    if hasattr(converted_date, 'to_pydatetime'):
                        crash_dates.append(converted_date.to_pydatetime())
                    else:
                        crash_dates.append(converted_date)
            except (ValueError, TypeError, pd.errors.OutOfBoundsDatetime):
                # å¤‰æ›ã§ããªã„å ´åˆã¯Noneã‚’è¨­å®š
                crash_dates.append(None)
        
        plot_data['crash_date_converted'] = crash_dates
        
        # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ã‹ã‚‰äºˆæ¸¬ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥ã¾ã§ã®æ—¥æ•°ã‚’è¨ˆç®—ï¼ˆå®‰å…¨ãªå‡¦ç†ï¼‰
        hover_texts = []
        for _, row in plot_data.iterrows():
            fitting_basis_date = row['fitting_basis_date']
            crash_date = row['crash_date_converted']
            
            try:
                # åŸºæº–æ—¥ã‹ã‚‰ã‚¯ãƒ©ãƒƒã‚·ãƒ¥äºˆæƒ³æ—¥ã¾ã§ã®æ—¥æ•°ã‚’è¨ˆç®—ï¼ˆå®‰å…¨ãªå‡¦ç†ï¼‰
                if crash_date is None or pd.isna(crash_date):
                    days_to_crash = "N/A"
                else:
                    # ä¸¡æ–¹ã®æ—¥ä»˜ã‚’ç¢ºå®Ÿã«Timestamp/datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                    if isinstance(fitting_basis_date, str):
                        fitting_basis_date = pd.to_datetime(fitting_basis_date)
                    if isinstance(crash_date, str):
                        crash_date = pd.to_datetime(crash_date)
                    
                    days_to_crash = (crash_date - fitting_basis_date).days
            except (ValueError, TypeError, AttributeError) as e:
                st.warning(f"æ—¥æ•°è¨ˆç®—ã‚¨ãƒ©ãƒ¼ (è¡Œ{_}): {str(e)}")
                days_to_crash = "Error"
            
            # hover_textä½œæˆï¼ˆdays_to_crashã®ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é©åˆ‡ã«è¡¨ç¤ºï¼‰
            if isinstance(days_to_crash, (int, float)):
                days_text = f"Days to Crash: {days_to_crash} days<br>"
            else:
                days_text = f"Days to Crash: {days_to_crash}<br>"
            
            hover_text = (days_text +
                         f"RÂ²: {row['r_squared']:.3f}<br>" +
                         f"Quality: {row['quality']}")
            hover_texts.append(hover_text)
        
        # Color by RÂ² score
        fig.add_trace(go.Scatter(
            x=plot_data['fitting_basis_date'],
            y=plot_data['crash_date_converted'],
            mode='markers',
            marker=dict(
                size=10,
                color=plot_data['r_squared'],
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(
                    title="RÂ² Score",
                    x=1.02,  # Move colorbar further right to avoid overlap
                    len=0.7,  # Make colorbar shorter
                    y=0.5     # Center vertically
                )
            ),
            text=hover_texts,
            hovertemplate='Fitting Basis Date: %{x}<br>Predicted Crash: %{y}<br>%{text}<extra></extra>',
            name='Predictions'
        ))
        
        # Add y=x reference line (predictions that match fitting date)
        x_range = [plot_data['fitting_basis_date'].min(), plot_data['fitting_basis_date'].max()]
        y_range = [plot_data['crash_date_converted'].min(), plot_data['crash_date_converted'].max()]
        
        # Limit the line to scatter plot x-range to avoid excessive white space
        line_start = min(x_range[0], y_range[0])
        line_end = x_range[1]  # Use latest fitting basis date as end point
        
        # Always add the y=x reference line (without annotations to avoid overlap)
        fig.add_trace(go.Scatter(
            x=[line_start, line_end],
            y=[line_start, line_end],
            mode='lines',
            line=dict(color='lightblue', width=1, dash='solid'),  # é’ç³»ã€ç´°ç·šã€å®Ÿç·š
            name='Reference Line',
            showlegend=False,  # Remove from legend to avoid clutter
            hoverinfo='skip'   # Remove hover info to avoid annotation overlap
        ))
        
        fig.update_layout(
            title=f"{symbol} - Prediction Data Visualization",
            xaxis_title="Fitting Basis Date",
            yaxis_title="Predicted Crash Date",
            height=600,
            hovermode='closest'
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Reference line explanation
        st.info("""
        ğŸ“ **Reference Line (Light Blue)**: The diagonal line represents the theoretical case where the predicted crash date equals the fitting basis date. If points are on the line, predictions suggest crashes on the same day as fitting (immediate risk).
        """)
        
        
    
    def render_crash_clustering_tab(self, symbol: str, analysis_data: pd.DataFrame):
        """
        æ–°ã‚¿ãƒ–: Crash Prediction Clustering (I052å®Ÿè£…) - ãƒ‡ãƒãƒƒã‚°ç°¡ç´ ç‰ˆ
        """
        
        st.header(f"ğŸ¯ {symbol} - Prediction Clustering")
        
        if analysis_data.empty:
            st.warning("No analysis data available for clustering")
            return
        
        
        # Clustering Analysis Settings
        st.subheader("âš™ï¸ Clustering Analysis Settings")
        
        # æœŸé–“è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³
        st.markdown("### ğŸ“… Analysis Data Period")
        st.caption("è§£æãƒ‡ãƒ¼ã‚¿ã®å¯¾è±¡ã¨ã™ã‚‹æœŸé–“")
        
        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®è¨ˆç®—ï¼ˆFrom/Toã®ä¸‹ã«è¡¨ç¤ºç”¨ï¼‰
        original_data = self.get_symbol_analysis_data(symbol, limit=1000)  # ãƒ•ã‚£ãƒ«ã‚¿å‰ã®å…¨ãƒ‡ãƒ¼ã‚¿
        if not original_data.empty:
            original_data['analysis_basis_date'] = pd.to_datetime(original_data['analysis_basis_date'])
            full_min_date = original_data['analysis_basis_date'].min()
            full_max_date = original_data['analysis_basis_date'].max()
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            if 'clustering_from_date' not in st.session_state:
                st.session_state.clustering_from_date = analysis_data['analysis_basis_date'].min().date()
            from_date = st.date_input("From", st.session_state.clustering_from_date, key='clustering_from_date_input')
            st.session_state.clustering_from_date = from_date
            # Oldest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Oldest Analysis: {full_min_date.strftime('%Y-%m-%d')}")
            
        with col2:
            if 'clustering_to_date' not in st.session_state:
                st.session_state.clustering_to_date = analysis_data['analysis_basis_date'].max().date()
            to_date = st.date_input("To", st.session_state.clustering_to_date, key='clustering_to_date_input')
            st.session_state.clustering_to_date = to_date
            # Latest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Latest Analysis: {full_max_date.strftime('%Y-%m-%d')}")
        
        # ğŸ“Š é¸æŠæœŸé–“ã®è¦–è¦šè¡¨ç¤ºï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã¿ãƒ»ã‚ˆã‚Šå¹…åºƒãï¼‰
        if not original_data.empty:
            selected_min = pd.to_datetime(from_date)
            selected_max = pd.to_datetime(to_date)
            
            # æœŸé–“ã®å‰²åˆè¨ˆç®—
            total_days = (full_max_date - full_min_date).days
            selected_duration = (selected_max - selected_min).days
            
            # é¸æŠæœŸé–“ã®é–‹å§‹ä½ç½®ã¨é•·ã•ã‚’è¨ˆç®—
            start_offset = (selected_min - full_min_date).days if total_days > 0 else 0
            selected_ratio = selected_duration / total_days if total_days > 0 else 1.0
        
        st.markdown("---")
        
        # Display Period ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        analysis_data['analysis_basis_date'] = pd.to_datetime(analysis_data['analysis_basis_date'])
        from_datetime = pd.to_datetime(from_date)
        to_datetime = pd.to_datetime(to_date)
        
        date_mask = (analysis_data['analysis_basis_date'] >= from_datetime) & (analysis_data['analysis_basis_date'] <= to_datetime)
        analysis_data = analysis_data[date_mask].copy()
        
        if len(analysis_data) == 0:
            st.warning(f"No data available for selected period: {from_date} to {to_date}")
            return
        
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™ - æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨æ¸ˆã¿ã®analysis_dataã‚’ä½¿ç”¨
        valid_data = analysis_data.dropna(subset=['predicted_crash_date', 'analysis_basis_date']).copy()
        
        if len(valid_data) < 5:
            st.warning(f"Insufficient data for clustering analysis (need at least 5 points, have {len(valid_data)})")
            return
        
        # æ—¥ä»˜å¤‰æ›
        valid_data['basis_date'] = pd.to_datetime(valid_data['analysis_basis_date'])
        valid_data['crash_date'] = pd.to_datetime(valid_data['predicted_crash_date'])
        
        # æ•°å€¤åŒ–ï¼ˆåˆ†æç”¨ï¼‰
        base_date = valid_data['basis_date'].min()  # å…±é€šåŸºæº–æ—¥
        valid_data['basis_days'] = (valid_data['basis_date'] - base_date).dt.days
        valid_data['crash_days'] = (valid_data['crash_date'] - base_date).dt.days
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        st.markdown("### ğŸ¯ Clustering Parameters")
        
        # ãƒ˜ãƒ«ãƒ—æƒ…å ±ã‚’expanderã«è¿½åŠ 
        with st.expander("â„¹ï¸ Understanding Clustering Parameters & Methods", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                **ğŸ“Š RÂ²-Weighted Average Method:**
                - Uses RÂ² scores to weight each prediction's influence
                - Higher RÂ² predictions have more impact on the cluster center
                - No time-series assumptions - purely quality-based weighting
                - Center line shows the weighted average prediction date
                
                **ğŸ“ Clustering Parameters:**
                - **Distance (days)**: Maximum time gap between predictions to group them
                - **Min Cluster Size**: Minimum predictions needed to form a valid cluster
                - **Min RÂ²**: Quality threshold for including data in clustering
                - **Min Days to Crash**: Minimum time between fitting date and predicted crash date
                """)
                
            with col2:
                st.markdown("""
                **ğŸ¯ Confidence Levels:**
                - **High**: Avg RÂ² > 0.7 and cluster size â‰¥ 5
                - **Medium**: Avg RÂ² > 0.5 and cluster size â‰¥ 3  
                - **Low**: Below medium criteria
                
                **ğŸ“ˆ Reference Line Interpretation:**
                - Blue dotted diagonal line where Fitting Date = Crash Date
                - **Above line**: Predicted crash is in the future
                - **On line**: Predicted crash is imminent (same day as fitting)
                - **Below line**: Predicted crash is in the past (expired prediction)
                """)
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã®ä¸€æ™‚å¤‰æ•°ï¼ˆApplyã¾ã§åæ˜ ã•ã‚Œãªã„ï¼‰
            if 'clustering_eps_days_preview' not in st.session_state:
                st.session_state.clustering_eps_days_preview = st.session_state.get('clustering_eps_days', 30)
            eps_days_preview = st.slider("Distance", 10, 90, 
                                       st.session_state.clustering_eps_days_preview, 
                                       key='eps_days_slider', help="Max days between predictions to be in same cluster")
            st.caption("days")
            
        with col2:
            if 'clustering_min_samples_preview' not in st.session_state:
                st.session_state.clustering_min_samples_preview = st.session_state.get('clustering_min_samples', 3)
            min_samples_preview = st.slider("Min Cluster", 2, 20, 
                                          st.session_state.clustering_min_samples_preview,
                                          key='min_samples_slider', help="Minimum predictions to form a cluster. Default: 3 for better cluster formation")
            st.caption("size")
            
        with col3:
            if 'clustering_r2_threshold_preview' not in st.session_state:
                st.session_state.clustering_r2_threshold_preview = st.session_state.get('clustering_r2_threshold', 0.8)
            r2_threshold_preview = st.slider("Min RÂ²", 0.0, 1.0, 
                                           st.session_state.clustering_r2_threshold_preview, 0.05,
                                           key='r2_threshold_slider', help="Minimum RÂ² value to include in clustering. Only data points with RÂ² above this threshold will be used for clustering analysis (Data Quality Filter)")
            
        with col4:
            if 'clustering_min_horizon_days_preview' not in st.session_state:
                st.session_state.clustering_min_horizon_days_preview = st.session_state.get('clustering_min_horizon_days', 21)
            min_horizon_days_preview = st.slider(
                "Min Days to Crash", 
                min_value=0, max_value=90, 
                value=st.session_state.clustering_min_horizon_days_preview,
                key='min_horizon_days_slider',
                help="Minimum days between fitting date and predicted crash date (excludes near-crash predictions with low accuracy)"
            )
            st.caption("days")
        
        # Min Days to Crash Details Button
        if st.button("â“ Min Days to Crash - Sornette Research Details", help="Click for scientific background and filter rationale"):
            st.session_state.show_horizon_details = not st.session_state.get('show_horizon_details', False)
            
        # è©³ç´°èª¬æ˜ã®è¡¨ç¤ºåˆ¶å¾¡
        if st.session_state.get('show_horizon_details', False):
            with st.expander("ğŸ“š Sornette Research: Prediction Horizon Theory", expanded=True):
                st.markdown(f"""
                **ğŸ¯ Current Filter Setting**: Excluding predictions within **{min_horizon_days_preview} days** of fitting date.
                
                **ğŸ“– Sornette Research Evidence**:
                - **Optimal Prediction Window**: 1-6 months ahead
                - **Minimum Practical Horizon**: ~30 days
                - **Near-Crash Problem**: Fittings too close to critical time suffer from:
                  - Increased noise sensitivity
                  - LPPL singularity effects
                  - Degraded predictive accuracy
                
                **âš™ï¸ Implementation**:
                - **0 days**: No filtering (include all predictions)
                - **10-30 days**: Conservative filtering (recommended)
                - **30+ days**: Strict filtering (Sornette theoretical minimum)
                
                **âœ… Scientific Basis**: Based on LPPL model behavior near critical time points.
                """)
                
                if st.button("âœ–ï¸ Close Details"):
                    st.session_state.show_horizon_details = False
        
        # Applyãƒœã‚¿ãƒ³ï¼ˆã™ã¹ã¦ã®è¨­å®šã‚’ä¸€åº¦ã«é©ç”¨ï¼‰
        st.markdown("---")
        col1, col2, col3 = st.columns([1, 3, 1])
        with col2:
            apply_settings = st.button("ğŸ”„ Apply Analysis Settings", type="primary", key='clustering_apply_settings',
                                      help="Apply all settings above: Analysis Period + Clustering Parameters", 
                                      use_container_width=True)
        
        # Applyãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã€ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼å€¤ã‚’å®Ÿéš›ã®å€¤ã«ã‚³ãƒ”ãƒ¼
        if apply_settings:
            st.session_state.clustering_period_applied = True
            st.session_state.clustering_eps_days = eps_days_preview
            st.session_state.clustering_min_samples = min_samples_preview
            st.session_state.clustering_r2_threshold = r2_threshold_preview
            st.session_state.clustering_min_horizon_days = min_horizon_days_preview
            
        # åˆå›è¡¨ç¤ºæ™‚ã®å‡¦ç†
        if 'clustering_period_applied' not in st.session_state:
            st.info("ğŸ’¡ **Getting Started**: Configure your settings above and click 'Apply' to start clustering analysis.")
            return
        
        # å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹å€¤ï¼ˆApplyå¾Œã®å€¤ï¼‰
        eps_days = st.session_state.get('clustering_eps_days', 30)
        min_samples = st.session_state.get('clustering_min_samples', 3)
        r2_threshold = st.session_state.get('clustering_r2_threshold', 0.8)
        min_horizon_days = st.session_state.get('clustering_min_horizon_days', 21)
            
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™ - æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é©ç”¨æ¸ˆã¿ã®analysis_dataã‚’ä½¿ç”¨
        valid_data = analysis_data.dropna(subset=['predicted_crash_date', 'analysis_basis_date']).copy()
        
        if len(valid_data) < 5:
            st.warning(f"Insufficient data for clustering analysis (need at least 5 points, have {len(valid_data)})")
            return
        
        # æ—¥ä»˜å¤‰æ›
        valid_data['basis_date'] = pd.to_datetime(valid_data['analysis_basis_date'])
        valid_data['crash_date'] = pd.to_datetime(valid_data['predicted_crash_date'])
        
        # äºˆæ¸¬æœŸé–“è¨ˆç®—ï¼ˆæœ€å°äºˆæ¸¬æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç”¨ï¼‰
        valid_data['prediction_horizon'] = (valid_data['crash_date'] - valid_data['basis_date']).dt.days
        
        # æœ€å°äºˆæ¸¬æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
        horizon_filtered_data = valid_data[valid_data['prediction_horizon'] >= min_horizon_days].copy()
        
        if len(horizon_filtered_data) < 5:
            st.warning(f"""
            **Insufficient Data After Horizon Filtering**
            
            After applying the {min_horizon_days}-day minimum prediction horizon, only {len(horizon_filtered_data)} data points remain.
            
            **ğŸ’¡ Solutions:**
            1. **Reduce Min Horizon**: Try {max(0, min_horizon_days-5)}-{min_horizon_days-1} days
            2. **Expand Analysis Period**: Select a longer time range
            3. **Review Filter Settings**: Consider if {min_horizon_days}-day horizon is too restrictive
            """)
            return
        
        # æ•°å€¤åŒ–ï¼ˆåˆ†æç”¨ï¼‰
        base_date = horizon_filtered_data['basis_date'].min()  # å…±é€šåŸºæº–æ—¥
        horizon_filtered_data['basis_days'] = (horizon_filtered_data['basis_date'] - base_date).dt.days
        horizon_filtered_data['crash_days'] = (horizon_filtered_data['crash_date'] - base_date).dt.days
        
        # RÂ²ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†é›¢ï¼ˆäºˆæ¸¬æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å¾Œã«é©ç”¨ï¼‰
        high_quality_mask = horizon_filtered_data['r_squared'] >= r2_threshold
        clustering_data = horizon_filtered_data[high_quality_mask].copy()
        low_quality_data = horizon_filtered_data[~high_quality_mask].copy()
        
        if len(clustering_data) < 5:
            st.warning(f"""
            **Insufficient Data for Clustering Analysis**
            
            - Current high-quality data: {len(clustering_data)} points (RÂ² â‰¥ {r2_threshold:.2f})
            - Required minimum: 5 points
            
            **ğŸ’¡ Solutions:**
            1. **Expand Analysis Period**: Select a longer time range (From/To dates)
            2. **Lower RÂ² Threshold**: Reduce Min RÂ² to {max(0.5, r2_threshold-0.1):.1f} or lower
            3. **Check Data Availability**: Ensure sufficient historical analysis data exists
            
            Current period: {from_date} to {to_date} ({len(valid_data)} total points)
            """)
            return
        
        # Step 2: é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã®ã¿ã§1æ¬¡å…ƒã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆäºˆæ¸¬ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥ã®ã¿ï¼‰
        from sklearn.cluster import DBSCAN
        clustering_input = clustering_data['crash_days'].values.reshape(-1, 1)
        clusterer = DBSCAN(eps=eps_days, min_samples=min_samples)
        clusters = clusterer.fit_predict(clustering_input)
        clustering_data['cluster'] = clusters
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµ±è¨ˆï¼ˆé«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        unique_clusters = [c for c in np.unique(clusters) if c != -1]
        n_clusters = len(unique_clusters)
        n_noise = np.sum(clusters == -1)
        
        # çµ±è¨ˆè¡¨ç¤ºï¼ˆData Quality Filter + Horizon Filterçµæœã‚’å«ã‚€ï¼‰
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("Raw Data", len(valid_data), help="All data points in selected period")
        with col2:
            removal_rate = (len(valid_data) - len(horizon_filtered_data)) / len(valid_data) * 100 if len(valid_data) > 0 else 0
            st.metric("Horizon Filtered", len(horizon_filtered_data), 
                     delta=f"-{removal_rate:.1f}%",
                     help=f"After excluding predictions within {min_horizon_days} days (Sornette filter)")
        with col3:
            st.metric("High Quality", len(clustering_data), 
                     delta=f"RÂ²â‰¥{r2_threshold:.2f}", 
                     help=f"Points with RÂ² â‰¥ {r2_threshold:.2f} (used for clustering)")
        with col4:
            st.metric("Clusters Found", n_clusters, help="Number of distinct clusters identified")
        with col5:
            st.metric("Isolated Points", n_noise, help="High-quality predictions that don't form clusters (insufficient density for min cluster size)")
        
        if n_clusters == 0:
            st.warning(f"""
            **No Clusters Found**
            
            All {len(clustering_data)} high-quality data points remain isolated (unclustered) with current parameters.
            
            **ğŸ’¡ Solutions:**
            1. **Increase Clustering Distance**: Try {eps_days + 10}-{eps_days + 30} days (currently {eps_days})
            2. **Decrease Min Cluster Size**: Try {max(2, min_samples - 2)}-{max(2, min_samples - 1)} (currently {min_samples})
            3. **Lower RÂ² Threshold**: Include more data by reducing to {max(0.5, r2_threshold - 0.1):.1f}
            
            **Current Settings**: Distance={eps_days}d, MinSize={min_samples}, RÂ²â‰¥{r2_threshold:.2f}
            """)
            return
        
        # Step 3: å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§RÂ²é‡ã¿ä»˜ãçµ±è¨ˆã‚µãƒãƒªãƒ¼ï¼ˆI054æ”¹å–„å®Ÿè£…ï¼‰
        cluster_predictions = {}
        
        for cluster_id in unique_clusters:
            cluster_subset = clustering_data[clustering_data['cluster'] == cluster_id]
            
            if len(cluster_subset) >= 1:  # å˜ä¸€ç‚¹ã§ã‚‚çµ±è¨ˆè¨ˆç®—å¯èƒ½
                # RÂ²é‡ã¿ä»˜ãã«ã‚ˆã‚‹é‡ã¿è¨ˆç®—
                r2_weights = cluster_subset['r_squared'].values
                # é‡ã¿ã‚’0.1-1.0ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼ˆä½RÂ²ã§ã‚‚æœ€å°é‡ã¿ã¯ä¿æŒï¼‰
                normalized_weights = 0.1 + 0.9 * (r2_weights - r2_weights.min()) / (r2_weights.max() - r2_weights.min() + 1e-10)
                
                # RÂ²é‡ã¿ä»˜ãå¹³å‡å€¤è¨ˆç®—ï¼ˆæ”¹å–„æ‰‹æ³•ï¼‰
                crash_days = cluster_subset['crash_days'].values
                weighted_mean = np.average(crash_days, weights=normalized_weights)
                
                # ã°ã‚‰ã¤ãæŒ‡æ¨™è¨ˆç®—
                weighted_std = np.sqrt(np.average((crash_days - weighted_mean)**2, weights=normalized_weights))
                simple_std = np.std(crash_days, ddof=1 if len(crash_days) > 1 else 0)
                
                # å››åˆ†ä½ç¯„å›²è¨ˆç®—
                q25 = np.percentile(crash_days, 25)
                q75 = np.percentile(crash_days, 75)
                iqr = q75 - q25
                
                # ä¿¡é ¼åº¦è©•ä¾¡ï¼ˆç°¡ç´ åŒ–ï¼šå¹³å‡RÂ²ã¨ãƒ‡ãƒ¼ã‚¿æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
                avg_r2 = r2_weights.mean()
                confidence = 'High' if avg_r2 > 0.7 and len(cluster_subset) >= 5 else \
                           'Medium' if avg_r2 > 0.5 and len(cluster_subset) >= 3 else 'Low'
                
                # å°†æ¥äºˆæ¸¬ï¼šé‡ã¿ä»˜ãå¹³å‡ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆãƒ—ãƒ­ãƒƒãƒˆã¨åŒã˜base_dateåŸºæº–ï¼‰
                future_crash_days = weighted_mean
                future_crash_date = base_date + timedelta(days=int(weighted_mean))
                
                cluster_predictions[cluster_id] = {
                    'weighted_mean': weighted_mean,
                    'weighted_std': weighted_std,
                    'simple_std': simple_std,
                    'q25': q25,
                    'q75': q75,
                    'iqr': iqr,
                    'size': len(cluster_subset),
                    'avg_r2': avg_r2,
                    'weight_range': f"{normalized_weights.min():.2f}-{normalized_weights.max():.2f}",
                    'future_crash_days': future_crash_days,
                    'future_crash_date': future_crash_date,
                    'confidence': confidence,
                    'mean_crash_date': cluster_subset['crash_date'].mean(),
                    'data_range': f"{crash_days.min():.0f}-{crash_days.max():.0f} days"
                }
        
        # å¯è¦–åŒ–: 2æ¬¡å…ƒæ•£å¸ƒå›³ + ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        fig = make_subplots(
            rows=1, cols=1,
            subplot_titles=("Crash Prediction Clustering",)
        )
        
        # ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªè‰²ï¼‰
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA726', '#AB47BC', '#66BB6A', '#EF5350', '#26C6DA']
        
        # ãƒ—ãƒ­ãƒƒãƒˆ1: é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        for i, cluster_id in enumerate(unique_clusters):
            cluster_subset = clustering_data[clustering_data['cluster'] == cluster_id]
            color = colors[i % len(colors)]
            
            # Centerç·šã®æƒ…å ±ã‚’å–å¾—ã—ã¦çµ±åˆã•ã‚ŒãŸåå‰ã‚’ä½œæˆ
            if cluster_id in cluster_predictions:
                pred = cluster_predictions[cluster_id]
                base_date = clustering_data['basis_date'].min()
                weighted_mean_date = base_date + timedelta(days=int(pred['weighted_mean']))
                
                # çµ±åˆã•ã‚ŒãŸè¡¨ç¤ºåï¼šæ—¥ä»˜ã€STDã€ã‚µã‚¤ã‚ºã‚’å…¨ã¦å«ã‚€
                cluster_name = f'C{cluster_id+1}: {weighted_mean_date.strftime("%Y-%m-%d")}, Â±{pred["weighted_std"]:.1f}d, n={len(cluster_subset)}'
            else:
                cluster_name = f'C{cluster_id+1}: n={len(cluster_subset)}'
            
            # æ•£å¸ƒå›³ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªå°ã•ã„ç‚¹ï¼‰ - çµ±åˆã•ã‚ŒãŸåå‰ã‚’ä½¿ç”¨
            fig.add_trace(go.Scatter(
                x=cluster_subset['basis_date'],
                y=cluster_subset['crash_date'],
                mode='markers',
                name=cluster_name,
                marker=dict(size=6, color=color, opacity=0.7),
                text=[f"LPPL RÂ²={row['r_squared']:.3f}<br>Weight={0.1 + 0.9 * (row['r_squared'] - cluster_subset['r_squared'].min()) / (cluster_subset['r_squared'].max() - cluster_subset['r_squared'].min() + 1e-10):.2f}"
                      for _, row in cluster_subset.iterrows()],
                hovertemplate='<b>Cluster %{fullData.name}</b><br>%{text}<br>Basis: %{x}<br>Predicted: %{y}<extra></extra>'
            ), row=1, col=1)
            
            # RÂ²é‡ã¿ä»˜ãå¹³å‡å€¤ã®æ°´å¹³ç·šï¼ˆLegendã¯éè¡¨ç¤ºï¼‰
            if cluster_id in cluster_predictions:
                pred = cluster_predictions[cluster_id]
                
                # Xè»¸ç¯„å›²ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã«æ‹¡å¼µï¼‰
                all_basis_min = clustering_data['basis_days'].min()
                all_basis_max = clustering_data['basis_days'].max()
                x_range = [base_date + timedelta(days=int(all_basis_min)), 
                          base_date + timedelta(days=int(all_basis_max))]
                
                # ä¸­å¿ƒç·šï¼ˆæ°´å¹³ãƒ»ç´°ã„ãƒ©ã‚¤ãƒ³ï¼‰ - Legendã¯éè¡¨ç¤º
                fig.add_trace(go.Scatter(
                    x=x_range,
                    y=[weighted_mean_date, weighted_mean_date],
                    mode='lines',
                    name='',  # åå‰ã‚’ç©ºã«
                    line=dict(color=color, width=1, dash='solid'),
                    showlegend=False,  # Legendè¡¨ç¤ºã‚’ç„¡åŠ¹åŒ–
                    hovertemplate=f'C{cluster_id+1}<br>RÂ²-Weighted Mean: {weighted_mean_date.strftime("%Y-%m-%d")}<br>Weighted STD: Â±{pred["weighted_std"]:.1f} days<extra></extra>'
                ), row=1, col=1)
        
        # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆï¼ˆé«˜å“è³ªãƒ‡ãƒ¼ã‚¿å†…ã®ãƒã‚¤ã‚ºï¼‰
        noise_data = clustering_data[clustering_data['cluster'] == -1]
        if len(noise_data) > 0:
            fig.add_trace(go.Scatter(
                x=noise_data['basis_date'],
                y=noise_data['crash_date'],
                mode='markers',
                name=f'Isolated (High Quality, n={len(noise_data)})',
                marker=dict(size=4, color='lightgray', symbol='x', opacity=0.5),
                hovertemplate='Isolated Prediction<br>LPPL RÂ²=%{customdata:.3f}<br>Basis: %{x}<br>Predicted: %{y}<extra></extra>',
                customdata=noise_data['r_squared']
            ), row=1, col=1)
            
        # ä½å“è³ªãƒ‡ãƒ¼ã‚¿ï¼ˆåˆ¥ã‚«ãƒ†ã‚´ãƒªã§è¡¨ç¤ºï¼‰
        if len(low_quality_data) > 0:
            fig.add_trace(go.Scatter(
                x=low_quality_data['basis_date'],
                y=low_quality_data['crash_date'],
                mode='markers',
                name=f'Low Quality (RÂ²<{r2_threshold:.2f}, n={len(low_quality_data)})',
                marker=dict(size=4, color='red', symbol='triangle-up', opacity=0.3),
                hovertemplate='Low Quality Data<br>LPPL RÂ²=%{customdata:.3f}<br>Basis: %{x}<br>Predicted: %{y}<extra></extra>',
                customdata=low_quality_data['r_squared']
            ), row=1, col=1)
            
        # åŸºæº–æ—¥=ã‚¯ãƒ©ãƒƒã‚·ãƒ¥æ—¥ã®ç›´ç·šï¼ˆy=xç·šï¼‰ - Centerç·šã¨åŒã˜Xè»¸ç¯„å›²ã‚’ä½¿ç”¨
        if len(clustering_data) > 0:
            # Centerç·šã¨åŒã˜Xè»¸ç¯„å›²ã‚’è¨ˆç®—
            all_basis_min = clustering_data['basis_days'].min()
            all_basis_max = clustering_data['basis_days'].max()
            x_range = [base_date + timedelta(days=int(all_basis_min)), 
                      base_date + timedelta(days=int(all_basis_max))]
            
            fig.add_trace(go.Scatter(
                x=x_range,
                y=x_range,
                mode='lines',
                name='Reference: Fitting Date = Crash Date',
                line=dict(color='lightblue', width=2, dash='dot'),
                hovertemplate='Reference Line<br>Fitting Date = Predicted Crash Date<extra></extra>',
                showlegend=True
            ), row=1, col=1)
        
        # ç¾åœ¨æ™‚åˆ»ãƒ©ã‚¤ãƒ³ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼ã®å•é¡Œã®ãŸã‚ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
        # today = datetime.datetime.now()
        # fig.add_hline(y=today, line_dash="dot", line_color="gray", 
        #              annotation_text="Today", row=1, col=1)
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
        fig.update_xaxes(title_text="Fitting Basis Date", row=1, col=1)
        fig.update_yaxes(title_text="Predicted Crash Date", row=1, col=1)
        
        fig.update_layout(
            height=600,  # æ£’ã‚°ãƒ©ãƒ•å‰Šé™¤ã«ã‚ˆã‚Šé«˜ã•ã‚’æ¸›å°‘
            showlegend=True,
            hovermode='closest',
            title_text=f"{symbol} - Crash Prediction Clustering Analysis"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
        if cluster_predictions:
            st.subheader("ğŸ“Š Cluster Analysis Results")
            
            # ä»Šæ—¥ã‹ã‚‰ã®æ—¥æ•°ã‚’è¨ˆç®—
            today = datetime.now().date()
            
            cluster_df = pd.DataFrame([
                {
                    'Cluster': f"C{cid+1}",
                    'Weight Mean Date': pred['future_crash_date'].strftime('%Y-%m-%d'),
                    'Days to Crash': f"{pred['future_crash_days']:.0f}",
                    'Days from Today': f"{(pred['future_crash_date'].date() - today).days}",
                    'Weighted STD': f"{pred['weighted_std']:.1f}",
                    'STD': f"{pred['simple_std']:.1f}",
                    'Size': pred['size'],
                    'Avg RÂ²': f"{pred['avg_r2']:.3f}",
                    'Confidence': pred['confidence']
                }
                for cid, pred in sorted(cluster_predictions.items())  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IDé †ã«ã‚½ãƒ¼ãƒˆ
            ])
            
            # DataFrameã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼IDé †ã§æ—¢ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼ˆä¸Šè¨˜ã®sorted()ã«ã‚ˆã‚Šï¼‰
            
            st.dataframe(cluster_df, use_container_width=True)
            
            # å˜ä½æƒ…å ±ã‚’åˆ¥è¡Œã§è¡¨ç¤º
            st.caption("**Units**: Days to Crash (days from analysis basis date) | Days from Today (days from current date) | Weighted STD & STD (Â±days) | Size (number of predictions)")
            st.caption("*Center predictions are calculated using RÂ²-weighted averaging, giving higher influence to predictions with better model fit*")
            
            # æœ€ã‚‚ä¿¡é ¼æ€§ã®é«˜ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ï¼ˆå¹³å‡RÂ²Ã—ã‚µã‚¤ã‚ºã§åˆ¤å®šï¼‰
            best_cluster = max(cluster_predictions.items(), 
                             key=lambda x: (x[1]['avg_r2'] * x[1]['size']))
            best_id, best_pred = best_cluster
            
            st.success(f"""
            **ğŸ¯ Most Reliable RÂ²-Weighted Prediction:**
            - **C{best_id+1}** with {best_pred['size']} data points
            - **Average RÂ²**: {best_pred['avg_r2']:.3f} (weight range: {best_pred['weight_range']})
            - **Predicted crash**: {best_pred['future_crash_date'].strftime('%Y-%m-%d')}
            - **Confidence**: {best_pred['confidence']}
            - **Days to predicted crash**: {best_pred['future_crash_days']:.0f}
            - **Data range**: {best_pred['data_range']}
            """)
    
    def render_parameters_tab(self, symbol: str, analysis_data: pd.DataFrame):
        """Tab 4: Parameters Only"""
        
        st.header(f"ğŸ“‹ {symbol} - LPPL Parameters")
        
        if analysis_data.empty:
            st.warning("No analysis data available for this symbol")
            return
        
        # Analysis Data Period functionality
        st.subheader("ğŸ“… Analysis Data Period")
        st.caption("è§£æãƒ‡ãƒ¼ã‚¿ã®å¯¾è±¡ã¨ã™ã‚‹æœŸé–“")
        
        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ã®è¨ˆç®—ï¼ˆFrom/Toã®ä¸‹ã«è¡¨ç¤ºç”¨ï¼‰
        original_data = self.get_symbol_analysis_data(symbol, limit=1000)  # ãƒ•ã‚£ãƒ«ã‚¿å‰ã®å…¨ãƒ‡ãƒ¼ã‚¿
        if not original_data.empty:
            original_data['analysis_basis_date'] = pd.to_datetime(original_data['analysis_basis_date'])
            full_min_date = original_data['analysis_basis_date'].min()
            full_max_date = original_data['analysis_basis_date'].max()
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            if 'parameters_from_date' not in st.session_state:
                st.session_state.parameters_from_date = analysis_data['analysis_basis_date'].min().date()
            from_date = st.date_input("From", st.session_state.parameters_from_date, key='parameters_from_date_input')
            st.session_state.parameters_from_date = from_date
            # Oldest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Oldest Analysis: {full_min_date.strftime('%Y-%m-%d')}")
            
        with col2:
            if 'parameters_to_date' not in st.session_state:
                st.session_state.parameters_to_date = analysis_data['analysis_basis_date'].max().date()
            to_date = st.date_input("To", st.session_state.parameters_to_date, key='parameters_to_date_input')
            st.session_state.parameters_to_date = to_date
            # Latest Analysisæƒ…å ±ã‚’ç›´ä¸‹ã«è¡¨ç¤º
            if not original_data.empty:
                st.caption(f"ğŸ“ Latest Analysis: {full_max_date.strftime('%Y-%m-%d')}")
        
        # ğŸ“Š é¸æŠæœŸé–“ã®è¦–è¦šè¡¨ç¤ºï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã¿ãƒ»ã‚ˆã‚Šå¹…åºƒãï¼‰
        if not original_data.empty:
            selected_min = pd.to_datetime(from_date)
            selected_max = pd.to_datetime(to_date)
            
            # æœŸé–“ã®å‰²åˆè¨ˆç®—
            total_days = (full_max_date - full_min_date).days
            selected_duration = (selected_max - selected_min).days
            
            # é¸æŠæœŸé–“ã®é–‹å§‹ä½ç½®ã¨é•·ã•ã‚’è¨ˆç®—
            start_offset = (selected_min - full_min_date).days if total_days > 0 else 0
            selected_ratio = selected_duration / total_days if total_days > 0 else 1.0
        
        st.markdown("---")
        
        # Period ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        analysis_data['analysis_basis_date'] = pd.to_datetime(analysis_data['analysis_basis_date'])
        from_datetime = pd.to_datetime(from_date)
        to_datetime = pd.to_datetime(to_date)
        
        date_mask = (analysis_data['analysis_basis_date'] >= from_datetime) & (analysis_data['analysis_basis_date'] <= to_datetime)
        analysis_data = analysis_data[date_mask].copy()
        
        if len(analysis_data) == 0:
            st.warning(f"No data available for selected period: {from_date} to {to_date}")
            return
        
        # Prepare detailed parameter table
        display_df = analysis_data.copy()
        
        # Add formatted predicted crash dateï¼ˆå®‰å…¨ãªæ—¥ä»˜å¤‰æ›ï¼‰
        if 'predicted_crash_date' in display_df.columns:
            def safe_format_date(x):
                if pd.isna(x) or x is None:
                    return 'N/A'
                try:
                    # æ–‡å­—åˆ—ã®å ´åˆã¯Timestampã«å¤‰æ›
                    if isinstance(x, str):
                        x = pd.to_datetime(x)
                    # strftimeãƒ¡ã‚½ãƒƒãƒ‰ãŒä½¿ãˆã‚‹å½¢å¼ã«å¤‰æ›
                    if hasattr(x, 'strftime'):
                        return x.strftime('%Y-%m-%d %H:%M')
                    else:
                        # ãã‚Œã§ã‚‚ä½¿ãˆãªã„å ´åˆã¯æ–‡å­—åˆ—ã§è¿”ã™
                        return str(x)
                except (ValueError, TypeError, AttributeError):
                    return str(x) if x is not None else 'N/A'
            
            display_df['predicted_crash_date_formatted'] = display_df['predicted_crash_date'].apply(safe_format_date)
        else:
            display_df['predicted_crash_date_formatted'] = 'N/A'
        
        # Add fitting basis date (ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥) - most important dateï¼ˆå®‰å…¨ãªå¤‰æ›ï¼‰
        def safe_format_date_simple(x):
            if pd.isna(x) or x is None:
                return 'N/A'
            try:
                if isinstance(x, str):
                    x = pd.to_datetime(x)
                if hasattr(x, 'strftime'):
                    return x.strftime('%Y-%m-%d')
                else:
                    return str(x)
            except (ValueError, TypeError, AttributeError):
                return str(x) if x is not None else 'N/A'
        
        if 'analysis_basis_date' in display_df.columns:
            display_df['fitting_basis_date_formatted'] = display_df['analysis_basis_date'].apply(safe_format_date_simple)
        elif 'data_period_end' in display_df.columns:
            # Fallback to data period end
            display_df['fitting_basis_date_formatted'] = display_df['data_period_end'].apply(safe_format_date_simple)
        else:
            display_df['fitting_basis_date_formatted'] = 'N/A'
        
        # Calculate data period (number of days used for fitting)
        data_period_days = []
        for _, row in display_df.iterrows():
            try:
                # Priority 1: Use window_days if available
                if 'window_days' in display_df.columns and pd.notna(row.get('window_days')):
                    days = int(row['window_days'])
                    data_period_days.append(f"{days} days")
                # Priority 2: Calculate from start and end dates
                elif ('data_period_start' in display_df.columns and 'data_period_end' in display_df.columns and
                      pd.notna(row.get('data_period_start')) and pd.notna(row.get('data_period_end'))):
                    start_dt = pd.to_datetime(row['data_period_start'])
                    end_dt = pd.to_datetime(row['data_period_end'])
                    days = (end_dt - start_dt).days + 1  # +1 to include both start and end
                    data_period_days.append(f"{days} days")
                # Priority 3: Use data_points as fallback
                elif 'data_points' in display_df.columns and pd.notna(row.get('data_points')):
                    points = int(row['data_points'])
                    data_period_days.append(f"{points} days")
                else:
                    data_period_days.append('N/A')
                    
            except Exception:
                data_period_days.append('N/A')
        
        display_df['data_period_days'] = data_period_days
        
        # Define column priority for display (left to right)
        priority_columns = [
            'predicted_crash_date_formatted',  # Most important
            'fitting_basis_date_formatted',    # ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ (replaces analysis_date)
            'data_period_days',                # Number of days (replaces data_period)
            'r_squared',
            'quality',                         # Quality and Confidence together
            'confidence',
            'tc',
            'beta', 'omega', 'phi',
            'A', 'B', 'C',
            'rmse'
        ]
        
        # Select existing columns in priority order
        existing_columns = [col for col in priority_columns if col in display_df.columns]
        
        final_df = display_df[existing_columns].copy()
        
        # Sort by fitting basis date (most recent first - analysis_basis_date)
        sort_col = None
        if 'analysis_basis_date' in analysis_data.columns:
            sort_col = 'analysis_basis_date'
        elif 'data_period_end' in analysis_data.columns:
            sort_col = 'data_period_end'
        elif 'predicted_crash_date' in analysis_data.columns:
            sort_col = 'predicted_crash_date'
        
        if sort_col:
            final_df = final_df.loc[analysis_data.sort_values(sort_col, na_position='last', ascending=False).index]
        
        # Format numeric columns
        numeric_columns = ['r_squared', 'confidence', 'tc', 'beta', 'omega', 'phi', 'A', 'B', 'C', 'rmse']
        for col in numeric_columns:
            if col in final_df.columns:
                final_df[col] = final_df[col].apply(
                    lambda x: f"{x:.4f}" if pd.notna(x) else 'N/A'
                )
        
        # Display the table
        st.subheader("ğŸ“Š Analysis Results (sorted by fitting basis date)")
        
        st.dataframe(
            final_df,
            use_container_width=True,
            height=400,
            column_config={
                "predicted_crash_date_formatted": st.column_config.TextColumn(
                    "ğŸ¯ Predicted Crash Date",
                    help="Predicted crash date/time converted from tc ratio",
                    width="large"
                ),
                "fitting_basis_date_formatted": st.column_config.TextColumn(
                    "ğŸ“… Fitting Basis Date",
                    help="ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°åŸºæº–æ—¥ - Final day of fitting period (most important for sorting)",
                    width="medium"
                ),
                "data_period_days": st.column_config.TextColumn(
                    "ğŸ“Š Data Period",
                    help="Number of days used for fitting analysis",
                    width="small"
                ),
                "r_squared": st.column_config.TextColumn(
                    "ğŸ“Š RÂ² Score",
                    help="Model fit quality (higher = better, 0-1 scale)"
                ),
                "quality": st.column_config.TextColumn(
                    "ğŸ¯ Quality",
                    help="Analysis quality assessment (high_quality/acceptable/poor)"
                ),
                "confidence": st.column_config.TextColumn(
                    "âœ… Confidence",
                    help="Prediction confidence level (0-1 scale, similar to Quality)"
                ),
                "tc": st.column_config.TextColumn(
                    "ğŸ”¢ tc Ratio",
                    help="Critical time ratio used for crash prediction"
                )
            }
        )
        
        # Explanatory text for Quality and Confidence metrics
        st.subheader("ğŸ“– Metric Definitions")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **ğŸ¯ Quality Assessment:**
            - `high_quality`: RÂ² > 0.85, stable parameters
            - `acceptable`: RÂ² > 0.60, reasonable fit  
            - `poor`: RÂ² < 0.60, unstable fitting
            
            Quality is determined by statistical criteria including RÂ² score, parameter stability, and fitting convergence.
            """)
        
        with col2:
            st.markdown("""
            **âœ… Confidence Level:**
            - Range: 0.0 - 1.0 (higher = more confident)
            - Based on: fitting quality, parameter consistency
            - Similar to Quality but expressed as continuous value
            
            Confidence represents the overall reliability of the LPPL model prediction for this analysis.
            """)
        
        # Summary statistics
        st.subheader("ğŸ“ˆ Summary Statistics")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            highest_r2 = analysis_data['r_squared'].max()
            st.metric("Highest RÂ²", f"{highest_r2:.3f}" if pd.notna(highest_r2) else "N/A")
        
        with col2:
            avg_r2 = analysis_data['r_squared'].mean()
            st.metric("Average RÂ²", f"{avg_r2:.3f}" if pd.notna(avg_r2) else "N/A")
        
        with col3:
            high_quality_count = len(analysis_data[analysis_data['quality'] == 'high_quality'])
            st.metric("High Quality", f"{high_quality_count}/{len(analysis_data)}")
        
        with col4:
            valid_predictions = len(analysis_data.dropna(subset=['predicted_crash_date']))
            st.metric("Valid Predictions", f"{valid_predictions}/{len(analysis_data)}")
        
        with col5:
            if 'predicted_crash_date' in analysis_data.columns:
                # å®‰å…¨ãªæ—¥ä»˜æ¯”è¼ƒï¼ˆæ–‡å­—åˆ—â†’Timestampå¤‰æ›ï¼‰
                try:
                    now = datetime.now()
                    # æ–‡å­—åˆ—æ—¥ä»˜ã‚’Timestampã«å¤‰æ›ã—ã¦ã‹ã‚‰æ¯”è¼ƒ
                    crash_dates_converted = pd.to_datetime(analysis_data['predicted_crash_date'], errors='coerce')
                    future_predictions = len(crash_dates_converted[crash_dates_converted > now])
                    st.metric("Future Predictions", future_predictions)
                except Exception as e:
                    st.metric("Future Predictions", "Error")
                    st.caption(f"âš ï¸ Date comparison error: {str(e)}")
            else:
                st.metric("Future Predictions", "N/A")
        
        # Download functionality
        csv = final_df.to_csv(index=False)
        st.download_button(
            label="ğŸ“¥ Download Parameter Data",
            data=csv,
            file_name=f"{symbol}_parameters_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    
    def render_references_tab(self, symbol: str, analysis_data: pd.DataFrame):
        """Tab 5: References and Benchmarks"""
        
        st.header("ğŸ“š References & Benchmarks")
        
        # Reference information from Sornette paper reproduction
        st.subheader("ğŸ“š Reference: Sornette Paper Reproduction")
        
        # Multiple historical crash references
        crash_tabs = st.tabs([
            "ğŸ¯ 1987 Black Monday", 
            "ğŸ’» 2000 Dot-com Bubble",
            "ğŸ“Š General Benchmarks"
        ])
        
        with crash_tabs[0]:
            st.markdown("""
            **1987 Black Monday LPPL Analysis** (100/100 score reproduction):
            
            ğŸ“Š **Validated Parameters**:
            - **Paper Reproduction Score**: 100/100 âœ…
            - **RÂ² Range**: Typically 0.85-0.95 for high-quality fits
            - **Î² (Beta) Parameter**: ~0.33 (critical exponent from theory)
            - **Ï‰ (Omega) Parameter**: ~6-8 (log-periodic oscillation frequency)
            - **Data Period**: 706 days pre-crash analysis
            - **Total Return**: +65.2% (bubble formation criteria met)
            - **Peak Return**: +85.1% (accelerating growth confirmed)
            - **Crash Magnitude**: -28.2% (major crash threshold exceeded)
            """)
            
        with crash_tabs[1]:
            st.markdown("""
            **2000 Dot-com Bubble LPPL Analysis** (Qualitative validation):
            
            ğŸ“Š **Reference Parameters**:
            - **Bubble Formation**: +417% total return (2000 peak)
            - **Î² (Beta) Parameter**: 0.1-0.5 range (theory-consistent)
            - **Ï‰ (Omega) Parameter**: 4-12 range (log-periodic patterns)
            - **RÂ² Performance**: 0.6-0.9 depending on fitting window
            - **Data Period**: Multi-year bubble formation analysis
            - **Crash Magnitude**: -78% from peak (major crash confirmed)
            
            ğŸ“‹ **Key Insights**:
            - Longer bubble formation periods show different parameter ranges
            - Technology sector bubbles exhibit distinct Ï‰ patterns
            - Higher volatility affects RÂ² consistency
            """)
            
        with crash_tabs[2]:
            st.markdown("""
            **General LPPL Parameter Benchmarks**:
            
            ğŸ“– **Interpretation Guidelines**:
            - **RÂ² > 0.8**: Excellent fit quality (paper-level accuracy)
            - **RÂ² 0.6-0.8**: Good fit quality (acceptable for analysis)
            - **RÂ² < 0.6**: Lower confidence (use with caution)
            - **Î² â‰ˆ 0.33**: Theoretical expectation from critical phenomena
            - **Î² = 0.1-0.5**: Acceptable range for most market conditions
            - **Ï‰ = 6-8**: Optimal log-periodic frequency (Black Monday)
            - **Ï‰ = 4-12**: Extended acceptable range for various bubbles
            
            ğŸ”¬ **Scientific Validation Standards**:
            - Our implementation achieves **identical results** to published papers
            - All parameters fall within theoretical bounds from literature
            - Multiple crash validations confirm prediction accuracy
            
            ğŸ“‹ **Quality Classification**:
            - **High Quality**: RÂ² > 0.8, Î² = 0.2-0.5, Ï‰ = 4-12
            - **Research Grade**: Matches or exceeds paper reproduction metrics
            - **Trading Grade**: High quality + recent data validation
            """)
    
    def run(self):
        """Main dashboard execution"""
        
        st.title("ğŸ“Š LPPL Market Analysis Dashboard")
        st.markdown("*Symbol-based analysis with trading position prioritization*")
        
        # Render new sidebar (v2) - Symbolé¸æŠã®ã¿
        selected_symbol = self.render_sidebar_v2()
        
        if selected_symbol is None:
            # åˆæœŸç”»é¢ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰ã‚’è¡¨ç¤º
            st.info("""
            ### ğŸ“‹ Getting Started
            
            Please use the sidebar to:
            1. **ğŸ›ï¸ Symbol Filters**: Set filters to find symbols of interest
            2. **ğŸ“ˆ Select Symbol**: Choose a symbol and click "Select Symbol"
            
            After selecting a symbol, each tab will have its own Display Period settings.
            """)
            return
        
        # ğŸ†• é¸æŠéŠ˜æŸ„ã®å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆå„ã‚¿ãƒ–ã§å€‹åˆ¥ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
        with st.spinner(f"Loading all analysis data for {selected_symbol}..."):
            # å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¯å„ã‚¿ãƒ–ã§å®Ÿæ–½ï¼‰
            analysis_data = self.get_symbol_analysis_data(selected_symbol, limit=None, period_selection=None)
        
        if analysis_data.empty:
            st.warning(f"No analysis data found for {selected_symbol}")
            return
        
        # ğŸ¯ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº† - æ–°ã‚·ã‚¹ãƒ†ãƒ ã§å…¨ã¦å‡¦ç†æ¸ˆã¿
        
        # Main content tabs - æ–°ã—ã„ã‚¿ãƒ–é †åºï¼ˆ2025-08-13ï¼‰
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š Crash Prediction Data",     # 1. ãƒ‡ãƒ¼ã‚¿ç¢ºèªãƒ»å¯è¦–åŒ–
            "ğŸ¯ Prediction Clustering",     # 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ  
            "ğŸ“ˆ LPPL Fitting Plot",         # 3. ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœ
            "ğŸ“‹ Parameters",                # 4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°
            "ğŸ“š References"                 # 5. å‚ç…§æƒ…å ±
        ])
        
        with tab1:
            self.render_prediction_data_tab(selected_symbol, analysis_data)
        
        with tab2:
            self.render_crash_clustering_tab(selected_symbol, analysis_data)
        
        with tab3:
            self.render_price_predictions_tab(selected_symbol, analysis_data)
        
        with tab4:
            self.render_parameters_tab(selected_symbol, analysis_data)
        
        with tab5:
            self.render_references_tab(selected_symbol, analysis_data)

def main():
    """Main execution function"""
    app = SymbolAnalysisDashboard()
    app.run()

if __name__ == "__main__":
    main()