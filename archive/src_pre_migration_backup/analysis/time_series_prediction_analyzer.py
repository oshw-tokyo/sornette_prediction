#!/usr/bin/env python3
"""
æ™‚ç³»åˆ—äºˆæ¸¬ä¸€è²«æ€§åˆ†æã‚·ã‚¹ãƒ†ãƒ 

å„éŠ˜æŸ„ã”ã¨ã«ã€æ§˜ã€…ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å®Ÿæ–½ã—ãŸã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã®äºˆæ¸¬ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã€
ãã‚Œã‚‰ã®äºˆæ¸¬ãŒãŠã‚ˆãç‰¹å®šã®æ™‚ç‚¹ã‚’æŒ‡ã—ç¶šã‘ã¦ã„ã‚‹ã®ã‹ã©ã†ã‹ã‚’åˆ†æã™ã‚‹
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class PredictionPoint:
    """å˜ä¸€ã®äºˆæ¸¬ãƒã‚¤ãƒ³ãƒˆ"""
    analysis_date: datetime
    predicted_crash_date: datetime
    tc: float
    beta: float
    omega: float
    r_squared: float
    rmse: float
    confidence: float
    window_days: int
    quality_assessment: Optional[Any] = None
    
    @property
    def days_to_prediction(self) -> int:
        """åˆ†ææ—¥ã‹ã‚‰äºˆæ¸¬æ—¥ã¾ã§ã®æ—¥æ•°"""
        return (self.predicted_crash_date - self.analysis_date).days

@dataclass 
class ConsistencyMetrics:
    """äºˆæ¸¬ä¸€è²«æ€§æŒ‡æ¨™"""
    symbol: str
    total_predictions: int
    usable_predictions: int
    prediction_std_days: float  # äºˆæ¸¬æ—¥ã®æ¨™æº–åå·®ï¼ˆæ—¥æ•°ï¼‰
    tc_std: float  # tcå€¤ã®æ¨™æº–åå·®
    confidence_mean: float  # å¹³å‡ä¿¡é ¼åº¦
    convergence_date: Optional[datetime]  # åæŸäºˆæ¸¬æ—¥
    convergence_confidence: float  # åæŸä¿¡é ¼åº¦
    outlier_count: int  # å¤–ã‚Œå€¤æ•°
    consistency_score: float  # 0-1ã®ä¸€è²«æ€§ã‚¹ã‚³ã‚¢

class TimeSeriesPredictionAnalyzer:
    """éŠ˜æŸ„åˆ¥æ™‚ç³»åˆ—äºˆæ¸¬åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.predictions = {}  # symbol -> List[PredictionPoint]
        self.consistency_metrics = {}  # symbol -> ConsistencyMetrics
        
        # åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.analysis_windows = [30, 60, 90, 180, 365, 730]  # åˆ†ææœŸé–“ï¼ˆæ—¥ï¼‰
        self.outlier_threshold = 2.0  # å¤–ã‚Œå€¤åˆ¤å®šã®æ¨™æº–åå·®å€æ•°
        
    def add_prediction(self, symbol: str, prediction: PredictionPoint):
        """äºˆæ¸¬ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ """
        if symbol not in self.predictions:
            self.predictions[symbol] = []
        self.predictions[symbol].append(prediction)
        
    def analyze_symbol_consistency(self, symbol: str, 
                                 data_client,
                                 analysis_start: datetime,
                                 analysis_end: datetime,
                                 analysis_interval_days: int = 7) -> ConsistencyMetrics:
        """
        ç‰¹å®šéŠ˜æŸ„ã®äºˆæ¸¬ä¸€è²«æ€§ã‚’åˆ†æ
        
        Args:
            symbol: åˆ†æå¯¾è±¡éŠ˜æŸ„
            data_client: ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆFREDç­‰ï¼‰
            analysis_start: åˆ†æé–‹å§‹æ—¥
            analysis_end: åˆ†æçµ‚äº†æ—¥  
            analysis_interval_days: åˆ†æå®Ÿè¡Œé–“éš”ï¼ˆæ—¥ï¼‰
            
        Returns:
            ConsistencyMetrics: ä¸€è²«æ€§æŒ‡æ¨™
        """
        from ..fitting.multi_criteria_selection import MultiCriteriaSelector
        
        print(f"ğŸ“Š {symbol} æ™‚ç³»åˆ—äºˆæ¸¬ä¸€è²«æ€§åˆ†æé–‹å§‹")
        print(f"   æœŸé–“: {analysis_start.date()} - {analysis_end.date()}")
        
        selector = MultiCriteriaSelector()
        predictions = []
        
        # åˆ†ææ—¥ç¨‹ã‚’ç”Ÿæˆ
        current_date = analysis_start
        analysis_dates = []
        
        while current_date <= analysis_end:
            analysis_dates.append(current_date)
            current_date += timedelta(days=analysis_interval_days)
            
        print(f"   åˆ†æå›æ•°: {len(analysis_dates)}å›")
        
        # å„åˆ†ææ—¥ã§è¤‡æ•°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®äºˆæ¸¬ã‚’å®Ÿè¡Œ
        for i, analysis_date in enumerate(analysis_dates):
            print(f"   é€²æ—: {i+1}/{len(analysis_dates)} ({analysis_date.date()})")
            
            for window_days in self.analysis_windows:
                try:
                    # ãƒ‡ãƒ¼ã‚¿æœŸé–“è¨ˆç®—
                    data_end = analysis_date
                    data_start = data_end - timedelta(days=window_days)
                    
                    # ãƒ‡ãƒ¼ã‚¿å–å¾—
                    data = self._get_market_data(
                        data_client, symbol, 
                        data_start.strftime('%Y-%m-%d'),
                        data_end.strftime('%Y-%m-%d')
                    )
                    
                    if data is None or len(data) < 50:
                        continue
                        
                    # LPPL ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å®Ÿè¡Œ
                    result = selector.perform_comprehensive_fitting(data)
                    best_result = result.get_selected_result()
                    
                    if best_result and best_result.quality_assessment:
                        qa = best_result.quality_assessment
                        
                        # äºˆæ¸¬æ—¥è¨ˆç®—ï¼ˆtcã‹ã‚‰å®Ÿéš›ã®æ—¥ä»˜ã«å¤‰æ›ï¼‰
                        predicted_crash_date = self._tc_to_date(
                            best_result.tc, analysis_date, window_days
                        )
                        
                        # äºˆæ¸¬ãƒã‚¤ãƒ³ãƒˆä½œæˆ
                        prediction = PredictionPoint(
                            analysis_date=analysis_date,
                            predicted_crash_date=predicted_crash_date,
                            tc=best_result.tc,
                            beta=best_result.beta,
                            omega=best_result.omega,
                            r_squared=best_result.r_squared,
                            rmse=best_result.rmse,
                            confidence=qa.confidence,
                            window_days=window_days,
                            quality_assessment=qa
                        )
                        
                        predictions.append(prediction)
                        
                except Exception as e:
                    print(f"      âš ï¸ {analysis_date.date()} (window={window_days}) ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
        
        # äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.predictions[symbol] = predictions
        
        # ä¸€è²«æ€§æŒ‡æ¨™è¨ˆç®—
        metrics = self._calculate_consistency_metrics(symbol, predictions)
        self.consistency_metrics[symbol] = metrics
        
        print(f"âœ… {symbol} åˆ†æå®Œäº†: {len(predictions)}å€‹ã®äºˆæ¸¬")
        
        return metrics
    
    def _get_market_data(self, client, symbol: str, start_date: str, end_date: str):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ã®çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        try:
            # çµ±åˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å ´åˆ
            if hasattr(client, 'get_data_with_fallback'):
                data, source = client.get_data_with_fallback(symbol, start_date, end_date)
                if data is not None:
                    print(f"      ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ ({source}): {len(data)}æ—¥åˆ†")
                return data
            
            # å¾“æ¥ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆFREDç­‰ï¼‰
            elif hasattr(client, 'get_series_data'):
                # FREDç”¨ã®éŠ˜æŸ„ãƒãƒƒãƒ”ãƒ³ã‚°
                fred_symbols = {
                    'NASDAQ': 'NASDAQCOM',
                    'SP500': 'SP500', 
                    'DJIA': 'DJIA',
                    'VIX': 'VIXCLS'
                }
                mapped_symbol = fred_symbols.get(symbol, symbol)
                return client.get_series_data(mapped_symbol, start_date, end_date)
            else:
                # ãã®ä»–ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                return client.get_data(symbol, start_date, end_date)
                
        except Exception as e:
            print(f"      ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ ({symbol}): {str(e)}")
            return None
    
    def _tc_to_date(self, tc: float, analysis_date: datetime, window_days: int) -> datetime:
        """tcå€¤ã‚’å®Ÿéš›ã®äºˆæ¸¬æ—¥ã«å¤‰æ›"""
        # tcã¯æ­£è¦åŒ–æ™‚é–“ã§ã®è‡¨ç•Œç‚¹
        # tc > 1.0 ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿æœŸé–“çµ‚äº†å¾Œã®æ—¥æ•°ã‚’è¨ˆç®—
        if tc > 1.0:
            days_beyond = (tc - 1.0) * window_days
            return analysis_date + timedelta(days=days_beyond)
        else:
            # tc <= 1.0 ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿æœŸé–“å†…
            days_from_start = tc * window_days
            return analysis_date - timedelta(days=window_days) + timedelta(days=days_from_start)
    
    def _calculate_consistency_metrics(self, symbol: str, 
                                     predictions: List[PredictionPoint]) -> ConsistencyMetrics:
        """ä¸€è²«æ€§æŒ‡æ¨™ã®è¨ˆç®—"""
        
        if not predictions:
            return ConsistencyMetrics(
                symbol=symbol, total_predictions=0, usable_predictions=0,
                prediction_std_days=float('inf'), tc_std=float('inf'),
                confidence_mean=0.0, convergence_date=None,
                convergence_confidence=0.0, outlier_count=0,
                consistency_score=0.0
            )
        
        # ä½¿ç”¨å¯èƒ½ãªäºˆæ¸¬ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        usable_predictions = [
            p for p in predictions 
            if p.quality_assessment and p.quality_assessment.is_usable
        ]
        
        if len(usable_predictions) < 2:
            return ConsistencyMetrics(
                symbol=symbol, total_predictions=len(predictions),
                usable_predictions=len(usable_predictions),
                prediction_std_days=float('inf'), tc_std=float('inf'),
                confidence_mean=np.mean([p.confidence for p in predictions]),
                convergence_date=None, convergence_confidence=0.0,
                outlier_count=0, consistency_score=0.0
            )
        
        # äºˆæ¸¬æ—¥ã®ä¸€è²«æ€§è¨ˆç®—
        predicted_dates = [p.predicted_crash_date for p in usable_predictions]
        date_timestamps = [d.timestamp() for d in predicted_dates]
        
        prediction_std_seconds = np.std(date_timestamps)
        prediction_std_days = prediction_std_seconds / (24 * 3600)
        
        # tcå€¤ã®ä¸€è²«æ€§
        tc_values = [p.tc for p in usable_predictions]
        tc_std = np.std(tc_values)
        
        # ä¿¡é ¼åº¦å¹³å‡
        confidence_mean = np.mean([p.confidence for p in usable_predictions])
        
        # å¤–ã‚Œå€¤æ¤œå‡º
        outliers = self._detect_outliers(usable_predictions)
        outlier_count = len(outliers)
        
        # åæŸäºˆæ¸¬æ—¥è¨ˆç®—ï¼ˆå¤–ã‚Œå€¤é™¤å¤–å¾Œã®ä¸­å¤®å€¤ï¼‰
        filtered_predictions = [p for p in usable_predictions if p not in outliers]
        if filtered_predictions:
            filtered_dates = [p.predicted_crash_date for p in filtered_predictions]
            convergence_timestamp = np.median([d.timestamp() for d in filtered_dates])
            convergence_date = datetime.fromtimestamp(convergence_timestamp)
            convergence_confidence = np.mean([p.confidence for p in filtered_predictions])
        else:
            convergence_date = None
            convergence_confidence = 0.0
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-1ã€1ãŒæœ€é«˜ï¼‰
        consistency_score = self._calculate_consistency_score(
            prediction_std_days, tc_std, confidence_mean, outlier_count, len(usable_predictions)
        )
        
        return ConsistencyMetrics(
            symbol=symbol,
            total_predictions=len(predictions),
            usable_predictions=len(usable_predictions),
            prediction_std_days=prediction_std_days,
            tc_std=tc_std,
            confidence_mean=confidence_mean,
            convergence_date=convergence_date,
            convergence_confidence=convergence_confidence,
            outlier_count=outlier_count,
            consistency_score=consistency_score
        )
    
    def _detect_outliers(self, predictions: List[PredictionPoint]) -> List[PredictionPoint]:
        """å¤–ã‚Œå€¤æ¤œå‡ºï¼ˆäºˆæ¸¬æ—¥ãƒ™ãƒ¼ã‚¹ï¼‰"""
        if len(predictions) < 3:
            return []
            
        dates = [p.predicted_crash_date.timestamp() for p in predictions]
        mean_date = np.mean(dates)
        std_date = np.std(dates)
        
        outliers = []
        for p in predictions:
            z_score = abs(p.predicted_crash_date.timestamp() - mean_date) / std_date
            if z_score > self.outlier_threshold:
                outliers.append(p)
                
        return outliers
    
    def _calculate_consistency_score(self, pred_std_days: float, tc_std: float,
                                   confidence_mean: float, outlier_count: int,
                                   total_predictions: int) -> float:
        """ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        # å„è¦ç´ ã®ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        
        # 1. äºˆæ¸¬æ—¥ã®ä¸€è²«æ€§ï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
        pred_consistency = 1.0 / (1.0 + pred_std_days / 30.0)  # 30æ—¥åŸºæº–
        
        # 2. tcå€¤ã®ä¸€è²«æ€§
        tc_consistency = 1.0 / (1.0 + tc_std / 0.1)  # 0.1åŸºæº–
        
        # 3. å¹³å‡ä¿¡é ¼åº¦
        confidence_score = confidence_mean
        
        # 4. å¤–ã‚Œå€¤ç‡ã®é€†æ•°
        outlier_rate = outlier_count / max(total_predictions, 1)
        outlier_score = 1.0 - outlier_rate
        
        # é‡ã¿ä»˜ãå¹³å‡
        weights = [0.4, 0.3, 0.2, 0.1]  # äºˆæ¸¬æ—¥ä¸€è²«æ€§ã‚’æœ€é‡è¦–
        scores = [pred_consistency, tc_consistency, confidence_score, outlier_score]
        
        return sum(w * s for w, s in zip(weights, scores))
    
    def visualize_consistency(self, symbol: str, save_path: Optional[str] = None):
        """ä¸€è²«æ€§åˆ†æçµæœã®å¯è¦–åŒ–"""
        
        if symbol not in self.predictions:
            print(f"âŒ {symbol} ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
            
        predictions = self.predictions[symbol]
        metrics = self.consistency_metrics.get(symbol)
        
        if not predictions:
            return
            
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'{symbol} Prediction Consistency Analysis', fontsize=16)
        
        # 1. äºˆæ¸¬æ—¥ã®æ™‚ç³»åˆ—
        ax1 = axes[0, 0]
        analysis_dates = [p.analysis_date for p in predictions]
        predicted_dates = [p.predicted_crash_date for p in predictions]
        colors = ['green' if p.quality_assessment.is_usable else 'red' for p in predictions]
        
        ax1.scatter(analysis_dates, predicted_dates, c=colors, alpha=0.6)
        if metrics and metrics.convergence_date:
            ax1.axhline(metrics.convergence_date, color='blue', linestyle='--', 
                       label=f'Convergence Date: {metrics.convergence_date.date()}')
        ax1.set_xlabel('Analysis Date')
        ax1.set_ylabel('Predicted Crash Date')
        ax1.set_title('Prediction Timeline')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. tcå€¤ã®æ¨ç§»
        ax2 = axes[0, 1]
        tc_values = [p.tc for p in predictions]
        ax2.scatter(analysis_dates, tc_values, c=colors, alpha=0.6)
        ax2.axhline(np.mean(tc_values), color='blue', linestyle='--', 
                   label=f'Mean tc: {np.mean(tc_values):.3f}')
        ax2.set_xlabel('Analysis Date')
        ax2.set_ylabel('tc value')
        ax2.set_title('tc Value Evolution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. ä¿¡é ¼åº¦åˆ†å¸ƒ
        ax3 = axes[0, 2]
        confidences = [p.confidence for p in predictions]
        ax3.hist(confidences, bins=20, alpha=0.7, color='steelblue', edgecolor='black')
        ax3.axvline(np.mean(confidences), color='red', linestyle='--',
                   label=f'Mean: {np.mean(confidences):.3f}')
        ax3.set_xlabel('Confidence')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Confidence Distribution')
        ax3.legend()
        
        # 4. ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ¥äºˆæ¸¬åˆ†å¸ƒ
        ax4 = axes[1, 0]
        window_groups = {}
        for p in predictions:
            if p.window_days not in window_groups:
                window_groups[p.window_days] = []
            window_groups[p.window_days].append(p.predicted_crash_date)
            
        for window, dates in window_groups.items():
            timestamps = [d.timestamp() for d in dates]
            ax4.scatter([window] * len(timestamps), timestamps, 
                       alpha=0.6, label=f'{window}d window')
        ax4.set_xlabel('Analysis Window (days)')
        ax4.set_ylabel('Predicted Date (timestamp)')
        ax4.set_title('Predictions by Window Size')
        
        # 5. ä¸€è²«æ€§æŒ‡æ¨™
        ax5 = axes[1, 1]
        if metrics:
            metric_names = ['Pred Consistency', 'tc Consistency', 'Confidence', 'Outlier Rate']
            metric_values = [
                1.0 / (1.0 + metrics.prediction_std_days / 30.0),
                1.0 / (1.0 + metrics.tc_std / 0.1),
                metrics.confidence_mean,
                1.0 - (metrics.outlier_count / max(metrics.total_predictions, 1))
            ]
            
            bars = ax5.bar(metric_names, metric_values, alpha=0.7, color='lightcoral')
            ax5.set_ylim(0, 1)
            ax5.set_ylabel('Score (0-1)')
            ax5.set_title('Consistency Metrics')
            
            # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
            for bar, value in zip(bars, metric_values):
                ax5.text(bar.get_x() + bar.get_width()/2., bar.get_height(),
                        f'{value:.3f}', ha='center', va='bottom')
        
        # 6. äºˆæ¸¬ã¾ã§ã®æ—¥æ•°åˆ†å¸ƒ
        ax6 = axes[1, 2]
        days_to_prediction = [p.days_to_prediction for p in predictions]
        ax6.hist(days_to_prediction, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax6.axvline(np.mean(days_to_prediction), color='red', linestyle='--',
                   label=f'Mean: {np.mean(days_to_prediction):.0f} days')
        ax6.set_xlabel('Days to Prediction')
        ax6.set_ylabel('Frequency')
        ax6.set_title('Prediction Horizon Distribution')
        ax6.legend()
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å¯è¦–åŒ–ä¿å­˜: {save_path}")
        plt.show()
    
    def generate_report(self, symbol: str) -> str:
        """ä¸€è²«æ€§åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        if symbol not in self.consistency_metrics:
            return f"âŒ {symbol} ã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“"
            
        metrics = self.consistency_metrics[symbol]
        predictions = self.predictions[symbol]
        
        report = f"""
ğŸ“Š {symbol} æ™‚ç³»åˆ—äºˆæ¸¬ä¸€è²«æ€§åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
{'=' * 60}

ğŸ”¢ åŸºæœ¬çµ±è¨ˆ:
   ç·äºˆæ¸¬æ•°: {metrics.total_predictions}
   ä½¿ç”¨å¯èƒ½äºˆæ¸¬æ•°: {metrics.usable_predictions}
   ä½¿ç”¨å¯èƒ½ç‡: {metrics.usable_predictions/max(metrics.total_predictions,1):.1%}

ğŸ“… äºˆæ¸¬ä¸€è²«æ€§:
   äºˆæ¸¬æ—¥æ¨™æº–åå·®: {metrics.prediction_std_days:.1f} æ—¥
   tcå€¤æ¨™æº–åå·®: {metrics.tc_std:.4f}
   å¤–ã‚Œå€¤æ•°: {metrics.outlier_count}

ğŸ¯ åæŸåˆ†æ:
   åæŸäºˆæ¸¬æ—¥: {metrics.convergence_date.date() if metrics.convergence_date else 'N/A'}
   åæŸä¿¡é ¼åº¦: {metrics.convergence_confidence:.2%}

ğŸ“ˆ å“è³ªæŒ‡æ¨™:
   å¹³å‡ä¿¡é ¼åº¦: {metrics.confidence_mean:.2%}
   ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {metrics.consistency_score:.3f} (0-1)

ğŸ’¡ è§£é‡ˆ:
"""
        
        # è§£é‡ˆã®è¿½åŠ 
        if metrics.consistency_score > 0.8:
            report += "   âœ… é«˜ã„äºˆæ¸¬ä¸€è²«æ€§ - ä¿¡é ¼ã§ãã‚‹åæŸäºˆæ¸¬"
        elif metrics.consistency_score > 0.6:
            report += "   ğŸŸ¡ ä¸­ç¨‹åº¦ã®äºˆæ¸¬ä¸€è²«æ€§ - æ³¨æ„æ·±ã„ç›£è¦–ãŒå¿…è¦"
        else:
            report += "   ğŸ”´ ä½ã„äºˆæ¸¬ä¸€è²«æ€§ - äºˆæ¸¬ã®ä¿¡é ¼æ€§ã«å•é¡Œ"
            
        if metrics.prediction_std_days < 30:
            report += "\\n   âœ… äºˆæ¸¬æ—¥ã®ã°ã‚‰ã¤ããŒå°ã•ã„"
        elif metrics.prediction_std_days < 90:
            report += "\\n   ğŸŸ¡ äºˆæ¸¬æ—¥ã®ã°ã‚‰ã¤ããŒä¸­ç¨‹åº¦"
        else:
            report += "\\n   ğŸ”´ äºˆæ¸¬æ—¥ã®ã°ã‚‰ã¤ããŒå¤§ãã„"
            
        return report
    
    def save_results(self, symbol: str, filepath: str):
        """åˆ†æçµæœã‚’CSVã§ä¿å­˜"""
        
        if symbol not in self.predictions:
            print(f"âŒ {symbol} ã®äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
            
        predictions = self.predictions[symbol]
        
        # DataFrameä½œæˆ
        data = []
        for p in predictions:
            data.append({
                'analysis_date': p.analysis_date.strftime('%Y-%m-%d'),
                'predicted_crash_date': p.predicted_crash_date.strftime('%Y-%m-%d'),
                'days_to_prediction': p.days_to_prediction,
                'tc': p.tc,
                'beta': p.beta,
                'omega': p.omega,
                'r_squared': p.r_squared,
                'rmse': p.rmse,
                'confidence': p.confidence,
                'window_days': p.window_days,
                'quality': p.quality_assessment.quality.value if p.quality_assessment else 'unknown',
                'is_usable': p.quality_assessment.is_usable if p.quality_assessment else False
            })
            
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False)
        print(f"ğŸ’¾ åˆ†æçµæœä¿å­˜: {filepath}")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    import sys
    import os
    sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
    
    from src.data_sources.fred_data_client import FREDDataClient
    from dotenv import load_dotenv
    
    load_dotenv()
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    analyzer = TimeSeriesPredictionAnalyzer()
    fred_client = FREDDataClient()
    
    # NASDAQ ã®6ãƒ¶æœˆé–“åˆ†æï¼ˆãƒ†ã‚¹ãƒˆç”¨çŸ­æœŸé–“ï¼‰
    end_date = datetime.now()
    start_date = end_date - timedelta(days=60)  # 2ãƒ¶æœˆé–“
    
    metrics = analyzer.analyze_symbol_consistency(
        'NASDAQ', fred_client, start_date, end_date, analysis_interval_days=7
    )
    
    print(analyzer.generate_report('NASDAQ'))